[
{
  "article_id": "2601.08831v1",
  "source": "arxiv",
  "doi": "",
  "title": "3AM: Segment Anything with Geometric Consistency in Videos",
  "abstract": "Video object segmentation methods like SAM2 achieve strong performance through memory-based architectures but struggle under large viewpoint changes due to reliance on appearance features. Traditional 3D instance segmentation methods address viewpoint consistency but require camera poses, depth maps, and expensive preprocessing. We introduce 3AM, a training-time enhancement that integrates 3D-aware features from MUSt3R into SAM2. Our lightweight Feature Merger fuses multi-level MUSt3R features that encode implicit geometric correspondence. Combined with SAM2's appearance features, the model achieves geometry-consistent recognition grounded in both spatial position and visual similarity. We propose a field-of-view aware sampling strategy ensuring frames observe spatially consistent object regions for reliable 3D correspondence learning. Critically, our method requires only RGB input at inference, with no camera poses or preprocessing. On challenging datasets with wide-baseline motion (ScanNet++, Replica), 3AM substantially outperforms SAM2 and extensions, achieving 90.6% IoU and 71.7% Positive IoU on ScanNet++'s Selected Subset, improving over state-of-the-art VOS methods by +15.9 and +30.4 points. Project page: https://jayisaking.github.io/3AM-Page/",
  "authors": [
    "Yang-Che Sun",
    "Cheng Sun",
    "Chin-Yang Lin",
    "Fu-En Yang",
    "Min-Hung Chen",
    "Yen-Yu Lin",
    "Yu-Lun Liu"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CV"
  ],
  "subjects": [
    "cs.CV"
  ],
  "keywords": [
    "machine learning"
  ],
  "url": "http://arxiv.org/abs/2601.08831v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08831v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:08.799225",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08828v1",
  "source": "arxiv",
  "doi": "",
  "title": "Motion Attribution for Video Generation",
  "abstract": "Despite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present Motive (MOTIon attribution for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. We use this to study which fine-tuning clips improve or degrade temporal dynamics. Motive isolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation. On text-to-video models, Motive identifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility. With Motive-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model. To our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate fine-tuning data.",
  "authors": [
    "Xindi Wu",
    "Despoina Paschalidou",
    "Jun Gao",
    "Antonio Torralba",
    "Laura Leal-Taixé",
    "Olga Russakovsky",
    "Sanja Fidler",
    "Jonathan Lorraine"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CV",
    "cs.AI",
    "cs.LG",
    "cs.MM",
    "cs.RO"
  ],
  "subjects": [
    "cs.CV",
    "cs.AI",
    "cs.LG",
    "cs.MM",
    "cs.RO"
  ],
  "keywords": [
    "machine learning"
  ],
  "url": "http://arxiv.org/abs/2601.08828v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08828v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:08.808062",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08813v1",
  "source": "arxiv",
  "doi": "",
  "title": "An Optimal Observable Machine for reinterpretable measurements in high-energy physics",
  "abstract": "A machine-learning-based framework for constructing generator-level observables optimized for parameter extraction in particle physics analyses is introduced, referred to as the Optimal Observable Machine (OOM). Unfoldable differential distributions are learned that maximize sensitivity to a parameter of interest while remaining robust against detector effects, systematic uncertainties, and biases introduced by the unfolding procedure. Detector response and systematic uncertainties are explicitly incorporated into the training through a likelihood-based loss function, enabling a direct optimization of the expected measurement precision while minimizing the bias from any assumption on the parameter of interest itself. The approach is demonstrated in an application to top quark physics, focusing on the measurement of a recently observed pseudoscalar excess at the top quark pair production threshold in dilepton final states. It is shown that a generator-level observable with enhanced sensitivity and long-term reinterpretability can be constructed using this method.",
  "authors": [
    "Torben Mohr",
    "Alejandro Quiroga Triviño",
    "Fabian Riemer",
    "Artur Monsch",
    "Matteo Defranchis",
    "Joscha Knolle",
    "Ankita Mehta",
    "Jan Kieseler",
    "Markus Klute"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "hep-ph",
    "hep-ex"
  ],
  "subjects": [
    "hep-ph",
    "hep-ex"
  ],
  "keywords": [
    "machine learning"
  ],
  "url": "http://arxiv.org/abs/2601.08813v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08813v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:08.811204",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08808v1",
  "source": "arxiv",
  "doi": "",
  "title": "Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge",
  "abstract": "Large language models often solve complex reasoning tasks more effectively with Chain-of-Thought (CoT), but at the cost of long, low-bandwidth token sequences. Humans, by contrast, often reason softly by maintaining a distribution over plausible next steps. Motivated by this, we propose Multiplex Thinking, a stochastic soft reasoning mechanism that, at each thinking step, samples K candidate tokens and aggregates their embeddings into a single continuous multiplex token. This preserves the vocabulary embedding prior and the sampling dynamics of standard discrete generation, while inducing a tractable probability distribution over multiplex rollouts. Consequently, multiplex trajectories can be directly optimized with on-policy reinforcement learning (RL). Importantly, Multiplex Thinking is self-adaptive: when the model is confident, the multiplex token is nearly discrete and behaves like standard CoT; when it is uncertain, it compactly represents multiple plausible next steps without increasing sequence length. Across challenging math reasoning benchmarks, Multiplex Thinking consistently outperforms strong discrete CoT and RL baselines from Pass@1 through Pass@1024, while producing shorter sequences. The code and checkpoints are available at https://github.com/GMLR-Penn/Multiplex-Thinking.",
  "authors": [
    "Yao Tang",
    "Li Dong",
    "Yaru Hao",
    "Qingxiu Dong",
    "Furu Wei",
    "Jiatao Gu"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ],
  "subjects": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ],
  "keywords": [
    "machine learning"
  ],
  "url": "http://arxiv.org/abs/2601.08808v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08808v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:08.813703",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08803v1",
  "source": "arxiv",
  "doi": "",
  "title": "Identifying Latent Intentions via Inverse Reinforcement Learning in Repeated Linear Public Good Games",
  "abstract": "Behavior in repeated public goods games continues to challenge standard theory: heterogeneous social preferences can explain first-round contributions, but not the substantial volatility observed across repeated interactions. Using 50,390 decisions from 2,938 participants, we introduce two methodological advances to address this gap. First, we cluster behavioral trajectories by their temporal shape using Dynamic Time Warping, yielding distinct and theoretically interpretable behavioral types. Second, we apply a hierarchical inverse Q-learning framework that models decisions as discrete switches between latent cooperative and defective intentions. This approach reveals a large (21.4%) and previously unmodeled behavioral type -- Switchers -- who frequently reverse intentions rather than commit to stable strategies. At the same time, the framework recovers canonical strategic behaviors such as persistent cooperation and free-riding. Substantively, recognizing intentional volatility helps sustain cooperation: brief defections by Switchers often reverse, so strategic patience can prevent unnecessary breakdowns.",
  "authors": [
    "Carina I. Hausladen",
    "Marcel H. Schubert",
    "Christoph Engel"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "econ.GN"
  ],
  "subjects": [
    "econ.GN"
  ],
  "keywords": [
    "machine learning"
  ],
  "url": "http://arxiv.org/abs/2601.08803v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08803v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:08.815994",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08797v1",
  "source": "arxiv",
  "doi": "",
  "title": "DentalX: Context-Aware Dental Disease Detection with Radiographs",
  "abstract": "Diagnosing dental diseases from radiographs is time-consuming and challenging due to the subtle nature of diagnostic evidence. Existing methods, which rely on object detection models designed for natural images with more distinct target patterns, struggle to detect dental diseases that present with far less visual support. To address this challenge, we propose {\\bf DentalX}, a novel context-aware dental disease detection approach that leverages oral structure information to mitigate the visual ambiguity inherent in radiographs. Specifically, we introduce a structural context extraction module that learns an auxiliary task: semantic segmentation of dental anatomy. The module extracts meaningful structural context and integrates it into the primary disease detection task to enhance the detection of subtle dental diseases. Extensive experiments on a dedicated benchmark demonstrate that DentalX significantly outperforms prior methods in both tasks. This mutual benefit arises naturally during model optimization, as the correlation between the two tasks is effectively captured. Our code is available at https://github.com/zhiqin1998/DentYOLOX.",
  "authors": [
    "Zhi Qin Tan",
    "Xiatian Zhu",
    "Owen Addison",
    "Yunpeng Li"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CV"
  ],
  "subjects": [
    "cs.CV"
  ],
  "keywords": [
    "machine learning"
  ],
  "url": "http://arxiv.org/abs/2601.08797v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08797v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:08.818221",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08790v1",
  "source": "arxiv",
  "doi": "",
  "title": "Aggregating Diverse Cue Experts for AI-Generated Image Detection",
  "abstract": "The rapid emergence of image synthesis models poses challenges to the generalization of AI-generated image detectors. However, existing methods often rely on model-specific features, leading to overfitting and poor generalization. In this paper, we introduce the Multi-Cue Aggregation Network (MCAN), a novel framework that integrates different yet complementary cues in a unified network. MCAN employs a mixture-of-encoders adapter to dynamically process these cues, enabling more adaptive and robust feature representation. Our cues include the input image itself, which represents the overall content, and high-frequency components that emphasize edge details. Additionally, we introduce a Chromatic Inconsistency (CI) cue, which normalizes intensity values and captures noise information introduced during the image acquisition process in real images, making these noise patterns more distinguishable from those in AI-generated content. Unlike prior methods, MCAN's novelty lies in its unified multi-cue aggregation framework, which integrates spatial, frequency-domain, and chromaticity-based information for enhanced representation learning. These cues are intrinsically more indicative of real images, enhancing cross-model generalization. Extensive experiments on the GenImage, Chameleon, and UniversalFakeDetect benchmark validate the state-of-the-art performance of MCAN. In the GenImage dataset, MCAN outperforms the best state-of-the-art method by up to 7.4% in average ACC across eight different image generators.",
  "authors": [
    "Lei Tan",
    "Shuwei Li",
    "Mohan Kankanhalli",
    "Robby T. Tan"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CV"
  ],
  "subjects": [
    "cs.CV"
  ],
  "keywords": [
    "machine learning"
  ],
  "url": "http://arxiv.org/abs/2601.08790v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08790v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:08.820519",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08784v1",
  "source": "arxiv",
  "doi": "",
  "title": "On the use of graph models to achieve individual and group fairness",
  "abstract": "Machine Learning algorithms are ubiquitous in key decision-making contexts such as justice, healthcare and finance, which has spawned a great demand for fairness in these procedures. However, the theoretical properties of such models in relation with fairness are still poorly understood, and the intuition behind the relationship between group and individual fairness is still lacking. In this paper, we provide a theoretical framework based on Sheaf Diffusion to leverage tools based on dynamical systems and homology to model fairness. Concretely, the proposed method projects input data into a bias-free space that encodes fairness constrains, resulting in fair solutions. Furthermore, we present a collection of network topologies handling different fairness metrics, leading to a unified method capable of dealing with both individual and group bias. The resulting models have a layer of interpretability in the form of closed-form expressions for their SHAP values, consolidating their place in the responsible Artificial Intelligence landscape. Finally, these intuitions are tested on a simulation study and standard fairness benchmarks, where the proposed methods achieve satisfactory results. More concretely, the paper showcases the performance of the proposed models in terms of accuracy and fairness, studying available trade-offs on the Pareto frontier, checking the effects of changing the different hyper-parameters, and delving into the interpretation of its outputs.",
  "authors": [
    "Arturo Pérez-Peralta",
    "Sandra Benítez-Peña",
    "Rosa E. Lillo"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "stat.ML",
    "cs.CY",
    "cs.LG"
  ],
  "subjects": [
    "stat.ML",
    "cs.CY",
    "cs.LG"
  ],
  "keywords": [
    "machine learning"
  ],
  "url": "http://arxiv.org/abs/2601.08784v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08784v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:08.822676",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08781v1",
  "source": "arxiv",
  "doi": "",
  "title": "Fast and explainable clustering in the Manhattan and Tanimoto distance",
  "abstract": "The CLASSIX algorithm is a fast and explainable approach to data clustering. In its original form, this algorithm exploits the sorting of the data points by their first principal component to truncate the search for nearby data points, with nearness being defined in terms of the Euclidean distance. Here we extend CLASSIX to other distance metrics, including the Manhattan distance and the Tanimoto distance. Instead of principal components, we use an appropriate norm of the data vectors as the sorting criterion, combined with the triangle inequality for search termination. In the case of Tanimoto distance, a provably sharper intersection inequality is used to further boost the performance of the new algorithm. On a real-world chemical fingerprint benchmark, CLASSIX Tanimoto is about 30 times faster than the Taylor--Butina algorithm, and about 80 times faster than DBSCAN, while computing higher-quality clusters in both cases.",
  "authors": [
    "Stefan Güttel",
    "Kaustubh Roy"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.LG"
  ],
  "subjects": [
    "cs.LG"
  ],
  "keywords": [
    "machine learning"
  ],
  "url": "http://arxiv.org/abs/2601.08781v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08781v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:08.825153",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08780v1",
  "source": "arxiv",
  "doi": "",
  "title": "LWM-Spectro: A Foundation Model for Wireless Baseband Signal Spectrograms",
  "abstract": "The received in-phase and quadrature (I/Q) baseband signals inherently encode physical-layer and channel characteristics of wireless links. Learning robust and transferable representations directly from such raw signals, however, remains challenging due to heterogeneous communication systems, diverse propagation environments, and limited labeled data. To address this, we present LWM-Spectro, a transformer-based foundation model pretrained on large-scale I/Q data represented as time-frequency spectrograms. The model leverages self-supervised masked modeling, contrastive learning, and a mixture-of-experts (MoE) architecture to learn general-purpose wireless representations. These representations transfer effectively to downstream tasks such as modulation classification and joint SNR/mobility recognition, even with minimal supervision. Across tasks, LWM-Spectro consistently outperforms state-of-the-art deep learning baselines in both few-shot and data-rich regimes, providing a unified foundation for wireless learning.",
  "authors": [
    "Namhyun Kim",
    "Sadjad Alikhani",
    "Ahmed Alkhateeb"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.IT",
    "eess.SP"
  ],
  "subjects": [
    "cs.IT",
    "eess.SP"
  ],
  "keywords": [
    "machine learning"
  ],
  "url": "http://arxiv.org/abs/2601.08780v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08780v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:08.827835",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08777v1",
  "source": "arxiv",
  "doi": "",
  "title": "Asymptotic Universal Alignment: A New Alignment Framework via Test-Time Scaling",
  "abstract": "Aligning large language models (LLMs) to serve users with heterogeneous and potentially conflicting preferences is a central challenge for personalized and trustworthy AI. We formalize an ideal notion of universal alignment through test-time scaling: for each prompt, the model produces $k\\ge 1$ candidate responses and a user selects their preferred one. We introduce $(k,f(k))$-robust alignment, which requires the $k$-output model to have win rate $f(k)$ against any other single-output model, and asymptotic universal alignment (U-alignment), which requires $f(k)\\to 1$ as $k\\to\\infty$. Our main result characterizes the optimal convergence rate: there exists a family of single-output policies whose $k$-sample product policies achieve U-alignment at rate $f(k)=\\frac{k}{k+1}$, and no method can achieve a faster rate in general. We show that popular post-training methods, including Nash learning from human feedback (NLHF), can fundamentally underutilize the benefits of test-time scaling. Even though NLHF is optimal for $k=1$, sampling from the resulting (often deterministic) policy cannot guarantee win rates above $\\tfrac{1}{2}$ except for an arbitrarily small slack. This stems from a lack of output diversity: existing alignment methods can collapse to a single majority-preferred response, making additional samples redundant. In contrast, our approach preserves output diversity and achieves the optimal test-time scaling rate. In particular, we propose a family of symmetric multi-player alignment games and prove that any symmetric Nash equilibrium policy of the $(k+1)$-player alignment game achieves the optimal $(k,\\frac{k}{k+1})$-robust alignment. Finally, we provide theoretical convergence guarantees for self-play learning dynamics in these games and extend the framework to opponents that also generate multiple responses.",
  "authors": [
    "Yang Cai",
    "Weiqiang Zheng"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.CL",
    "cs.GT"
  ],
  "subjects": [
    "cs.LG",
    "cs.AI",
    "cs.CL",
    "cs.GT"
  ],
  "keywords": [
    "machine learning"
  ],
  "url": "http://arxiv.org/abs/2601.08777v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08777v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:08.829647",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08776v1",
  "source": "arxiv",
  "doi": "",
  "title": "Translating Light-Sheet Microscopy Images to Virtual H&E Using CycleGAN",
  "abstract": "Histopathology analysis relies on Hematoxylin and Eosin (H&E) staining, but fluorescence microscopy offers complementary information. Converting fluorescence images to H&E-like appearance can aid interpretation and integration with standard workflows. We present a Cycle-Consistent Adversarial Network (CycleGAN) approach for unpaired image-to-image translation from multi-channel fluorescence microscopy to pseudo H&E stained histopathology images. The method combines C01 and C02 fluorescence channels into RGB and learns a bidirectional mapping between fluorescence and H&E domains without paired training data. The architecture uses ResNet-based generators with residual blocks and PatchGAN discriminators, trained with adversarial, cycle-consistency, and identity losses. Experiments on fluorescence microscopy datasets show the model generates realistic pseudo H&E images that preserve morphological structures while adopting H&E-like color characteristics. This enables visualization of fluorescence data in a format familiar to pathologists and supports integration with existing H&E-based analysis pipelines.",
  "authors": [
    "Yanhua Zhao"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CV",
    "cs.AI"
  ],
  "subjects": [
    "cs.CV",
    "cs.AI"
  ],
  "keywords": [
    "machine learning"
  ],
  "url": "http://arxiv.org/abs/2601.08776v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08776v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:08.831770",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08772v1",
  "source": "arxiv",
  "doi": "",
  "title": "Enhancing classical simulation with noisy quantum devices",
  "abstract": "As quantum devices continue to improve in scale and precision, a central challenge is how to effectively utilize noisy hardware for meaningful computation. Most existing approaches aim to recover noiseless circuit outputs from noisy ones through error mitigation or correction. Here, we show that noisy quantum devices can be directly leveraged as computational resources to enhance the classical simulation of quantum circuits. We introduce the Noisy-device-enhanced Classical Simulation (NDE-CS) protocol, which improves stabilizer-based classical Monte Carlo simulation methods by incorporating data obtained from noisy quantum hardware. Specifically, NDE-CS uses noisy executions of a target circuit together with noisy Clifford circuits to learn how the target circuit can be expressed in terms of Clifford circuits under realistic noise. The same learned relation can then be reused in the noiseless Clifford limit, enabling accurate estimation of ideal expectation values with substantially reduced sampling cost. Numerical simulations on Trotterized Ising circuits demonstrate that NDE-CS achieves orders-of-magnitude reductions in sampling cost compared to the underlying purely classical Monte Carlo approaches from which it is derived, while maintaining the same accuracy. We also compare NDE-CS with Sparse Pauli Dynamics (SPD), a powerful classical framework capable of simulating quantum circuits at previously inaccessible scales, and provide an example where the cost of SPD scales exponentially with system size, while NDE-CS scales much more favorably. These results establish NDE-CS as a scalable hybrid simulation approach for quantum circuits, where noise can be harnessed as a computational asset.",
  "authors": [
    "Ruiqi Zhang",
    "Fuchuan Wei",
    "Zhaohui Wei"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "quant-ph"
  ],
  "subjects": [
    "quant-ph"
  ],
  "keywords": [
    "machine learning"
  ],
  "url": "http://arxiv.org/abs/2601.08772v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08772v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:08.833303",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08764v1",
  "source": "arxiv",
  "doi": "",
  "title": "FusID: Modality-Fused Semantic IDs for Generative Music Recommendation",
  "abstract": "Generative recommendation systems have achieved significant advances by leveraging semantic IDs to represent items. However, existing approaches that tokenize each modality independently face two critical limitations: (1) redundancy across modalities that reduces efficiency, and (2) failure to capture inter-modal interactions that limits item representation. We introduce FusID, a modality-fused semantic ID framework that addresses these limitations through three key components: (i) multimodal fusion that learns unified representations by jointly encoding information across modalities, (ii) representation learning that brings frequently co-occurring item embeddings closer while maintaining distinctiveness and preventing feature redundancy, and (iii) product quantization that converts the fused continuous embeddings into multiple discrete tokens to mitigate ID conflict. Evaluated on a multimodal next-song recommendation (i.e., playlist continuation) benchmark, FusID achieves zero ID conflicts, ensuring that each token sequence maps to exactly one song, mitigates codebook underutilization, and outperforms baselines in terms of MRR and Recall@k (k = 1, 5, 10, 20).",
  "authors": [
    "Haven Kim",
    "Yupeng Hou",
    "Julian McAuley"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.IR",
    "cs.SD",
    "eess.AS"
  ],
  "subjects": [
    "cs.IR",
    "cs.SD",
    "eess.AS"
  ],
  "keywords": [
    "machine learning"
  ],
  "url": "http://arxiv.org/abs/2601.08764v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08764v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:08.835068",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08763v1",
  "source": "arxiv",
  "doi": "",
  "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs",
  "abstract": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@$k$ across large sampling budgets and increases the area under the pass@$k$ curve (AUC@$K$) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.",
  "authors": [
    "Zhiyuan Hu",
    "Yucheng Wang",
    "Yufei He",
    "Jiaying Wu",
    "Yilun Zhao",
    "See-Kiong Ng",
    "Cynthia Breazeal",
    "Anh Tuan Luu",
    "Hae Won Park",
    "Bryan Hooi"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.LG",
    "cs.CL"
  ],
  "subjects": [
    "cs.LG",
    "cs.CL"
  ],
  "keywords": [
    "machine learning"
  ],
  "url": "http://arxiv.org/abs/2601.08763v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08763v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:08.837603",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08760v1",
  "source": "arxiv",
  "doi": "",
  "title": "Adaptive Requesting in Decentralized Edge Networks via Non-Stationary Bandits",
  "abstract": "We study a decentralized collaborative requesting problem that aims to optimize the information freshness of time-sensitive clients in edge networks consisting of multiple clients, access nodes (ANs), and servers. Clients request content through ANs acting as gateways, without observing AN states or the actions of other clients. We define the reward as the age of information reduction resulting from a client's selection of an AN, and formulate the problem as a non-stationary multi-armed bandit. In this decentralized and partially observable setting, the resulting reward process is history-dependent and coupled across clients, and exhibits both abrupt and gradual changes in expected rewards, rendering classical bandit-based approaches ineffective. To address these challenges, we propose the AGING BANDIT WITH ADAPTIVE RESET algorithm, which combines adaptive windowing with periodic monitoring to track evolving reward distributions. We establish theoretical performance guarantees showing that the proposed algorithm achieves near-optimal performance, and we validate the theoretical results through simulations.",
  "authors": [
    "Yi Zhuang",
    "Kun Yang",
    "Xingran Chen"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.LG",
    "cs.MA"
  ],
  "subjects": [
    "cs.LG",
    "cs.MA"
  ],
  "keywords": [
    "machine learning"
  ],
  "url": "http://arxiv.org/abs/2601.08760v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08760v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:08.839544",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08734v1",
  "source": "arxiv",
  "doi": "10.1145/3786583.3786898",
  "title": "TerraFormer: Automated Infrastructure-as-Code with LLMs Fine-Tuned via Policy-Guided Verifier Feedback",
  "abstract": "Automating Infrastructure-as-Code (IaC) is challenging, and large language models (LLMs) often produce incorrect configurations from natural language (NL). We present TerraFormer, a neuro-symbolic framework for IaC generation and mutation that combines supervised fine-tuning with verifier-guided reinforcement learning, using formal verification tools to provide feedback on syntax, deployability, and policy compliance. We curate two large, high-quality NL-to-IaC datasets, TF-Gen (152k instances) and TF-Mutn (52k instances), via multi-stage verification and iterative LLM self-correction. Evaluations against 17 state-of-the-art LLMs, including ~50x larger models like Sonnet 3.7, DeepSeek-R1, and GPT-4.1, show that TerraFormer improves correctness over its base LLM by 15.94% on IaC-Eval, 11.65% on TF-Gen (Test), and 19.60% on TF-Mutn (Test). It outperforms larger models on both TF-Gen (Test) and TF-Mutn (Test), ranks third on IaC-Eval, and achieves top best-practices and security compliance.",
  "authors": [
    "Prithwish Jana",
    "Sam Davidson",
    "Bhavana Bhasker",
    "Andrey Kan",
    "Anoop Deoras",
    "Laurent Callot"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.SE",
    "cs.AI"
  ],
  "subjects": [
    "cs.SE",
    "cs.AI"
  ],
  "keywords": [
    "machine learning"
  ],
  "url": "http://arxiv.org/abs/2601.08734v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08734v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:08.842793",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08733v1",
  "source": "arxiv",
  "doi": "10.1109/SLAAI-ICAI68534.2025.11318441",
  "title": "A Novel Approach to Explainable AI with Quantized Active Ingredients in Decision Making",
  "abstract": "Artificial Intelligence (AI) systems have shown good success at classifying. However, the lack of explainability is a true and significant challenge, especially in high-stakes domains, such as health and finance, where understanding is paramount. We propose a new solution to this challenge: an explainable AI framework based on our comparative study with Quantum Boltzmann Machines (QBMs) and Classical Boltzmann Machines (CBMs). We leverage principles of quantum computing within classical machine learning to provide substantive transparency around decision-making. The design involves training both models on a binarised and dimensionally reduced MNIST dataset, where Principal Component Analysis (PCA) is applied for preprocessing. For interpretability, we employ gradient-based saliency maps in QBMs and SHAP (SHapley Additive exPlanations) in CBMs to evaluate feature attributions.QBMs deploy hybrid quantum-classical circuits with strongly entangling layers, allowing for richer latent representations, whereas CBMs serve as a classical baseline that utilises contrastive divergence. Along the way, we found that QBMs outperformed CBMs on classification accuracy (83.5% vs. 54%) and had more concentrated distributions in feature attributions as quantified by entropy (1.27 vs. 1.39). In other words, QBMs not only produced better predictive performance than CBMs, but they also provided clearer identification of \"active ingredient\" or the most important features behind model predictions. To conclude, our results illustrate that quantum-classical hybrid models can display improvements in both accuracy and interpretability, which leads us toward more trustworthy and explainable AI systems.",
  "authors": [
    "A. M. A. S. D. Alagiyawanna",
    "Asoka Karunananda",
    "Thushari Silva",
    "A. Mahasinghe"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.LG",
    "quant-ph"
  ],
  "subjects": [
    "cs.LG",
    "quant-ph"
  ],
  "keywords": [
    "machine learning"
  ],
  "url": "http://arxiv.org/abs/2601.08733v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08733v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:08.844708",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08732v1",
  "source": "arxiv",
  "doi": "",
  "title": "ISLA: A U-Net for MRI-based acute ischemic stroke lesion segmentation with deep supervision, attention, domain adaptation, and ensemble learning",
  "abstract": "Accurate delineation of acute ischemic stroke lesions in MRI is a key component of stroke diagnosis and management. In recent years, deep learning models have been successfully applied to the automatic segmentation of such lesions. While most proposed architectures are based on the U-Net framework, they primarily differ in their choice of loss functions and in the use of deep supervision, residual connections, and attention mechanisms. Moreover, many implementations are not publicly available, and the optimal configuration for acute ischemic stroke (AIS) lesion segmentation remains unclear. In this work, we introduce ISLA (Ischemic Stroke Lesion Analyzer), a new deep learning model for AIS lesion segmentation from diffusion MRI, trained on three multicenter databases totaling more than 1500 AIS participants. Through systematic optimization of the loss function, convolutional architecture, deep supervision, and attention mechanisms, we developed a robust segmentation framework. We further investigated unsupervised domain adaptation to improve generalization to an external clinical dataset. ISLA outperformed two state-of-the-art approaches for AIS lesion segmentation on an external test set. Codes and trained models will be made publicly available to facilitate reuse and reproducibility.",
  "authors": [
    "Vincent Roca",
    "Martin Bretzner",
    "Hilde Henon",
    "Laurent Puy",
    "Grégory Kuchcinski",
    "Renaud Lopes"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CV",
    "cs.AI"
  ],
  "subjects": [
    "cs.CV",
    "cs.AI"
  ],
  "keywords": [
    "machine learning"
  ],
  "url": "http://arxiv.org/abs/2601.08732v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08732v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:08.847096",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08731v1",
  "source": "arxiv",
  "doi": "",
  "title": "Learning from Demonstrations via Capability-Aware Goal Sampling",
  "abstract": "Despite its promise, imitation learning often fails in long-horizon environments where perfect replication of demonstrations is unrealistic and small errors can accumulate catastrophically. We introduce Cago (Capability-Aware Goal Sampling), a novel learning-from-demonstrations method that mitigates the brittle dependence on expert trajectories for direct imitation. Unlike prior methods that rely on demonstrations only for policy initialization or reward shaping, Cago dynamically tracks the agent's competence along expert trajectories and uses this signal to select intermediate steps--goals that are just beyond the agent's current reach--to guide learning. This results in an adaptive curriculum that enables steady progress toward solving the full task. Empirical results demonstrate that Cago significantly improves sample efficiency and final performance across a range of sparse-reward, goal-conditioned tasks, consistently outperforming existing learning from-demonstrations baselines.",
  "authors": [
    "Yuanlin Duan",
    "Yuning Wang",
    "Wenjie Qiu",
    "He Zhu"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.AI"
  ],
  "subjects": [
    "cs.AI"
  ],
  "keywords": [
    "machine learning"
  ],
  "url": "http://arxiv.org/abs/2601.08731v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08731v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:08.849455",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08513v1",
  "source": "arxiv",
  "doi": "10.5121/ijcsit.2025.17604",
  "title": "A decentralized academic certificate issuance system using smart contracts on the tron network",
  "abstract": "This paper presents the design, implementation, and evaluation of a decentralized system for issuing and verifying academic certificates based on blockchain technology. The proposed solution addresses common limitations of traditional certification models, such as susceptibility to forgery, reliance on centralized infrastructures, and inefficient verification processes. The system is built on the TRON blockchain and integrates smart contracts written in Solidity, a decentralized web application (dApp) for user interaction, and the InterPlanetary File System (IPFS) for decentralized storage of certificate metadata. The methodology comprised architectural design, smart contract development, and the implementation of a web-based interface, followed by functional, security, performance, and usability evaluations. Experimental results show that the system correctly supports certificate issuance and public verification, enforces access control, and resists common misuse scenarios. Performance analysis indicates low confirmation latency and negligible transaction costs, making the solution suitable for large-scale academic environments. Additionally, usability assessment using the System Usability Scale (SUS) resulted in a score of 76.67, indicating good user acceptance. Overall, the results demonstrate the technical feasibility and practical viability of the proposed approach, highlighting the TRON blockchain as an effective and cost-efficient infrastructure for decentralized academic certification systems.",
  "authors": [
    "Ana Julia Evangelista Andrade",
    "Flavio Cezar Amate"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.NI"
  ],
  "subjects": [
    "cs.NI"
  ],
  "keywords": [
    "blockchain"
  ],
  "url": "http://arxiv.org/abs/2601.08513v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08513v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:11.012659",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08339v1",
  "source": "arxiv",
  "doi": "10.1016/j.energy.2023.130110",
  "title": "Blockchain-Enabled Renewable Energy Certificate Trading: A Secure and Privacy-Preserving Approach",
  "abstract": "In the 21st century, transitioning to renewable energy sources is imperative, with fossil fuel reserves depleting rapidly and recognizing critical environmental issues such as climate change, air pollution, water pollution, and habitat destruction. Embracing renewable energy is not only an environmental necessity but also a strategic move with multiple benefits. By shifting to renewable energy sources and supporting their production through the acquisition of renewable energy certificates, we foster innovation and drive economic growth in the renewable energy sector. This, in turn, reduces greenhouse gas emissions, aligning with global efforts to mitigate climate change. Additionally, renewable energy certificates ensure compliance with regulations that mandate the use of renewable energy, enhancing legal adherence while promoting transparency and trust in energy sourcing. To monitor the uptake of renewable energy, governments have implemented Renewable Energy Certificates (RECs) as a tracking mechanism for the production and consumption of renewable energy. However, there are two main challenges to the existing REC schema: 1) The RECs have not been globally adopted due to inconsistent design; 2) The consumer privacy has not been well incorporated in the design of blockchain. In this study, we investigate the trading of RECs between suppliers and consumers using the directed acyclic graph (DAG) blockchain system and introduce a trading schema to help protect consumer information. Our results demonstrate lower transaction time by 41\\% and energy consumption by 65\\% compared to proof-of-stake.",
  "authors": [
    "Wei-Jen Liu",
    "Wei-Yu Chiu",
    "Weiqi Hua"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "eess.SY"
  ],
  "subjects": [
    "eess.SY"
  ],
  "keywords": [
    "blockchain"
  ],
  "url": "http://arxiv.org/abs/2601.08339v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08339v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:11.018576",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08091v1",
  "source": "arxiv",
  "doi": "",
  "title": "Decentralized Firmware Integrity Verification for Cyber-Physical Systems Using Ethereum Blockchain",
  "abstract": "Firmware integrity is a foundational requirement for securing Cyber-Physical Systems (CPS), where malicious or compromised firmware can result in persistent backdoors, unauthorized control, or catastrophic system failures. Traditional verification mechanisms such as secure boot, digital signatures, and centralized hash databases are increasingly inadequate due to risks from insider threats and single points of failure. In this paper, we propose a decentralized firmware integrity verification framework built on the Ethereum blockchain, offering tamperproof, transparent, and trustless validation. Our system stores SHA-256 hashes of firmware binaries within smart contracts deployed on the Ethereum Sepolia testnet, using Web3 and Infura for seamless on-chain interaction. A Python-based client tool computes firmware hashes and communicates with the blockchain to register and verify firmware authenticity in realtime. We implement and evaluate a fully functional prototype using real firmware samples, demonstrating successful contract deployment, hash registration, and integrity verification through live blockchain transactions. Experimental results confirm the reliability and low cost (in gas fees) of our approach, highlighting its practicality and scalability for real-world CPS applications. To enhance scalability and performance, we discuss extensions using Layer-2 rollups and off-chain storage via the InterPlanetary File System (IPFS). We also outline integration pathways with secure boot mechanisms, Trusted Platform Module (TPM)based attestation, and zero-trust architectures. This work contributes a practical and extensible model for blockchain-based firmware verification, significantly strengthening the defense against firmware tampering and supply chain attacks in critical CPS environments.",
  "authors": [
    "S M Mostaq Hossain",
    "Amani Altarawneh"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CR"
  ],
  "subjects": [
    "cs.CR"
  ],
  "keywords": [
    "blockchain"
  ],
  "url": "http://arxiv.org/abs/2601.08091v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08091v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:11.024130",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.07726v1",
  "source": "arxiv",
  "doi": "",
  "title": "TeeMAF: A TEE-Based Mutual Attestation Framework for On-Chain and Off-Chain Functions in Blockchain DApps",
  "abstract": "The rapid development of Internet of Things (IoT) technology has led to growing concerns about data security and user privacy in the interactions within distributed systems. Decentralized Applications (DApps) in distributed systems consist of on-chain and off-chain functions, where on-chain functions are smart contracts running in the blockchain network, while off-chain functions operate outside the blockchain. Since smart contracts cannot access off-chain information, they cannot verify whether the off-chain functions, i.e. the software components, they interact with have been tampered or not. As a result, establishing mutual trust between the on-chain smart contracts and the off-chain functions remains a significant challenge. To address the challenge, this paper introduces TeeMAF, a generic framework for mutual attestation between on-chain and off-chain functions, leveraging Trusted Execution Environments (TEE), specifically Intel Software Guard Extensions (SGX), SCONE (a TEE container on top of Intel SGX), and remote attestation technologies. This ensures that the deployed off-chain functions of a DApp execute in a provably secure computing environment and achieve mutual attestation with the interacting on-chain functions. Through a security analysis of TeeMAF, the reliability of deployed DApps can be verified, ensuring their correct execution. Furthermore, based on this framework, this paper proposes a decentralized resource orchestration platform (a specific DApp) for deploying applications over untrusted environments. The system is implemented on Ethereum and benchmarked using Hyperledger Caliper. Performance evaluation focusing on throughput and latency demonstrates that, compared to platforms without a mutual attestation scheme, the performance overhead remains within an acceptable range.",
  "authors": [
    "Xiangyu Liu",
    "Brian Lee",
    "Yuansong Qiao"
  ],
  "affiliations": [],
  "date_published": "2026-01-12",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CR",
    "cs.NI"
  ],
  "subjects": [
    "cs.CR",
    "cs.NI"
  ],
  "keywords": [
    "blockchain"
  ],
  "url": "http://arxiv.org/abs/2601.07726v1",
  "pdf_url": "https://arxiv.org/pdf/2601.07726v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:11.030934",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.07654v1",
  "source": "arxiv",
  "doi": "",
  "title": "Towards Automating Blockchain Consensus Verification with IsabeLLM",
  "abstract": "Consensus protocols are crucial for a blockchain system as they are what allow agreement between the system's nodes in a potentially adversarial environment. For this reason, it is paramount to ensure their correct design and implementation to prevent such adversaries from carrying out malicious behaviour. Formal verification allows us to ensure the correctness of such protocols, but requires high levels of effort and expertise to carry out and thus is often omitted in the development process. In this paper, we present IsabeLLM, a tool that integrates the proof assistant Isabelle with a Large Language Model to assist and automate proofs. We demonstrate the effectiveness of IsabeLLM by using it to develop a novel model of Bitcoin's Proof of Work consensus protocol and verify its correctness. We use the DeepSeek R1 API for this demonstration and found that we were able to generate correct proofs for each of the non-trivial lemmas present in the verification.",
  "authors": [
    "Elliot Jones",
    "William Knottenbelt"
  ],
  "affiliations": [],
  "date_published": "2026-01-12",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CR",
    "cs.AI"
  ],
  "subjects": [
    "cs.CR",
    "cs.AI"
  ],
  "keywords": [
    "blockchain"
  ],
  "url": "http://arxiv.org/abs/2601.07654v1",
  "pdf_url": "https://arxiv.org/pdf/2601.07654v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:11.034719",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.07334v1",
  "source": "arxiv",
  "doi": "10.1016/j.jss.2025.112593",
  "title": "Examining the Effectiveness of Transformer-Based Smart Contract Vulnerability Scan",
  "abstract": "Smart contract technology facilitates self-executing agreements on the blockchain, eliminating dependency on an external trusted authority. However, smart contracts may expose vulnerabilities that can lead to financial losses and disruptions in decentralized applications. In this work, we evaluate deep learning-based approaches for vulnerability scanning of Ethereum smart contracts. We propose VASCOT, a Vulnerability Analyzer for Smart COntracts using Transformers, which performs sequential analysis of Ethereum Virtual Machine (EVM) bytecode and incorporates a sliding window mechanism to overcome input length constraints. To assess VASCOT's detection efficacy, we construct a dataset of 16,469 verified Ethereum contracts deployed in 2022, and annotate it using trace analysis with concrete validation to mitigate false positives. VASCOT's performance is then compared against a state-of-the-art LSTM-based vulnerability detection model on both our dataset and an older public dataset. Our findings highlight the strengths and limitations of each model, providing insights into their detection capabilities and generalizability.",
  "authors": [
    "Emre Balci",
    "Timucin Aydede",
    "Gorkem Yilmaz",
    "Ece Gelal Soyak"
  ],
  "affiliations": [],
  "date_published": "2026-01-12",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CR",
    "eess.SY"
  ],
  "subjects": [
    "cs.CR",
    "eess.SY"
  ],
  "keywords": [
    "blockchain"
  ],
  "url": "http://arxiv.org/abs/2601.07334v1",
  "pdf_url": "https://arxiv.org/pdf/2601.07334v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:11.037766",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.07134v1",
  "source": "arxiv",
  "doi": "",
  "title": "Proof of Reasoning for Privacy Enhanced Federated Blockchain Learning at the Edge",
  "abstract": "Consensus mechanisms are the core of any blockchain system. However, the majority of these mechanisms do not target federated learning directly nor do they aid in the aggregation step. This paper introduces Proof of Reasoning (PoR), a novel consensus mechanism specifically designed for federated learning using blockchain, aimed at preserving data privacy, defending against malicious attacks, and enhancing the validation of participating networks. Unlike generic blockchain consensus mechanisms commonly found in the literature, PoR integrates three distinct processes tailored for federated learning. Firstly, a masked autoencoder (MAE) is trained to generate an encoder that functions as a feature map and obfuscates input data, rendering it resistant to human reconstruction and model inversion attacks. Secondly, a downstream classifier is trained at the edge, receiving input from the trained encoder. The downstream network's weights, a single encoded datapoint, the network's output and the ground truth are then added to a block for federated aggregation. Lastly, this data facilitates the aggregation of all participating networks, enabling more complex and verifiable aggregation methods than previously possible. This three-stage process results in more robust networks with significantly reduced computational complexity, maintaining high accuracy by training only the downstream classifier at the edge. PoR scales to large IoT networks with low latency and storage growth, and adapts to evolving data, regulations, and network conditions.",
  "authors": [
    "James Calo",
    "Benny Lo"
  ],
  "affiliations": [],
  "date_published": "2026-01-12",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CR",
    "cs.CV",
    "cs.LG"
  ],
  "subjects": [
    "cs.CR",
    "cs.CV",
    "cs.LG"
  ],
  "keywords": [
    "blockchain"
  ],
  "url": "http://arxiv.org/abs/2601.07134v1",
  "pdf_url": "https://arxiv.org/pdf/2601.07134v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:11.045388",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.07019v1",
  "source": "arxiv",
  "doi": "",
  "title": "Zer0n: An AI-Assisted Vulnerability Discovery and Blockchain-Backed Integrity Framework",
  "abstract": "As vulnerability research increasingly adopts generative AI, a critical reliance on opaque model outputs has emerged, creating a \"trust gap\" in security automation. We address this by introducing Zer0n, a framework that anchors the reasoning capabilities of Large Language Models (LLMs) to the immutable audit trails of blockchain technology. Specifically, we integrate Gemini 2.0 Pro for logic-based vulnerability detection with the Avalanche C-Chain for tamper-evident artifact logging. Unlike fully decentralized solutions that suffer from high latency, Zer0n employs a hybrid architecture: execution remains off-chain for performance, while integrity proofs are finalized on-chain. Our evaluation on a dataset of 500 endpoints reveals that this approach achieves 80% detection accuracy with only a marginal 22.9% overhead, effectively demonstrating that decentralized integrity can coexist with high-speed security workflows.",
  "authors": [
    "Harshil Parmar",
    "Pushti Vyas",
    "Prayers Khristi",
    "Priyank Panchal"
  ],
  "affiliations": [],
  "date_published": "2026-01-11",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CR",
    "cs.AI",
    "cs.SE"
  ],
  "subjects": [
    "cs.CR",
    "cs.AI",
    "cs.SE"
  ],
  "keywords": [
    "blockchain"
  ],
  "url": "http://arxiv.org/abs/2601.07019v1",
  "pdf_url": "https://arxiv.org/pdf/2601.07019v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:11.047698",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.06699v1",
  "source": "arxiv",
  "doi": "",
  "title": "Incentive Mechanism Design for Privacy-Preserving Decentralized Blockchain Relayers",
  "abstract": "Public blockchains, though renowned for their transparency and immutability, suffer from significant privacy concerns. Network-level analysis and long-term observation of publicly available transactions can often be used to infer user identities. To mitigate this, several blockchain applications rely on relayers, which serve as intermediary nodes between users and smart contracts deployed on the blockchain. However, dependence on a single relayer not only creates a single point of failure but also introduces exploitable vulnerabilities that weaken the system's privacy guarantees. This paper proposes a decentralized relayer architecture that enhances privacy and reliability through game-theoretic incentive design. We model the interaction among relayers as a non-cooperative game and design an incentive mechanism in which probabilistic uploading emerges as a unique mixed Nash equilibrium. Using evolutionary game analysis, we demonstrate the equilibrium's stability against perturbations and coordinated deviations. Through numerical evaluations, we analyze how equilibrium strategies and system behavior evolve with key parameters such as the number of relayers, upload costs, rewards, and penalties. In particular, we show that even with high transaction costs, the system maintains reliability with an outage probability below 0.05 . Furthermore, our results highlight a fundamental trade-off between privacy, reliability, robustness, and cost in decentralized relayer systems.",
  "authors": [
    "Boutaina Jebari",
    "Khalil Ibrahimi",
    "Hamidou Tembine",
    "Mounir Ghogho"
  ],
  "affiliations": [],
  "date_published": "2026-01-10",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CR",
    "cs.MA"
  ],
  "subjects": [
    "cs.CR",
    "cs.MA"
  ],
  "keywords": [
    "blockchain"
  ],
  "url": "http://arxiv.org/abs/2601.06699v1",
  "pdf_url": "https://arxiv.org/pdf/2601.06699v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:11.050281",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.06667v1",
  "source": "arxiv",
  "doi": "",
  "title": "zkRansomware: Proof-of-Data Recoverability and Multi-round Game Theoretic Modeling of Ransomware Decisions",
  "abstract": "Ransomware is still one of the most serious cybersecurity threats. Victims often pay but fail to regain access to their data, while also facing the danger of losing data privacy. These uncertainties heavily shape the attacker-victim dynamics in decision-making. In this paper, we introduce and analyze zkRansomware. This new ransomware model integrates zero-knowledge proofs to enable verifiable data recovery and uses smart contracts to enforce multi-round payments while mitigating the risk of data disclosure and privacy loss. We show that zkRansomware is technically feasible using existing cryptographic and blockchain tools and, perhaps counterintuitively, can align incentives between the attacker and the victim. Finally, we develop a theoretical decision-making framework for zkRansomware that distinguishes it from known ransomware decision models and discusses its implications for ransomware risk analysis and response decision support.",
  "authors": [
    "Xinyu Hou",
    "Yang Lu",
    "Rabimba Karanjai",
    "Lei Xu",
    "Weidong Shi"
  ],
  "affiliations": [],
  "date_published": "2026-01-10",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CR",
    "cs.SE"
  ],
  "subjects": [
    "cs.CR",
    "cs.SE"
  ],
  "keywords": [
    "blockchain"
  ],
  "url": "http://arxiv.org/abs/2601.06667v1",
  "pdf_url": "https://arxiv.org/pdf/2601.06667v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:11.053078",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.05534v2",
  "source": "arxiv",
  "doi": "",
  "title": "Blockchain Verifiable Proof of Quantum Supremacy as a Trigger for Quantum-Secure Signatures",
  "abstract": "Blockchain is a decentralized, distributed ledger technology that ensures transparency, security, and immutability through cryptographic techniques. However, advancements in quantum computing threaten the security of classical cryptographic schemes, jeopardizing blockchain integrity once cryptographic quantum supremacy is achieved. This milestone, defined here as the realization of quantum computers to solve practical cryptographic problems, would render existing security standards vulnerable, exposing blockchain assets (currency, data, etc.) to fraud and theft. To address this risk, we propose and implement a smart contract deployable on the Ethereum blockchain, having the ability to run applications on its blockchain, that generates classically intractable puzzles by probabilistically generating large, hard-to-factor numbers without requiring secret information. This contract then serves two purposes: to establish a mechanism (1) for a trustless, unbiased proof of cryptographic quantum supremacy by verifying solutions to these puzzles, and (2) to protect user funds on Ethereum by triggering quantum-secure fallback protocols upon detecting cryptographic quantum supremacy, since it is desirable to wait as long as possible to fall back to a quantum-secure scheme because of its inherent additional cost and complexity. These mechanisms demonstrate the ability to identify cryptographic vulnerabilities and ensure a smooth transition to quantum-secure standards, safeguarding blockchain assets in a post-quantum era.",
  "authors": [
    "Nicholas J. C. Papadopoulos"
  ],
  "affiliations": [],
  "date_published": "2026-01-09",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CR"
  ],
  "subjects": [
    "cs.CR"
  ],
  "keywords": [
    "blockchain"
  ],
  "url": "http://arxiv.org/abs/2601.05534v2",
  "pdf_url": "https://arxiv.org/pdf/2601.05534v2",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:11.056473",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.05417v1",
  "source": "arxiv",
  "doi": "",
  "title": "Mean Field Analysis of Blockchain Systems",
  "abstract": "We present a novel framework for analyzing blockchain consensus mechanisms by modeling blockchain growth as a Partially Observable Stochastic Game (POSG) which we reduce to a set of Partially Observable Markov Decision Processes (POMDPs) through the use of the mean field approximation. This approach formalizes the decision-making process of miners in Proof-of-Work (PoW) systems and enables a principled examination of block selection strategies as well as steady state analysis of the induced Markov chain. By leveraging a mean field game formulation, we efficiently characterize the information asymmetries that arise in asynchronous blockchain networks. Our first main result is an exact characterization of the tradeoff between network delay and PoW efficiency--the fraction of blocks which end up in the longest chain. We demonstrate that the tradeoff observed in our model at steady state aligns closely with theoretical findings, validating our use of the mean field approximation. Our second main result is a rigorous equilibrium analysis of the Longest Chain Rule (LCR). We show that the LCR is a mean field equilibrium and that it is uniquely optimal in maximizing PoW efficiency under certain mild assumptions. This result provides the first formal justification for continued use of the LCR in decentralized consensus protocols, offering both theoretical validation and practical insights. Beyond these core results, our framework supports flexible experimentation with alternative block selection strategies, system dynamics, and reward structures. It offers a systematic and scalable substitute for expensive test-net deployments or ad hoc analysis. While our primary focus is on Nakamoto-style blockchains, the model is general enough to accommodate other architectures through modifications to the underlying MDP.",
  "authors": [
    "Yanni Georghiades",
    "Takashi Tanaka",
    "Sriram Vishwanath"
  ],
  "affiliations": [],
  "date_published": "2026-01-08",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.GT"
  ],
  "subjects": [
    "cs.GT"
  ],
  "keywords": [
    "blockchain"
  ],
  "url": "http://arxiv.org/abs/2601.05417v1",
  "pdf_url": "https://arxiv.org/pdf/2601.05417v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:11.063000",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.04583v1",
  "source": "arxiv",
  "doi": "",
  "title": "Autonomous Agents on Blockchains: Standards, Execution Models, and Trust Boundaries",
  "abstract": "Advances in large language models have enabled agentic AI systems that can reason, plan, and interact with external tools to execute multi-step workflows, while public blockchains have evolved into a programmable substrate for value transfer, access control, and verifiable state transitions. Their convergence introduces a high-stakes systems challenge: designing standard, interoperable, and secure interfaces that allow agents to observe on-chain state, formulate transaction intents, and authorize execution without exposing users, protocols, or organizations to unacceptable security, governance, or economic risks. This survey systematizes the emerging landscape of agent-blockchain interoperability through a systematic literature review, identifying 317 relevant works from an initial pool of over 3000 records. We contribute a five-part taxonomy of integration patterns spanning read-only analytics, simulation and intent generation, delegated execution, autonomous signing, and multi-agent workflows; a threat model tailored to agent-driven transaction pipelines that captures risks ranging from prompt injection and policy misuse to key compromise, adversarial execution dynamics, and multi-agent collusion; and a comparative capability matrix analyzing more than 20 representative systems across 13 dimensions, including custody models, permissioning, policy enforcement, observability, and recovery. Building on the gaps revealed by this analysis, we outline a research roadmap centered on two interface abstractions: a Transaction Intent Schema for portable and unambiguous goal specification, and a Policy Decision Record for auditable, verifiable policy enforcement across execution environments. We conclude by proposing a reproducible evaluation suite and benchmarks for assessing the safety, reliability, and economic robustness of agent-mediated on-chain execution.",
  "authors": [
    "Saad Alqithami"
  ],
  "affiliations": [],
  "date_published": "2026-01-08",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.AI",
    "cs.MA"
  ],
  "subjects": [
    "cs.AI",
    "cs.MA"
  ],
  "keywords": [
    "blockchain"
  ],
  "url": "http://arxiv.org/abs/2601.04583v1",
  "pdf_url": "https://arxiv.org/pdf/2601.04583v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:11.065802",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.03390v1",
  "source": "arxiv",
  "doi": "",
  "title": "Revisiting Speculative Leaderless Protocols for Low-Latency BFT Replication",
  "abstract": "As Byzantine Fault Tolerant (BFT) protocols begin to be used in permissioned blockchains for user-facing applications such as payments, it is crucial that they provide low latency. In pursuit of low latency, some recently proposed BFT consensus protocols employ a leaderless optimistic fast path, in which clients broadcast their requests directly to replicas without first serializing requests at a leader, resulting in an end-to-end commit latency of 2 message delays ($2Δ$) during fault-free, synchronous periods. However, such a fast path only works if there is no contention: concurrent contending requests can cause replicas to diverge if they receive conflicting requests in different orders, triggering costly recovery procedures. In this work, we present Aspen, a leaderless BFT protocol that achieves a near-optimal latency of $2Δ+ \\varepsilon$, where $\\varepsilon$ indicates a short waiting delay. Aspen removes the no-contention condition by utilizing a best-effort sequencing layer based on loosely synchronized clocks and network delay estimates. Aspen requires $n = 3f + 2p + 1$ replicas to cope with up to $f$ Byzantine nodes. The $2p$ extra nodes allow Aspen's fast path to proceed even if up to $p$ replicas diverge due to unpredictable network delays. When its optimistic conditions do not hold, Aspen falls back to PBFT-style protocol, guaranteeing safety and liveness under partial synchrony. In experiments with wide-area distributed replicas, Aspen commits requests in less than 75 ms, a 1.2 to 3.3$\\times$ improvement compared to previous protocols, while supporting 19,000 requests per second.",
  "authors": [
    "Daniel Qian",
    "Xiyu Hao",
    "Jinkun Geng",
    "Yuncheng Yao",
    "Aurojit Panda",
    "Jinyang Li",
    "Anirudh Sivaraman"
  ],
  "affiliations": [],
  "date_published": "2026-01-06",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.DC"
  ],
  "subjects": [
    "cs.DC"
  ],
  "keywords": [
    "blockchain"
  ],
  "url": "http://arxiv.org/abs/2601.03390v1",
  "pdf_url": "https://arxiv.org/pdf/2601.03390v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:11.068591",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.02984v1",
  "source": "arxiv",
  "doi": "",
  "title": "Selfish Mining in Multi-Attacker Scenarios: An Empirical Evaluation of Nakamoto, Fruitchain, and Strongchain",
  "abstract": "The aim of this work is to enhance blockchain security by deepening the understanding of selfish mining attacks in various consensus protocols, especially the ones that have the potential to mitigate selfish mining. Previous research was mainly focused on a particular protocol with a single selfish miner, while only limited studies have been conducted on two or more attackers. To address this gap, we proposed a stochastic simulation framework that enables analysis of selfish mining with multiple attackers across various consensus protocols. We created the model of Proof-of-Work (PoW) Nakamoto consensus (serving as the baseline) as well as models of two additional consensus protocols designed to mitigate selfish mining: Fruitchain and Strongchain. Using our framework, thresholds reported in the literature were verified, and several novel thresholds were discovered for 2 and more attackers. We made the source code of our framework available, enabling researchers to evaluate any newly added protocol with one or more selfish miners and cross-compare it with already modeled protocols.",
  "authors": [
    "Martin Perešíni",
    "Tomáš Hladký",
    "Jakub Kubík",
    "Ivan Homoliak"
  ],
  "affiliations": [],
  "date_published": "2026-01-06",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CR"
  ],
  "subjects": [
    "cs.CR"
  ],
  "keywords": [
    "blockchain"
  ],
  "url": "http://arxiv.org/abs/2601.02984v1",
  "pdf_url": "https://arxiv.org/pdf/2601.02984v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:11.070505",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.02949v1",
  "source": "arxiv",
  "doi": "",
  "title": "Exploring Blockchain Interoperability: Frameworks, Use Cases, and Future Challenges",
  "abstract": "Trust between entities in any scenario without a trusted third party is very difficult, and trust is exactly what blockchain aims to bring into the digital world with its basic features. Many applications are moving to blockchain adoption, enabling users to work in a trustworthy manner. The early generations of blockchain have a problem; they cannot share information with other blockchains. As more and more entities move their applications to the blockchain, they generate large volumes of data, and as applications have become more complex, sharing information between different blockchains has become a necessity. This has led to the research and development of interoperable solutions allowing blockchains to connect together. This paper discusses a few blockchain platforms that provide interoperable solutions, emphasising their ability to connect heterogeneous blockchains. It also discusses a case study scenario to illustrate the importance and benefits of using interoperable solutions. We also present a few topics that need to be solved in the realm of interoperability.",
  "authors": [
    "Stanly Wilson",
    "Kwabena Adu-Duodu",
    "Yinhao Li",
    "Ellis Solaiman",
    "Omer Rana",
    "Rajiv Ranjan"
  ],
  "affiliations": [],
  "date_published": "2026-01-06",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CR",
    "cs.DC"
  ],
  "subjects": [
    "cs.CR",
    "cs.DC"
  ],
  "keywords": [
    "blockchain"
  ],
  "url": "http://arxiv.org/abs/2601.02949v1",
  "pdf_url": "https://arxiv.org/pdf/2601.02949v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:11.072632",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.02804v1",
  "source": "arxiv",
  "doi": "",
  "title": "Distributionally Robust Game for Proof-of-Work Blockchain Mining Under Resource Uncertainties",
  "abstract": "Blockchain plays a crucial role in ensuring the security and integrity of decentralized systems, with the proof-of-work (PoW) mechanism being fundamental for achieving distributed consensus. As PoW blockchains see broader adoption, an increasingly diverse set of miners with varying computing capabilities participate in the network. In this paper, we consider the PoW blockchain mining, where the miners are associated with resource uncertainties. To characterize the uncertainty computing resources at different mining participants, we establish an ambiguous set representing uncertainty of resource distributions. Then, the networked mining is formulated as a non-cooperative game, where distributionally robust performance is calculated for each individual miner to tackle the resource uncertainties. We prove the existence of the equilibrium of the distributionally robust mining game. To derive the equilibrium, we propose the conditional value-at-risk (CVaR)-based reinterpretation of the best response of each miner. We then solve the individual strategy with alternating optimization, which facilitates the iteration among miners towards the game equilibrium. Furthermore, we consider the case that the ambiguity of resource distribution reduces to Gaussian distribution and the case that another uncertainties vanish, and then characterize the properties of the equilibrium therein along with a distributed algorithm to achieve the equilibrium. Simulation results show that the proposed approaches effectively converge to the equilibrium, and effectively tackle the uncertainties in blockchain mining to achieve a robust performance guarantee.",
  "authors": [
    "Xunqiang Lan",
    "Xiao Tang",
    "Ruonan Zhang",
    "Bin Li",
    "Qinghe Du",
    "Dusit Niyato",
    "Zhu Han"
  ],
  "affiliations": [],
  "date_published": "2026-01-06",
  "year": 2026,
  "month": 1,
  "categories": [
    "eess.SY"
  ],
  "subjects": [
    "eess.SY"
  ],
  "keywords": [
    "blockchain"
  ],
  "url": "http://arxiv.org/abs/2601.02804v1",
  "pdf_url": "https://arxiv.org/pdf/2601.02804v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:11.076004",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.02720v1",
  "source": "arxiv",
  "doi": "",
  "title": "Privacy-Preserving AI-Enabled Decentralized Learning and Employment Records System",
  "abstract": "Learning and Employment Record (LER) systems are emerging as critical infrastructure for securely compiling and sharing educational and work achievements. Existing blockchain-based platforms leverage verifiable credentials but typically lack automated skill-credential generation and the ability to incorporate unstructured evidence of learning. In this paper,a privacy-preserving, AI-enabled decentralized LER system is proposed to address these gaps. Digitally signed transcripts from educational institutions are accepted, and verifiable self-issued skill credentials are derived inside a trusted execution environment (TEE) by a natural language processing pipeline that analyzes formal records (e.g., transcripts, syllabi) and informal artifacts. All verification and job-skill matching are performed inside the enclave with selective disclosure, so raw credentials and private keys remain enclave-confined. Job matching relies solely on attested skill vectors and is invariant to non-skill resume fields, thereby reducing opportunities for screening bias.The NLP component was evaluated on sample learner data; the mapping follows the validated Syllabus-to-O*NET methodology,and a stability test across repeated runs observed <5% variance in top-ranked skills. Formal security statements and proof sketches are provided showing that derived credentials are unforgeable and that sensitive information remains confidential. The proposed system thus supports secure education and employment credentialing, robust transcript verification,and automated, privacy-preserving skill extraction within a decentralized framework.",
  "authors": [
    "Yuqiao Xu",
    "Mina Namazi",
    "Sahith Reddy Jalapally",
    "Osama Zafar",
    "Youngjin Yoo",
    "Erman Ayday"
  ],
  "affiliations": [],
  "date_published": "2026-01-06",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CR",
    "cs.AI"
  ],
  "subjects": [
    "cs.CR",
    "cs.AI"
  ],
  "keywords": [
    "blockchain"
  ],
  "url": "http://arxiv.org/abs/2601.02720v1",
  "pdf_url": "https://arxiv.org/pdf/2601.02720v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:11.078220",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.01706v1",
  "source": "arxiv",
  "doi": "",
  "title": "Semantic Non-Fungibility and Violations of the Law of One Price in Prediction Markets",
  "abstract": "Prediction markets are designed to aggregate dispersed information about future events, yet today's ecosystem is fragmented across heterogeneous operator-run platforms and blockchain-based protocols that independently list economically identical events. In the absence of a shared notion of event identity, liquidity fails to pool across venues, arbitrage becomes capital-intensive or unenforceable, and prices systematically violate the Law of One Price. As a result, market prices reflect platform-local beliefs rather than a single, globally aggregated probability, undermining the core information-aggregation function of prediction markets. We address this gap by introducing a semantic alignment framework that makes cross-platform event identity explicit through joint analysis of natural-language descriptions, resolution semantics, and temporal scope. Applying this framework, we construct the first human-validated, cross-platform dataset of aligned prediction markets, covering over 100 000 events across ten major venues from 2018 to 2025. Using this dataset, we show that roughly 6% of all events are concurrently listed across platforms and that semantically equivalent markets exhibit persistent execution-aware price deviations of 2-4% on average, even in highly liquid and information-rich settings. These mispricings give rise to persistent cross-platform arbitrage opportunities driven by structural frictions rather than informational disagreement. Overall, our results demonstrate that semantic non-fungibility is a fundamental barrier to price convergence, and that resolving event identity is a prerequisite for prediction markets to aggregate information at a global scale.",
  "authors": [
    "Jonas Gebele",
    "Florian Matthes"
  ],
  "affiliations": [],
  "date_published": "2026-01-05",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CE"
  ],
  "subjects": [
    "cs.CE"
  ],
  "keywords": [
    "blockchain"
  ],
  "url": "http://arxiv.org/abs/2601.01706v1",
  "pdf_url": "https://arxiv.org/pdf/2601.01706v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:11.080100",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.01436v1",
  "source": "arxiv",
  "doi": "",
  "title": "Bithoven: Formal Safety for Expressive Bitcoin Smart Contracts",
  "abstract": "The rigorous security model of Bitcoin's UTXO architecture often comes at the cost of developer usability, forcing a reliance on manual stack manipulation that leads to critical financial vulnerabilities like signature malleability, unspendable states and unconstrained execution paths. Industry standards such as Miniscript provide necessary abstractions for policy verification but do not model the full imperative logic required for complex contracts, leaving gaps in state management and resource liveness. This paper introduces Bithoven, a high-level language designed to bridge the gap between expressiveness and formal safety. By integrating a strict type checker and a resource liveness analyzer with a semantic control-flow analyzer, Bithoven eliminates major categories of consensus and logic defects defined in our fault model prior to deployment. Our results indicate that this safety comes at modest cost: Bithoven compiles to Bitcoin Script with efficiency comparable to hand-optimized code, demonstrating that type-safe, developer-friendly abstractions are viable even within the strict byte-size constraints of the Bitcoin blockchain.",
  "authors": [
    "Hyunhum Cho",
    "Ik Rae Jeong"
  ],
  "affiliations": [],
  "date_published": "2026-01-04",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CR",
    "cs.PL"
  ],
  "subjects": [
    "cs.CR",
    "cs.PL"
  ],
  "keywords": [
    "blockchain"
  ],
  "url": "http://arxiv.org/abs/2601.01436v1",
  "pdf_url": "https://arxiv.org/pdf/2601.01436v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:11.081758",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08831v1",
  "source": "arxiv",
  "doi": "",
  "title": "3AM: Segment Anything with Geometric Consistency in Videos",
  "abstract": "Video object segmentation methods like SAM2 achieve strong performance through memory-based architectures but struggle under large viewpoint changes due to reliance on appearance features. Traditional 3D instance segmentation methods address viewpoint consistency but require camera poses, depth maps, and expensive preprocessing. We introduce 3AM, a training-time enhancement that integrates 3D-aware features from MUSt3R into SAM2. Our lightweight Feature Merger fuses multi-level MUSt3R features that encode implicit geometric correspondence. Combined with SAM2's appearance features, the model achieves geometry-consistent recognition grounded in both spatial position and visual similarity. We propose a field-of-view aware sampling strategy ensuring frames observe spatially consistent object regions for reliable 3D correspondence learning. Critically, our method requires only RGB input at inference, with no camera poses or preprocessing. On challenging datasets with wide-baseline motion (ScanNet++, Replica), 3AM substantially outperforms SAM2 and extensions, achieving 90.6% IoU and 71.7% Positive IoU on ScanNet++'s Selected Subset, improving over state-of-the-art VOS methods by +15.9 and +30.4 points. Project page: https://jayisaking.github.io/3AM-Page/",
  "authors": [
    "Yang-Che Sun",
    "Cheng Sun",
    "Chin-Yang Lin",
    "Fu-En Yang",
    "Min-Hung Chen",
    "Yen-Yu Lin",
    "Yu-Lun Liu"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CV"
  ],
  "subjects": [
    "cs.CV"
  ],
  "keywords": [
    "deep learning"
  ],
  "url": "http://arxiv.org/abs/2601.08831v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08831v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:12.137207",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08828v1",
  "source": "arxiv",
  "doi": "",
  "title": "Motion Attribution for Video Generation",
  "abstract": "Despite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present Motive (MOTIon attribution for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. We use this to study which fine-tuning clips improve or degrade temporal dynamics. Motive isolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation. On text-to-video models, Motive identifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility. With Motive-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model. To our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate fine-tuning data.",
  "authors": [
    "Xindi Wu",
    "Despoina Paschalidou",
    "Jun Gao",
    "Antonio Torralba",
    "Laura Leal-Taixé",
    "Olga Russakovsky",
    "Sanja Fidler",
    "Jonathan Lorraine"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CV",
    "cs.AI",
    "cs.LG",
    "cs.MM",
    "cs.RO"
  ],
  "subjects": [
    "cs.CV",
    "cs.AI",
    "cs.LG",
    "cs.MM",
    "cs.RO"
  ],
  "keywords": [
    "deep learning"
  ],
  "url": "http://arxiv.org/abs/2601.08828v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08828v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:12.142827",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08813v1",
  "source": "arxiv",
  "doi": "",
  "title": "An Optimal Observable Machine for reinterpretable measurements in high-energy physics",
  "abstract": "A machine-learning-based framework for constructing generator-level observables optimized for parameter extraction in particle physics analyses is introduced, referred to as the Optimal Observable Machine (OOM). Unfoldable differential distributions are learned that maximize sensitivity to a parameter of interest while remaining robust against detector effects, systematic uncertainties, and biases introduced by the unfolding procedure. Detector response and systematic uncertainties are explicitly incorporated into the training through a likelihood-based loss function, enabling a direct optimization of the expected measurement precision while minimizing the bias from any assumption on the parameter of interest itself. The approach is demonstrated in an application to top quark physics, focusing on the measurement of a recently observed pseudoscalar excess at the top quark pair production threshold in dilepton final states. It is shown that a generator-level observable with enhanced sensitivity and long-term reinterpretability can be constructed using this method.",
  "authors": [
    "Torben Mohr",
    "Alejandro Quiroga Triviño",
    "Fabian Riemer",
    "Artur Monsch",
    "Matteo Defranchis",
    "Joscha Knolle",
    "Ankita Mehta",
    "Jan Kieseler",
    "Markus Klute"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "hep-ph",
    "hep-ex"
  ],
  "subjects": [
    "hep-ph",
    "hep-ex"
  ],
  "keywords": [
    "deep learning"
  ],
  "url": "http://arxiv.org/abs/2601.08813v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08813v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:12.145535",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08808v1",
  "source": "arxiv",
  "doi": "",
  "title": "Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge",
  "abstract": "Large language models often solve complex reasoning tasks more effectively with Chain-of-Thought (CoT), but at the cost of long, low-bandwidth token sequences. Humans, by contrast, often reason softly by maintaining a distribution over plausible next steps. Motivated by this, we propose Multiplex Thinking, a stochastic soft reasoning mechanism that, at each thinking step, samples K candidate tokens and aggregates their embeddings into a single continuous multiplex token. This preserves the vocabulary embedding prior and the sampling dynamics of standard discrete generation, while inducing a tractable probability distribution over multiplex rollouts. Consequently, multiplex trajectories can be directly optimized with on-policy reinforcement learning (RL). Importantly, Multiplex Thinking is self-adaptive: when the model is confident, the multiplex token is nearly discrete and behaves like standard CoT; when it is uncertain, it compactly represents multiple plausible next steps without increasing sequence length. Across challenging math reasoning benchmarks, Multiplex Thinking consistently outperforms strong discrete CoT and RL baselines from Pass@1 through Pass@1024, while producing shorter sequences. The code and checkpoints are available at https://github.com/GMLR-Penn/Multiplex-Thinking.",
  "authors": [
    "Yao Tang",
    "Li Dong",
    "Yaru Hao",
    "Qingxiu Dong",
    "Furu Wei",
    "Jiatao Gu"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ],
  "subjects": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ],
  "keywords": [
    "deep learning"
  ],
  "url": "http://arxiv.org/abs/2601.08808v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08808v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:12.148542",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08803v1",
  "source": "arxiv",
  "doi": "",
  "title": "Identifying Latent Intentions via Inverse Reinforcement Learning in Repeated Linear Public Good Games",
  "abstract": "Behavior in repeated public goods games continues to challenge standard theory: heterogeneous social preferences can explain first-round contributions, but not the substantial volatility observed across repeated interactions. Using 50,390 decisions from 2,938 participants, we introduce two methodological advances to address this gap. First, we cluster behavioral trajectories by their temporal shape using Dynamic Time Warping, yielding distinct and theoretically interpretable behavioral types. Second, we apply a hierarchical inverse Q-learning framework that models decisions as discrete switches between latent cooperative and defective intentions. This approach reveals a large (21.4%) and previously unmodeled behavioral type -- Switchers -- who frequently reverse intentions rather than commit to stable strategies. At the same time, the framework recovers canonical strategic behaviors such as persistent cooperation and free-riding. Substantively, recognizing intentional volatility helps sustain cooperation: brief defections by Switchers often reverse, so strategic patience can prevent unnecessary breakdowns.",
  "authors": [
    "Carina I. Hausladen",
    "Marcel H. Schubert",
    "Christoph Engel"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "econ.GN"
  ],
  "subjects": [
    "econ.GN"
  ],
  "keywords": [
    "deep learning"
  ],
  "url": "http://arxiv.org/abs/2601.08803v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08803v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:12.151395",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08798v1",
  "source": "arxiv",
  "doi": "",
  "title": "Near-perfect photo-ID of the Hula painted frog with zero-shot deep local-feature matching",
  "abstract": "Accurate individual identification is essential for monitoring rare amphibians, yet invasive marking is often unsuitable for critically endangered species. We evaluate state-of-the-art computer-vision methods for photographic re-identification of the Hula painted frog (Latonia nigriventer) using 1,233 ventral images from 191 individuals collected during 2013-2020 capture-recapture surveys. We compare deep local-feature matching in a zero-shot setting with deep global-feature embedding models. The local-feature pipeline achieves 98% top-1 closed-set identification accuracy, outperforming all global-feature models; fine-tuning improves the best global-feature model to 60% top-1 (91% top-10) but remains below local matching. To combine scalability with accuracy, we implement a two-stage workflow in which a fine-tuned global-feature model retrieves a short candidate list that is re-ranked by local-feature matching, reducing end-to-end runtime from 6.5-7.8 hours to ~38 minutes while maintaining ~96% top-1 closed-set accuracy on the labeled dataset. Separation of match scores between same- and different-individual pairs supports thresholding for open-set identification, enabling practical handling of novel individuals. We deploy this pipeline as a web application for routine field use, providing rapid, standardized, non-invasive identification to support conservation monitoring and capture-recapture analyses. Overall, in this species, zero-shot deep local-feature matching outperformed global-feature embedding and provides a strong default for photo-identification.",
  "authors": [
    "Maayan Yesharim",
    "R. G. Bina Perl",
    "Uri Roll",
    "Sarig Gafny",
    "Eli Geffen",
    "Yoav Ram"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CV",
    "q-bio.QM"
  ],
  "subjects": [
    "cs.CV",
    "q-bio.QM"
  ],
  "keywords": [
    "deep learning"
  ],
  "url": "http://arxiv.org/abs/2601.08798v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08798v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:12.153323",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08797v1",
  "source": "arxiv",
  "doi": "",
  "title": "DentalX: Context-Aware Dental Disease Detection with Radiographs",
  "abstract": "Diagnosing dental diseases from radiographs is time-consuming and challenging due to the subtle nature of diagnostic evidence. Existing methods, which rely on object detection models designed for natural images with more distinct target patterns, struggle to detect dental diseases that present with far less visual support. To address this challenge, we propose {\\bf DentalX}, a novel context-aware dental disease detection approach that leverages oral structure information to mitigate the visual ambiguity inherent in radiographs. Specifically, we introduce a structural context extraction module that learns an auxiliary task: semantic segmentation of dental anatomy. The module extracts meaningful structural context and integrates it into the primary disease detection task to enhance the detection of subtle dental diseases. Extensive experiments on a dedicated benchmark demonstrate that DentalX significantly outperforms prior methods in both tasks. This mutual benefit arises naturally during model optimization, as the correlation between the two tasks is effectively captured. Our code is available at https://github.com/zhiqin1998/DentYOLOX.",
  "authors": [
    "Zhi Qin Tan",
    "Xiatian Zhu",
    "Owen Addison",
    "Yunpeng Li"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CV"
  ],
  "subjects": [
    "cs.CV"
  ],
  "keywords": [
    "deep learning"
  ],
  "url": "http://arxiv.org/abs/2601.08797v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08797v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:12.155807",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08790v1",
  "source": "arxiv",
  "doi": "",
  "title": "Aggregating Diverse Cue Experts for AI-Generated Image Detection",
  "abstract": "The rapid emergence of image synthesis models poses challenges to the generalization of AI-generated image detectors. However, existing methods often rely on model-specific features, leading to overfitting and poor generalization. In this paper, we introduce the Multi-Cue Aggregation Network (MCAN), a novel framework that integrates different yet complementary cues in a unified network. MCAN employs a mixture-of-encoders adapter to dynamically process these cues, enabling more adaptive and robust feature representation. Our cues include the input image itself, which represents the overall content, and high-frequency components that emphasize edge details. Additionally, we introduce a Chromatic Inconsistency (CI) cue, which normalizes intensity values and captures noise information introduced during the image acquisition process in real images, making these noise patterns more distinguishable from those in AI-generated content. Unlike prior methods, MCAN's novelty lies in its unified multi-cue aggregation framework, which integrates spatial, frequency-domain, and chromaticity-based information for enhanced representation learning. These cues are intrinsically more indicative of real images, enhancing cross-model generalization. Extensive experiments on the GenImage, Chameleon, and UniversalFakeDetect benchmark validate the state-of-the-art performance of MCAN. In the GenImage dataset, MCAN outperforms the best state-of-the-art method by up to 7.4% in average ACC across eight different image generators.",
  "authors": [
    "Lei Tan",
    "Shuwei Li",
    "Mohan Kankanhalli",
    "Robby T. Tan"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CV"
  ],
  "subjects": [
    "cs.CV"
  ],
  "keywords": [
    "deep learning"
  ],
  "url": "http://arxiv.org/abs/2601.08790v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08790v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:12.157820",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08784v1",
  "source": "arxiv",
  "doi": "",
  "title": "On the use of graph models to achieve individual and group fairness",
  "abstract": "Machine Learning algorithms are ubiquitous in key decision-making contexts such as justice, healthcare and finance, which has spawned a great demand for fairness in these procedures. However, the theoretical properties of such models in relation with fairness are still poorly understood, and the intuition behind the relationship between group and individual fairness is still lacking. In this paper, we provide a theoretical framework based on Sheaf Diffusion to leverage tools based on dynamical systems and homology to model fairness. Concretely, the proposed method projects input data into a bias-free space that encodes fairness constrains, resulting in fair solutions. Furthermore, we present a collection of network topologies handling different fairness metrics, leading to a unified method capable of dealing with both individual and group bias. The resulting models have a layer of interpretability in the form of closed-form expressions for their SHAP values, consolidating their place in the responsible Artificial Intelligence landscape. Finally, these intuitions are tested on a simulation study and standard fairness benchmarks, where the proposed methods achieve satisfactory results. More concretely, the paper showcases the performance of the proposed models in terms of accuracy and fairness, studying available trade-offs on the Pareto frontier, checking the effects of changing the different hyper-parameters, and delving into the interpretation of its outputs.",
  "authors": [
    "Arturo Pérez-Peralta",
    "Sandra Benítez-Peña",
    "Rosa E. Lillo"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "stat.ML",
    "cs.CY",
    "cs.LG"
  ],
  "subjects": [
    "stat.ML",
    "cs.CY",
    "cs.LG"
  ],
  "keywords": [
    "deep learning"
  ],
  "url": "http://arxiv.org/abs/2601.08784v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08784v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:12.159581",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08781v1",
  "source": "arxiv",
  "doi": "",
  "title": "Fast and explainable clustering in the Manhattan and Tanimoto distance",
  "abstract": "The CLASSIX algorithm is a fast and explainable approach to data clustering. In its original form, this algorithm exploits the sorting of the data points by their first principal component to truncate the search for nearby data points, with nearness being defined in terms of the Euclidean distance. Here we extend CLASSIX to other distance metrics, including the Manhattan distance and the Tanimoto distance. Instead of principal components, we use an appropriate norm of the data vectors as the sorting criterion, combined with the triangle inequality for search termination. In the case of Tanimoto distance, a provably sharper intersection inequality is used to further boost the performance of the new algorithm. On a real-world chemical fingerprint benchmark, CLASSIX Tanimoto is about 30 times faster than the Taylor--Butina algorithm, and about 80 times faster than DBSCAN, while computing higher-quality clusters in both cases.",
  "authors": [
    "Stefan Güttel",
    "Kaustubh Roy"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.LG"
  ],
  "subjects": [
    "cs.LG"
  ],
  "keywords": [
    "deep learning"
  ],
  "url": "http://arxiv.org/abs/2601.08781v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08781v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:12.161409",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08780v1",
  "source": "arxiv",
  "doi": "",
  "title": "LWM-Spectro: A Foundation Model for Wireless Baseband Signal Spectrograms",
  "abstract": "The received in-phase and quadrature (I/Q) baseband signals inherently encode physical-layer and channel characteristics of wireless links. Learning robust and transferable representations directly from such raw signals, however, remains challenging due to heterogeneous communication systems, diverse propagation environments, and limited labeled data. To address this, we present LWM-Spectro, a transformer-based foundation model pretrained on large-scale I/Q data represented as time-frequency spectrograms. The model leverages self-supervised masked modeling, contrastive learning, and a mixture-of-experts (MoE) architecture to learn general-purpose wireless representations. These representations transfer effectively to downstream tasks such as modulation classification and joint SNR/mobility recognition, even with minimal supervision. Across tasks, LWM-Spectro consistently outperforms state-of-the-art deep learning baselines in both few-shot and data-rich regimes, providing a unified foundation for wireless learning.",
  "authors": [
    "Namhyun Kim",
    "Sadjad Alikhani",
    "Ahmed Alkhateeb"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.IT",
    "eess.SP"
  ],
  "subjects": [
    "cs.IT",
    "eess.SP"
  ],
  "keywords": [
    "deep learning"
  ],
  "url": "http://arxiv.org/abs/2601.08780v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08780v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:12.162997",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08777v1",
  "source": "arxiv",
  "doi": "",
  "title": "Asymptotic Universal Alignment: A New Alignment Framework via Test-Time Scaling",
  "abstract": "Aligning large language models (LLMs) to serve users with heterogeneous and potentially conflicting preferences is a central challenge for personalized and trustworthy AI. We formalize an ideal notion of universal alignment through test-time scaling: for each prompt, the model produces $k\\ge 1$ candidate responses and a user selects their preferred one. We introduce $(k,f(k))$-robust alignment, which requires the $k$-output model to have win rate $f(k)$ against any other single-output model, and asymptotic universal alignment (U-alignment), which requires $f(k)\\to 1$ as $k\\to\\infty$. Our main result characterizes the optimal convergence rate: there exists a family of single-output policies whose $k$-sample product policies achieve U-alignment at rate $f(k)=\\frac{k}{k+1}$, and no method can achieve a faster rate in general. We show that popular post-training methods, including Nash learning from human feedback (NLHF), can fundamentally underutilize the benefits of test-time scaling. Even though NLHF is optimal for $k=1$, sampling from the resulting (often deterministic) policy cannot guarantee win rates above $\\tfrac{1}{2}$ except for an arbitrarily small slack. This stems from a lack of output diversity: existing alignment methods can collapse to a single majority-preferred response, making additional samples redundant. In contrast, our approach preserves output diversity and achieves the optimal test-time scaling rate. In particular, we propose a family of symmetric multi-player alignment games and prove that any symmetric Nash equilibrium policy of the $(k+1)$-player alignment game achieves the optimal $(k,\\frac{k}{k+1})$-robust alignment. Finally, we provide theoretical convergence guarantees for self-play learning dynamics in these games and extend the framework to opponents that also generate multiple responses.",
  "authors": [
    "Yang Cai",
    "Weiqiang Zheng"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.CL",
    "cs.GT"
  ],
  "subjects": [
    "cs.LG",
    "cs.AI",
    "cs.CL",
    "cs.GT"
  ],
  "keywords": [
    "deep learning"
  ],
  "url": "http://arxiv.org/abs/2601.08777v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08777v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:12.164651",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08776v1",
  "source": "arxiv",
  "doi": "",
  "title": "Translating Light-Sheet Microscopy Images to Virtual H&E Using CycleGAN",
  "abstract": "Histopathology analysis relies on Hematoxylin and Eosin (H&E) staining, but fluorescence microscopy offers complementary information. Converting fluorescence images to H&E-like appearance can aid interpretation and integration with standard workflows. We present a Cycle-Consistent Adversarial Network (CycleGAN) approach for unpaired image-to-image translation from multi-channel fluorescence microscopy to pseudo H&E stained histopathology images. The method combines C01 and C02 fluorescence channels into RGB and learns a bidirectional mapping between fluorescence and H&E domains without paired training data. The architecture uses ResNet-based generators with residual blocks and PatchGAN discriminators, trained with adversarial, cycle-consistency, and identity losses. Experiments on fluorescence microscopy datasets show the model generates realistic pseudo H&E images that preserve morphological structures while adopting H&E-like color characteristics. This enables visualization of fluorescence data in a format familiar to pathologists and supports integration with existing H&E-based analysis pipelines.",
  "authors": [
    "Yanhua Zhao"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CV",
    "cs.AI"
  ],
  "subjects": [
    "cs.CV",
    "cs.AI"
  ],
  "keywords": [
    "deep learning"
  ],
  "url": "http://arxiv.org/abs/2601.08776v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08776v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:12.166284",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08772v1",
  "source": "arxiv",
  "doi": "",
  "title": "Enhancing classical simulation with noisy quantum devices",
  "abstract": "As quantum devices continue to improve in scale and precision, a central challenge is how to effectively utilize noisy hardware for meaningful computation. Most existing approaches aim to recover noiseless circuit outputs from noisy ones through error mitigation or correction. Here, we show that noisy quantum devices can be directly leveraged as computational resources to enhance the classical simulation of quantum circuits. We introduce the Noisy-device-enhanced Classical Simulation (NDE-CS) protocol, which improves stabilizer-based classical Monte Carlo simulation methods by incorporating data obtained from noisy quantum hardware. Specifically, NDE-CS uses noisy executions of a target circuit together with noisy Clifford circuits to learn how the target circuit can be expressed in terms of Clifford circuits under realistic noise. The same learned relation can then be reused in the noiseless Clifford limit, enabling accurate estimation of ideal expectation values with substantially reduced sampling cost. Numerical simulations on Trotterized Ising circuits demonstrate that NDE-CS achieves orders-of-magnitude reductions in sampling cost compared to the underlying purely classical Monte Carlo approaches from which it is derived, while maintaining the same accuracy. We also compare NDE-CS with Sparse Pauli Dynamics (SPD), a powerful classical framework capable of simulating quantum circuits at previously inaccessible scales, and provide an example where the cost of SPD scales exponentially with system size, while NDE-CS scales much more favorably. These results establish NDE-CS as a scalable hybrid simulation approach for quantum circuits, where noise can be harnessed as a computational asset.",
  "authors": [
    "Ruiqi Zhang",
    "Fuchuan Wei",
    "Zhaohui Wei"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "quant-ph"
  ],
  "subjects": [
    "quant-ph"
  ],
  "keywords": [
    "deep learning"
  ],
  "url": "http://arxiv.org/abs/2601.08772v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08772v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:12.167935",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08764v1",
  "source": "arxiv",
  "doi": "",
  "title": "FusID: Modality-Fused Semantic IDs for Generative Music Recommendation",
  "abstract": "Generative recommendation systems have achieved significant advances by leveraging semantic IDs to represent items. However, existing approaches that tokenize each modality independently face two critical limitations: (1) redundancy across modalities that reduces efficiency, and (2) failure to capture inter-modal interactions that limits item representation. We introduce FusID, a modality-fused semantic ID framework that addresses these limitations through three key components: (i) multimodal fusion that learns unified representations by jointly encoding information across modalities, (ii) representation learning that brings frequently co-occurring item embeddings closer while maintaining distinctiveness and preventing feature redundancy, and (iii) product quantization that converts the fused continuous embeddings into multiple discrete tokens to mitigate ID conflict. Evaluated on a multimodal next-song recommendation (i.e., playlist continuation) benchmark, FusID achieves zero ID conflicts, ensuring that each token sequence maps to exactly one song, mitigates codebook underutilization, and outperforms baselines in terms of MRR and Recall@k (k = 1, 5, 10, 20).",
  "authors": [
    "Haven Kim",
    "Yupeng Hou",
    "Julian McAuley"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.IR",
    "cs.SD",
    "eess.AS"
  ],
  "subjects": [
    "cs.IR",
    "cs.SD",
    "eess.AS"
  ],
  "keywords": [
    "deep learning"
  ],
  "url": "http://arxiv.org/abs/2601.08764v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08764v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:12.169540",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08763v1",
  "source": "arxiv",
  "doi": "",
  "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs",
  "abstract": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@$k$ across large sampling budgets and increases the area under the pass@$k$ curve (AUC@$K$) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.",
  "authors": [
    "Zhiyuan Hu",
    "Yucheng Wang",
    "Yufei He",
    "Jiaying Wu",
    "Yilun Zhao",
    "See-Kiong Ng",
    "Cynthia Breazeal",
    "Anh Tuan Luu",
    "Hae Won Park",
    "Bryan Hooi"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.LG",
    "cs.CL"
  ],
  "subjects": [
    "cs.LG",
    "cs.CL"
  ],
  "keywords": [
    "deep learning"
  ],
  "url": "http://arxiv.org/abs/2601.08763v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08763v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:12.171810",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08760v1",
  "source": "arxiv",
  "doi": "",
  "title": "Adaptive Requesting in Decentralized Edge Networks via Non-Stationary Bandits",
  "abstract": "We study a decentralized collaborative requesting problem that aims to optimize the information freshness of time-sensitive clients in edge networks consisting of multiple clients, access nodes (ANs), and servers. Clients request content through ANs acting as gateways, without observing AN states or the actions of other clients. We define the reward as the age of information reduction resulting from a client's selection of an AN, and formulate the problem as a non-stationary multi-armed bandit. In this decentralized and partially observable setting, the resulting reward process is history-dependent and coupled across clients, and exhibits both abrupt and gradual changes in expected rewards, rendering classical bandit-based approaches ineffective. To address these challenges, we propose the AGING BANDIT WITH ADAPTIVE RESET algorithm, which combines adaptive windowing with periodic monitoring to track evolving reward distributions. We establish theoretical performance guarantees showing that the proposed algorithm achieves near-optimal performance, and we validate the theoretical results through simulations.",
  "authors": [
    "Yi Zhuang",
    "Kun Yang",
    "Xingran Chen"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.LG",
    "cs.MA"
  ],
  "subjects": [
    "cs.LG",
    "cs.MA"
  ],
  "keywords": [
    "deep learning"
  ],
  "url": "http://arxiv.org/abs/2601.08760v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08760v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:12.174359",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08732v1",
  "source": "arxiv",
  "doi": "",
  "title": "ISLA: A U-Net for MRI-based acute ischemic stroke lesion segmentation with deep supervision, attention, domain adaptation, and ensemble learning",
  "abstract": "Accurate delineation of acute ischemic stroke lesions in MRI is a key component of stroke diagnosis and management. In recent years, deep learning models have been successfully applied to the automatic segmentation of such lesions. While most proposed architectures are based on the U-Net framework, they primarily differ in their choice of loss functions and in the use of deep supervision, residual connections, and attention mechanisms. Moreover, many implementations are not publicly available, and the optimal configuration for acute ischemic stroke (AIS) lesion segmentation remains unclear. In this work, we introduce ISLA (Ischemic Stroke Lesion Analyzer), a new deep learning model for AIS lesion segmentation from diffusion MRI, trained on three multicenter databases totaling more than 1500 AIS participants. Through systematic optimization of the loss function, convolutional architecture, deep supervision, and attention mechanisms, we developed a robust segmentation framework. We further investigated unsupervised domain adaptation to improve generalization to an external clinical dataset. ISLA outperformed two state-of-the-art approaches for AIS lesion segmentation on an external test set. Codes and trained models will be made publicly available to facilitate reuse and reproducibility.",
  "authors": [
    "Vincent Roca",
    "Martin Bretzner",
    "Hilde Henon",
    "Laurent Puy",
    "Grégory Kuchcinski",
    "Renaud Lopes"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CV",
    "cs.AI"
  ],
  "subjects": [
    "cs.CV",
    "cs.AI"
  ],
  "keywords": [
    "deep learning"
  ],
  "url": "http://arxiv.org/abs/2601.08732v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08732v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:12.177778",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08828v1",
  "source": "arxiv",
  "doi": "",
  "title": "Motion Attribution for Video Generation",
  "abstract": "Despite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present Motive (MOTIon attribution for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. We use this to study which fine-tuning clips improve or degrade temporal dynamics. Motive isolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation. On text-to-video models, Motive identifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility. With Motive-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model. To our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate fine-tuning data.",
  "authors": [
    "Xindi Wu",
    "Despoina Paschalidou",
    "Jun Gao",
    "Antonio Torralba",
    "Laura Leal-Taixé",
    "Olga Russakovsky",
    "Sanja Fidler",
    "Jonathan Lorraine"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CV",
    "cs.AI",
    "cs.LG",
    "cs.MM",
    "cs.RO"
  ],
  "subjects": [
    "cs.CV",
    "cs.AI",
    "cs.LG",
    "cs.MM",
    "cs.RO"
  ],
  "keywords": [
    "big data"
  ],
  "url": "http://arxiv.org/abs/2601.08828v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08828v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:13.274173",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08823v1",
  "source": "arxiv",
  "doi": "",
  "title": "Lattice-based equation of state with a critical point from constant entropy contours and its comparison to effective QCD approaches",
  "abstract": "In this work, we systematically assess the performance of a new method from [H. Shah et al., Phys. Rev. C 113, L012201] for locating the QCD critical point using constant-entropy contours by testing it against various effective QCD approaches. We demonstrate that, while the method yields spurious critical points in purely hadronic models (HRG) due to non-parabolic contour behavior at low temperatures ($T \\lesssim 120$ MeV), it accurately reproduces the CP location in frameworks that feature a genuine phase transition and benchmarked against lattice QCD, such as Holographic Einstein-Maxwell-Dilaton, and Functional QCD approaches. Building on our previous determination of constant entropy contours using lattice data, we extend that analysis to construct a complete Lattice-based Equation of State (EoS) at finite density, which features a critical point at $(T, μ_B) \\approx (114, 602)$ MeV. By integrating the extrapolated entropy density with respect to temperature, we reconstruct the pressure, baryon density, susceptibility, and speed of sound in the critical region, and analyze the focusing behavior of isentropic trajectories in the vicinity of the critical point.",
  "authors": [
    "Hitansh Shah",
    "Mauricio Hippert",
    "Jorge Noronha",
    "Claudia Ratti",
    "Volodymyr Vovchenko"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "hep-ph",
    "nucl-th"
  ],
  "subjects": [
    "hep-ph",
    "nucl-th"
  ],
  "keywords": [
    "big data"
  ],
  "url": "http://arxiv.org/abs/2601.08823v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08823v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:13.280311",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08811v1",
  "source": "arxiv",
  "doi": "",
  "title": "Reasoning Matters for 3D Visual Grounding",
  "abstract": "The recent development of Large Language Models (LLMs) with strong reasoning ability has driven research in various domains such as mathematics, coding, and scientific discovery. Meanwhile, 3D visual grounding, as a fundamental task in 3D understanding, still remains challenging due to the limited reasoning ability of recent 3D visual grounding models. Most of the current methods incorporate a text encoder and visual feature encoder to generate cross-modal fuse features and predict the referring object. These models often require supervised training on extensive 3D annotation data. On the other hand, recent research also focus on scaling synthetic data to train stronger 3D visual grounding LLM, however, the performance gain remains limited and non-proportional to the data collection cost. In this work, we propose a 3D visual grounding data pipeline, which is capable of automatically synthesizing 3D visual grounding data along with corresponding reasoning process. Additionally, we leverage the generated data for LLM fine-tuning and introduce Reason3DVG-8B, a strong 3D visual grounding LLM that outperforms previous LLM-based method 3D-GRAND using only 1.6% of their training data, demonstrating the effectiveness of our data and the importance of reasoning in 3D visual grounding.",
  "authors": [
    "Hsiang-Wei Huang",
    "Kuang-Ming Chen",
    "Wenhao Chai",
    "Cheng-Yen Yang",
    "Jen-Hao Cheng",
    "Jenq-Neng Hwang"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CV",
    "cs.AI"
  ],
  "subjects": [
    "cs.CV",
    "cs.AI"
  ],
  "keywords": [
    "big data"
  ],
  "url": "http://arxiv.org/abs/2601.08811v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08811v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:13.282369",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08799v1",
  "source": "arxiv",
  "doi": "",
  "title": "Collapse of statistical equilibrium in large-scale hydroelastic turbulent waves",
  "abstract": "At scales larger than the forcing scale, some out-of-equilibrium turbulent systems (such as hydrodynamic turbulence, wave turbulence, and nonlinear optics) exhibit a state of statistical equilibrium where energy is equipartitioned among large-scale modes, in line with the Rayleigh-Jeans spectrum. Key open questions now pertain to either the emergence, decay, collapse, or other nonstationary evolutions from this state. Here, we experimentally investigate the free decay of large-scale hydroelastic turbulent waves, initially in a regime of statistical equilibrium. Using space- and time-resolved measurements, we show that the total energy of these large-scale tensional waves decays as a power law in time. We derive an energy decay law from the theoretical initial equilibrium spectrum and the linear viscous damping, as no net energy flux is carried. Our prediction then shows a good agreement with experimental data over nearly two decades in time, for various initial effective temperatures of the statistical equilibrium state. We further identify the dissipation mechanism and confirm it experimentally. Our approach could be applied to other decaying turbulence systems, with the large scales initially in statistical equilibrium.",
  "authors": [
    "Marlone Vernet",
    "Eric Falcon"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "physics.flu-dyn",
    "cond-mat.stat-mech",
    "nlin.CD",
    "physics.ao-ph",
    "physics.geo-ph"
  ],
  "subjects": [
    "physics.flu-dyn",
    "cond-mat.stat-mech",
    "nlin.CD",
    "physics.ao-ph",
    "physics.geo-ph"
  ],
  "keywords": [
    "big data"
  ],
  "url": "http://arxiv.org/abs/2601.08799v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08799v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:13.284777",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08784v1",
  "source": "arxiv",
  "doi": "",
  "title": "On the use of graph models to achieve individual and group fairness",
  "abstract": "Machine Learning algorithms are ubiquitous in key decision-making contexts such as justice, healthcare and finance, which has spawned a great demand for fairness in these procedures. However, the theoretical properties of such models in relation with fairness are still poorly understood, and the intuition behind the relationship between group and individual fairness is still lacking. In this paper, we provide a theoretical framework based on Sheaf Diffusion to leverage tools based on dynamical systems and homology to model fairness. Concretely, the proposed method projects input data into a bias-free space that encodes fairness constrains, resulting in fair solutions. Furthermore, we present a collection of network topologies handling different fairness metrics, leading to a unified method capable of dealing with both individual and group bias. The resulting models have a layer of interpretability in the form of closed-form expressions for their SHAP values, consolidating their place in the responsible Artificial Intelligence landscape. Finally, these intuitions are tested on a simulation study and standard fairness benchmarks, where the proposed methods achieve satisfactory results. More concretely, the paper showcases the performance of the proposed models in terms of accuracy and fairness, studying available trade-offs on the Pareto frontier, checking the effects of changing the different hyper-parameters, and delving into the interpretation of its outputs.",
  "authors": [
    "Arturo Pérez-Peralta",
    "Sandra Benítez-Peña",
    "Rosa E. Lillo"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "stat.ML",
    "cs.CY",
    "cs.LG"
  ],
  "subjects": [
    "stat.ML",
    "cs.CY",
    "cs.LG"
  ],
  "keywords": [
    "big data"
  ],
  "url": "http://arxiv.org/abs/2601.08784v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08784v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:13.285952",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08781v1",
  "source": "arxiv",
  "doi": "",
  "title": "Fast and explainable clustering in the Manhattan and Tanimoto distance",
  "abstract": "The CLASSIX algorithm is a fast and explainable approach to data clustering. In its original form, this algorithm exploits the sorting of the data points by their first principal component to truncate the search for nearby data points, with nearness being defined in terms of the Euclidean distance. Here we extend CLASSIX to other distance metrics, including the Manhattan distance and the Tanimoto distance. Instead of principal components, we use an appropriate norm of the data vectors as the sorting criterion, combined with the triangle inequality for search termination. In the case of Tanimoto distance, a provably sharper intersection inequality is used to further boost the performance of the new algorithm. On a real-world chemical fingerprint benchmark, CLASSIX Tanimoto is about 30 times faster than the Taylor--Butina algorithm, and about 80 times faster than DBSCAN, while computing higher-quality clusters in both cases.",
  "authors": [
    "Stefan Güttel",
    "Kaustubh Roy"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.LG"
  ],
  "subjects": [
    "cs.LG"
  ],
  "keywords": [
    "big data"
  ],
  "url": "http://arxiv.org/abs/2601.08781v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08781v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:13.288964",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08780v1",
  "source": "arxiv",
  "doi": "",
  "title": "LWM-Spectro: A Foundation Model for Wireless Baseband Signal Spectrograms",
  "abstract": "The received in-phase and quadrature (I/Q) baseband signals inherently encode physical-layer and channel characteristics of wireless links. Learning robust and transferable representations directly from such raw signals, however, remains challenging due to heterogeneous communication systems, diverse propagation environments, and limited labeled data. To address this, we present LWM-Spectro, a transformer-based foundation model pretrained on large-scale I/Q data represented as time-frequency spectrograms. The model leverages self-supervised masked modeling, contrastive learning, and a mixture-of-experts (MoE) architecture to learn general-purpose wireless representations. These representations transfer effectively to downstream tasks such as modulation classification and joint SNR/mobility recognition, even with minimal supervision. Across tasks, LWM-Spectro consistently outperforms state-of-the-art deep learning baselines in both few-shot and data-rich regimes, providing a unified foundation for wireless learning.",
  "authors": [
    "Namhyun Kim",
    "Sadjad Alikhani",
    "Ahmed Alkhateeb"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.IT",
    "eess.SP"
  ],
  "subjects": [
    "cs.IT",
    "eess.SP"
  ],
  "keywords": [
    "big data"
  ],
  "url": "http://arxiv.org/abs/2601.08780v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08780v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:13.290716",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08778v1",
  "source": "arxiv",
  "doi": "",
  "title": "Pervasive Annotation Errors Break Text-to-SQL Benchmarks and Leaderboards",
  "abstract": "Researchers have proposed numerous text-to-SQL techniques to streamline data analytics and accelerate the development of database-driven applications. To compare these techniques and select the best one for deployment, the community depends on public benchmarks and their leaderboards. Since these benchmarks heavily rely on human annotations during question construction and answer evaluation, the validity of the annotations is crucial. In this paper, we conduct an empirical study that (i) benchmarks annotation error rates for two widely used text-to-SQL benchmarks, BIRD and Spider 2.0-Snow, and (ii) corrects a subset of the BIRD development (Dev) set to measure the impact of annotation errors on text-to-SQL agent performance and leaderboard rankings. Through expert analysis, we show that BIRD Mini-Dev and Spider 2.0-Snow have error rates of 52.8% and 62.8%, respectively. We re-evaluate all 16 open-source agents from the BIRD leaderboard on both the original and the corrected BIRD Dev subsets. We show that performance changes range from -7% to 31% (in relative terms) and rank changes range from $-9$ to $+9$ positions. We further assess whether these impacts generalize to the full BIRD Dev set. We find that the rankings of agents on the uncorrected subset correlate strongly with those on the full Dev set (Spearman's $r_s$=0.85, $p$=3.26e-5), whereas they correlate weakly with those on the corrected subset (Spearman's $r_s$=0.32, $p$=0.23). These findings show that annotation errors can significantly distort reported performance and rankings, potentially misguiding research directions or deployment choices. Our code and data are available at https://github.com/uiuc-kang-lab/text_to_sql_benchmarks.",
  "authors": [
    "Tengjun Jin",
    "Yoojin Choi",
    "Yuxuan Zhu",
    "Daniel Kang"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.AI",
    "cs.DB"
  ],
  "subjects": [
    "cs.AI",
    "cs.DB"
  ],
  "keywords": [
    "big data"
  ],
  "url": "http://arxiv.org/abs/2601.08778v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08778v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:13.292575",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08776v1",
  "source": "arxiv",
  "doi": "",
  "title": "Translating Light-Sheet Microscopy Images to Virtual H&E Using CycleGAN",
  "abstract": "Histopathology analysis relies on Hematoxylin and Eosin (H&E) staining, but fluorescence microscopy offers complementary information. Converting fluorescence images to H&E-like appearance can aid interpretation and integration with standard workflows. We present a Cycle-Consistent Adversarial Network (CycleGAN) approach for unpaired image-to-image translation from multi-channel fluorescence microscopy to pseudo H&E stained histopathology images. The method combines C01 and C02 fluorescence channels into RGB and learns a bidirectional mapping between fluorescence and H&E domains without paired training data. The architecture uses ResNet-based generators with residual blocks and PatchGAN discriminators, trained with adversarial, cycle-consistency, and identity losses. Experiments on fluorescence microscopy datasets show the model generates realistic pseudo H&E images that preserve morphological structures while adopting H&E-like color characteristics. This enables visualization of fluorescence data in a format familiar to pathologists and supports integration with existing H&E-based analysis pipelines.",
  "authors": [
    "Yanhua Zhao"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CV",
    "cs.AI"
  ],
  "subjects": [
    "cs.CV",
    "cs.AI"
  ],
  "keywords": [
    "big data"
  ],
  "url": "http://arxiv.org/abs/2601.08776v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08776v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:13.295506",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08772v1",
  "source": "arxiv",
  "doi": "",
  "title": "Enhancing classical simulation with noisy quantum devices",
  "abstract": "As quantum devices continue to improve in scale and precision, a central challenge is how to effectively utilize noisy hardware for meaningful computation. Most existing approaches aim to recover noiseless circuit outputs from noisy ones through error mitigation or correction. Here, we show that noisy quantum devices can be directly leveraged as computational resources to enhance the classical simulation of quantum circuits. We introduce the Noisy-device-enhanced Classical Simulation (NDE-CS) protocol, which improves stabilizer-based classical Monte Carlo simulation methods by incorporating data obtained from noisy quantum hardware. Specifically, NDE-CS uses noisy executions of a target circuit together with noisy Clifford circuits to learn how the target circuit can be expressed in terms of Clifford circuits under realistic noise. The same learned relation can then be reused in the noiseless Clifford limit, enabling accurate estimation of ideal expectation values with substantially reduced sampling cost. Numerical simulations on Trotterized Ising circuits demonstrate that NDE-CS achieves orders-of-magnitude reductions in sampling cost compared to the underlying purely classical Monte Carlo approaches from which it is derived, while maintaining the same accuracy. We also compare NDE-CS with Sparse Pauli Dynamics (SPD), a powerful classical framework capable of simulating quantum circuits at previously inaccessible scales, and provide an example where the cost of SPD scales exponentially with system size, while NDE-CS scales much more favorably. These results establish NDE-CS as a scalable hybrid simulation approach for quantum circuits, where noise can be harnessed as a computational asset.",
  "authors": [
    "Ruiqi Zhang",
    "Fuchuan Wei",
    "Zhaohui Wei"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "quant-ph"
  ],
  "subjects": [
    "quant-ph"
  ],
  "keywords": [
    "big data"
  ],
  "url": "http://arxiv.org/abs/2601.08772v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08772v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:13.297215",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08771v1",
  "source": "arxiv",
  "doi": "",
  "title": "Failure of uniqueness for scalar conservation laws",
  "abstract": "In this article, we develop what are, to the best of our knowledge, the first negative results for scalar conservation laws. We begin with explicit examples where bounded initial data leads to $L^{\\infty}$ blow-up despite flux regularity. More strikingly, we demonstrate that Kružkov's entropy equalities alone fail to ensure uniqueness in this regime by constructing infinitely many entropy solutions to a single Cauchy problem with bounded initial datum, each continuous in time with respect to the $L^{1}$ norm. Thus, we demonstrate that the $L^{\\infty}$ assumption is essential for the doubling of variables argument, and hence for the uniqueness of entropy solutions to scalar conservation laws. On the positive side, we develop a novel theory for scalar conservation laws with spatial heterogeneity by adapting the front tracking method. We recover uniqueness by imposing a Lax-type condition in addition to the entropy inequality, motivated by the properties of our front tracking approximations. Unbounded Kružkov solutions do not necessarily satisfy the weak formulation; we show that global weak solutions may not even exist in a natural class for some Cauchy problems of this form, even when Kružkov entropy solutions exist. Finally, we construct an explicit example of global ill-posedness with bounded initial datum.",
  "authors": [
    "Shyam Sundar Ghoshal",
    "Abraham Sylla",
    "Parasuram Venkatesh"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "math.AP"
  ],
  "subjects": [
    "math.AP"
  ],
  "keywords": [
    "big data"
  ],
  "url": "http://arxiv.org/abs/2601.08771v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08771v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:13.299575",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08768v1",
  "source": "arxiv",
  "doi": "",
  "title": "AI as Entertainment",
  "abstract": "Generative AI systems are predominantly designed, evaluated, and marketed as intelligent systems which will benefit society by augmenting or automating human cognitive labor, promising to increase personal, corporate, and macroeconomic productivity. But this mainstream narrative about what AI is and what it can do is in tension with another emerging use case: entertainment. We argue that the field of AI is unprepared to measure or respond to how the proliferation of entertaining AI-generated content will impact society. Emerging data suggest AI is already widely adopted for entertainment purposes -- especially by young people -- and represents a large potential source of revenue. We contend that entertainment will become a primary business model for major AI corporations seeking returns on massive infrastructure investments; this will exert a powerful influence on the technology these companies produce in the coming years. Examining current evaluation practices, we identify a critical asymmetry: while AI assessments rigorously measure both benefits and harms of intelligence, they focus almost exclusively on cultural harms. We lack frameworks for articulating how cultural outputs might be actively beneficial. Drawing on insights from the humanities, we propose \"thick entertainment\" as a framework for evaluating AI-generated cultural content -- one that considers entertainment's role in meaning-making, identity formation, and social connection rather than simply minimizing harm. While AI is often touted for its potential to revolutionize productivity, in the long run we may find that AI turns out to be as much about \"intelligence\" as social media is about social connection.",
  "authors": [
    "Cody Kommers",
    "Ari Holtzman"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.AI",
    "cs.CY"
  ],
  "subjects": [
    "cs.AI",
    "cs.CY"
  ],
  "keywords": [
    "big data"
  ],
  "url": "http://arxiv.org/abs/2601.08768v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08768v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:13.301446",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08759v1",
  "source": "arxiv",
  "doi": "",
  "title": "Convergence analysis and adaptive computation of a Banach-space mixed finite element method for generalized bioconvective flows",
  "abstract": "We develop and analyse an adaptive fully mixed finite element method for stationary generalized bioconvective flows, where the Navier--Stokes equations with concentration-dependent viscosity are coupled with a conservation law for swimming microorganisms. The formulation introduces auxiliary variables including the trace-free velocity gradient, a symmetric pseudo-stress tensor, the concentration gradient, and a semi-advective microorganism flux, which also allows for a consistent treatment of Robin-type boundary condition. The variational problem is posed within a Banach space framework and reformulated as a fixed-point operator. Existence of solutions follows from Schauder's theorem, while uniqueness is obtained under suitable data assumptions. The discrete problem is constructed using Raviart--Thomas finite element spaces together with piecewise polynomial approximations on macroelement-structured meshes, and existence of discrete solutions is established via Brouwer's theorem. An a priori error analysis yields optimal convergence rates. We further derive a residual-based a posteriori error estimator and prove its reliability using global inf-sup conditions, Helmholtz decompositions, and suitable projection operators, while efficiency is ensured through localization techniques and bubble functions. Numerical experiments in two and three dimensions confirm the theoretical results, demonstrate the effectiveness of adaptive refinement for singular solutions and complex geometries with inclusions, and illustrate the robustness of the method for a bioconvective benchmark with plume formation governed by an Einstein--Batchelor-type viscosity law.",
  "authors": [
    "Eligio Colmenares",
    "Ricardo Ruiz-Baier",
    "Dalidet Sanhueza"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "math.NA"
  ],
  "subjects": [
    "math.NA"
  ],
  "keywords": [
    "big data"
  ],
  "url": "http://arxiv.org/abs/2601.08759v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08759v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:13.303656",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08757v1",
  "source": "arxiv",
  "doi": "",
  "title": "Evidence of sloshing-driven mini-halo formation in the cool-core cluster RXCJ1558.3-1410",
  "abstract": "Radio mini-halos are perplexing features, typically hosted by X-ray cool-core galaxy clusters. Understanding the connection between thermal X-ray and non-thermal radio emission is key to uncovering their origin. Here, we present a multiwavelength study of the cool-core cluster RXCJ1558.3-1410 using archival Chandra X-ray and wideband uGMRT radio data (Bands 3, 4 and 5). Our improved analysis confirms a previously known X-ray cavity at $\\sim$36 kpc south-east of the cluster centre and we report a new cavity at $\\sim$42 kpc to the north-west. These cavities suggest that the AGN provides mechanical power of $\\sim$$6.0 \\times 10^{44}$ erg s$^{-1}$, sufficient to offset radiative cooling in the ICM. We also detect a sharp surface brightness edge at $\\sim$72 kpc south-east of the centre, characterised by a temperature jump and pressure continuity, consistent with a cold front, likely caused by gas sloshing from a minor merger. Our uGMRT images reveals an interesting diffuse emission surrounding the brightest cluster galaxy (BCG), with its edge spatially coinciding with the sloshing cold front and roughly with the cooling radius. Furthermore, a low star formation rate and uniform metal abundance up to the sloshing edge are consistent with the earlier findings of suppression of star formation and metallicity homogenisation by mixing core gas through sloshing. Finally, the spatial correlation between the mini-halo and the observed X-ray features indicates that ICM sloshing, rather than AGN feedback, plays a dominant role in powering the proposed radio mini-halo emission.",
  "authors": [
    "Vishal S. Kale",
    "Sonali K. Kadam",
    "Sameer Salunkhe",
    "Satish S. Sonkamble",
    "Nilkanth D. Vagshette",
    "Surajit Paul",
    "Ruta Kale",
    "S. Ilani Loubser",
    "M. K. Patil"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "astro-ph.CO",
    "astro-ph.GA",
    "astro-ph.HE"
  ],
  "subjects": [
    "astro-ph.CO",
    "astro-ph.GA",
    "astro-ph.HE"
  ],
  "keywords": [
    "big data"
  ],
  "url": "http://arxiv.org/abs/2601.08757v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08757v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:13.305253",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08754v1",
  "source": "arxiv",
  "doi": "10.1038/s41598-025-31970-7",
  "title": "Evolving spatiotemporal patterns and urban scaling of deaths from external causes",
  "abstract": "Urban scaling theory posits that urban indicators follow power-law relations with population, yet the evolution of these patterns - and the role of regional differences in settings marked by social inequalities and unplanned urbanization - remains poorly understood. Here, we analyze nearly three decades of mortality data from Brazilian cities to investigate the scaling of external causes of death: homicides, suicides, and accidents. Using a hierarchical Bayesian framework and spatial correlation analysis, we find that these mortality indicators exhibit distinct, regionally heterogeneous scaling trajectories. Homicide mortality has significantly attenuated its typical superlinear scaling with increased spatial clustering, suggesting a redistribution of violence to smaller cities and intensified intercity interactions, possibly linked to the consolidation of organized crime. Suicide mortality, usually sublinear, has trended upward, implying a weakening of urban agglomerations' protective effect. Accident mortality remains superlinear, with transport fatalities scaling nearly proportionally, and non-transport accidents becoming superlinear. The scaling changes for suicides and accidents coincide with less correlated and stable spatial patterns, suggesting that the underlying processes predominantly operate within city boundaries. Finally, while scaling exponents have evolved more homogeneously across Brazilian states, scale-adjusted mortality remains highly heterogeneous, indicating that fundamental processes govern scaling laws, whereas state-specific factors drive scale-adjusted metrics.",
  "authors": [
    "Cesar I. N. Sampaio Filho",
    "Humberto A. Carmona",
    "Antonio S. Lima Neto",
    "Monica V. Prates",
    "Haroldo V. Ribeiro",
    "Marcia C. Castro",
    "Jose S. Andrade"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "physics.soc-ph",
    "physics.data-an"
  ],
  "subjects": [
    "physics.soc-ph",
    "physics.data-an"
  ],
  "keywords": [
    "big data"
  ],
  "url": "http://arxiv.org/abs/2601.08754v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08754v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:13.306768",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08753v1",
  "source": "arxiv",
  "doi": "10.1109/ITSC58415.2024.10920218",
  "title": "Grid-Aware Charging and Operational Optimization for Mixed-Fleet Public Transit",
  "abstract": "The rapid growth of urban populations and the increasing need for sustainable transportation solutions have prompted a shift towards electric buses in public transit systems. However, the effective management of mixed fleets consisting of both electric and diesel buses poses significant operational challenges. One major challenge is coping with dynamic electricity pricing, where charging costs vary throughout the day. Transit agencies must optimize charging assignments in response to such dynamism while accounting for secondary considerations such as seating constraints. This paper presents a comprehensive mixed-integer linear programming (MILP) model to address these challenges by jointly optimizing charging schedules and trip assignments for mixed (electric and diesel bus) fleets while considering factors such as dynamic electricity pricing, vehicle capacity, and route constraints. We address the potential computational intractability of the MILP formulation, which can arise even with relatively small fleets, by employing a hierarchical approach tailored to the fleet composition. By using real-world data from the city of Chattanooga, Tennessee, USA, we show that our approach can result in significant savings in the operating costs of the mixed transit fleets.",
  "authors": [
    "Rishav Sen",
    "Amutheezan Sivagnanam",
    "Aron Laszka",
    "Ayan Mukhopadhyay",
    "Abhishek Dubey"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "math.OC",
    "cs.AI",
    "eess.SY"
  ],
  "subjects": [
    "math.OC",
    "cs.AI",
    "eess.SY"
  ],
  "keywords": [
    "big data"
  ],
  "url": "http://arxiv.org/abs/2601.08753v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08753v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:13.307911",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08752v1",
  "source": "arxiv",
  "doi": "",
  "title": "Phenomenological study of $Ω_c\\rightarrow Ω^-π^+$ at polarized electron-positron collider",
  "abstract": "The exploration of symmetry laws stands as a cutting-edge direction in modern physics research. This work delves into the examination of P and CP symmetry properties within the charm quark system by analyzing asymmetry parameters in the two-body decay process of $Ω_c$. By accounting for the polarization effects of electron and positron beams and employing the helicity formalism, we systematically analyze the decay characteristics of $Ω_c$ and its subsequent hyperon decays through specific asymmetry parameters. A comprehensive formulation of the angular distribution for these decay processes has been developed. The research assesses the detection sensitivity of asymmetry parameters in the $Ω_c\\rightarrow Ω^-π^+$ decay mode across different experimental conditions, including varying data sample sizes and beam polarization configurations. These results contribute to enriching a theoretical foundation for forthcoming experimental endeavors at the STCF, offering significant implications for symmetry studies in the charm sector.",
  "authors": [
    "Yunlu Wang",
    "Yunlong Xiao",
    "Pengcheng Hong"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "hep-ph"
  ],
  "subjects": [
    "hep-ph"
  ],
  "keywords": [
    "big data"
  ],
  "url": "http://arxiv.org/abs/2601.08752v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08752v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:13.309554",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08750v1",
  "source": "arxiv",
  "doi": "",
  "title": "Spatial Context Improves the Integration of Text with Remote Sensing for Mapping Environmental Variables",
  "abstract": "Recent developments in natural language processing highlight text as an emerging data source for ecology. Textual resources carry unique information that can be used in complementarity with geospatial data sources, thus providing insights at the local scale into environmental conditions and properties hidden from more traditional data sources. Leveraging textual information in a spatial context presents several challenges. First, the contribution of textual data remains poorly defined in an ecological context, and it is unclear for which tasks it should be incorporated. Unlike ubiquitous satellite imagery or environmental covariates, the availability of textual data is sparse and irregular; its integration with geospatial data is not straightforward. In response to these challenges, this work proposes an attention-based approach that combines aerial imagery and geolocated text within a spatial neighbourhood, i.e. integrating contributions from several nearby observations. Our approach combines vision and text representations with a geolocation encoding, with an attention-based module that dynamically selects spatial neighbours that are useful for predictive tasks.The proposed approach is applied to the EcoWikiRS dataset, which combines high-resolution aerial imagery with sentences extracted from Wikipedia describing local environmental conditions across Switzerland. Our model is evaluated on the task of predicting 103 environmental variables from the SWECO25 data cube. Our approach consistently outperforms single-location or unimodal, i.e. image-only or text-only, baselines. When analysing variables by thematic groups, results show a significant improvement in performance for climatic, edaphic, population and land use/land cover variables, underscoring the benefit of including the spatial context when combining text and image data.",
  "authors": [
    "Valerie Zermatten",
    "Chiara Vanalli",
    "Gencer Sumbul",
    "Diego Marcos",
    "Devis Tuia"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CL"
  ],
  "subjects": [
    "cs.CL"
  ],
  "keywords": [
    "big data"
  ],
  "url": "http://arxiv.org/abs/2601.08750v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08750v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:13.310651",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08746v1",
  "source": "arxiv",
  "doi": "",
  "title": "Search for Cosmic Ray Electron Boosted Dark Matter with the CDEX-10 Experiment",
  "abstract": "We present new constraints on the cosmic ray electron boosted light dark matter (CReDM) using the 205.4 kg$\\cdot$day data of the CDEX-10 experiment located at the China Jinping Underground Laboratory. The cosmic ray electron spectrum and distribution in the Galaxy are generated by the $\\tt GALPROP$ code package. In the calculation process of DM-electron scattering process in the Galaxy, we consider the energy-dependency of the DM-electron scattering cross section. The constraints on CReDM are set for both heavy and light mediator scenarios using the CDEX-10 dataset. The result exceeds previous Standard Halo Model (SHM) limits for DM mass lower than 0.6 MeV in heavy mediator case and corresponds to the best sensitivity among all direct detection experiments from 1 keV to 0.5 MeV in the light mediator scenario.",
  "authors": [
    "R. Xu",
    "L. T. Yang",
    "Q. Yue",
    "K. J. Kang",
    "Y. J. Li",
    "H. P. An",
    "Greeshma C.",
    "J. P. Chang",
    "H. Chen",
    "Y. H. Chen",
    "J. P. Cheng",
    "J. Y. Cui",
    "W. H. Dai",
    "Z. Deng",
    "Y. X. Dong",
    "C. H. Fang",
    "H. Gong",
    "Q. J. Guo",
    "T. Guo",
    "X. Y. Guo",
    "L. He",
    "J. R. He",
    "H. X. Huang",
    "T. C. Huang",
    "S. Karmakar",
    "Y. S. Lan",
    "H. B. Li",
    "H. Y. Li",
    "J. M. Li",
    "J. Li",
    "M. C. Li",
    "Q. Y. Li",
    "R. M. J. Li",
    "X. Q. Li",
    "Y. L. Li",
    "Y. F. Liang",
    "B. Liao",
    "F. K. Lin",
    "S. T. Lin",
    "J. X. Liu",
    "R. Z. Liu",
    "S. K. Liu",
    "Y. D. Liu",
    "Y. Liu",
    "Y. Y. Liu",
    "H. Ma",
    "Y. C. Mao",
    "A. Mureed",
    "H. Pan",
    "N. C. Qi",
    "J. Ren",
    "X. C. Ruan",
    "M. B. Shen",
    "H. Y. Shi",
    "M. K. Singh",
    "T. X. Sun",
    "W. L. Sun",
    "C. J. Tang",
    "Y. Tian",
    "H. F. Wan",
    "G. F. Wang",
    "J. Z. Wang",
    "L. Wang",
    "Q. Wang",
    "Q. Wang",
    "Y. F. Wang",
    "Y. X. Wang",
    "H. T. Wong",
    "Y. C. Wu",
    "H. Y. Xing",
    "K. Z. Xiong",
    "Y. Xu",
    "T. Xue",
    "Y. L. Yan",
    "N. Yi",
    "C. X. Yu",
    "H. J. Yu",
    "X. Yu",
    "M. Zeng",
    "Z. Zeng",
    "F. S. Zhang",
    "P. Zhang",
    "P. Zhang",
    "Z. Y. Zhang",
    "M. G. Zhao",
    "J. F. Zhou",
    "Z. Y. Zhou",
    "J. J. Zhu"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "hep-ex",
    "hep-ph",
    "physics.ins-det"
  ],
  "subjects": [
    "hep-ex",
    "hep-ph",
    "physics.ins-det"
  ],
  "keywords": [
    "big data"
  ],
  "url": "http://arxiv.org/abs/2601.08746v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08746v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:13.311848",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08735v1",
  "source": "arxiv",
  "doi": "",
  "title": "QCD phase-transition under the light of Thermofractal",
  "abstract": "The deconfining transition in $SU(3)$ gauge theory, traditionally interpreted through the Gross-Witten-Wadia (GWW) model as a sharp third-order phase transition in the large-$N_c$ limit, appears as a smooth crossover in lattice QCD. This work demonstrates that the transition is topologically smoothed into a crossover by incorporating the fractal momentum space structure inherent to thermofractals. By matching the non-extensive $β$-function to one-loop QCD results, a fundamental scaling of the thermofractal index $q$ is derived as a function of the number of flavours $N_f$. It is proven that applying a $q$-deformed derivative operator $\\mathcal{D}_q$ to the $q$-logarithm of the eigenvalue distance results in a non-extensive measure that effectively smears the topological stiffness of the gauge vacuum. A unified master equation for the Polyakov loop $\\langle L \\rangle$ is presented, governed by the thermofractal index $q$ and a single variance parameter $σ^2(T)$ that scales as $T^{1/(q-1)}$. The observed phase dynamics are shown to be asymptotic limits of this unified density: a ``soft'' algebraic growth $\\langle L \\rangle \\propto T^{11}$ in the 1D string-like confined regime for $N_f=0$, and a rapid $1 - \\langle L \\rangle \\propto T^{-21}$ suppression in the 3D deconfined volume for $N_f=3$. This approach provides a microscopic foundation for partial deconfinement theory and reproduces lattice QCD data with a reduced $χ^2 \\approx 1.12$, offering a rigorous reconciliation between matrix model topology and the continuous QCD crossover.",
  "authors": [
    "Airton Deppman"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "hep-lat",
    "hep-ex",
    "hep-ph",
    "hep-th"
  ],
  "subjects": [
    "hep-lat",
    "hep-ex",
    "hep-ph",
    "hep-th"
  ],
  "keywords": [
    "big data"
  ],
  "url": "http://arxiv.org/abs/2601.08735v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08735v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:13.313287",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08828v1",
  "source": "arxiv",
  "doi": "",
  "title": "Motion Attribution for Video Generation",
  "abstract": "Despite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present Motive (MOTIon attribution for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. We use this to study which fine-tuning clips improve or degrade temporal dynamics. Motive isolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation. On text-to-video models, Motive identifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility. With Motive-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model. To our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate fine-tuning data.",
  "authors": [
    "Xindi Wu",
    "Despoina Paschalidou",
    "Jun Gao",
    "Antonio Torralba",
    "Laura Leal-Taixé",
    "Olga Russakovsky",
    "Sanja Fidler",
    "Jonathan Lorraine"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CV",
    "cs.AI",
    "cs.LG",
    "cs.MM",
    "cs.RO"
  ],
  "subjects": [
    "cs.CV",
    "cs.AI",
    "cs.LG",
    "cs.MM",
    "cs.RO"
  ],
  "keywords": [
    "data mining"
  ],
  "url": "http://arxiv.org/abs/2601.08828v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08828v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:16.039762",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08823v1",
  "source": "arxiv",
  "doi": "",
  "title": "Lattice-based equation of state with a critical point from constant entropy contours and its comparison to effective QCD approaches",
  "abstract": "In this work, we systematically assess the performance of a new method from [H. Shah et al., Phys. Rev. C 113, L012201] for locating the QCD critical point using constant-entropy contours by testing it against various effective QCD approaches. We demonstrate that, while the method yields spurious critical points in purely hadronic models (HRG) due to non-parabolic contour behavior at low temperatures ($T \\lesssim 120$ MeV), it accurately reproduces the CP location in frameworks that feature a genuine phase transition and benchmarked against lattice QCD, such as Holographic Einstein-Maxwell-Dilaton, and Functional QCD approaches. Building on our previous determination of constant entropy contours using lattice data, we extend that analysis to construct a complete Lattice-based Equation of State (EoS) at finite density, which features a critical point at $(T, μ_B) \\approx (114, 602)$ MeV. By integrating the extrapolated entropy density with respect to temperature, we reconstruct the pressure, baryon density, susceptibility, and speed of sound in the critical region, and analyze the focusing behavior of isentropic trajectories in the vicinity of the critical point.",
  "authors": [
    "Hitansh Shah",
    "Mauricio Hippert",
    "Jorge Noronha",
    "Claudia Ratti",
    "Volodymyr Vovchenko"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "hep-ph",
    "nucl-th"
  ],
  "subjects": [
    "hep-ph",
    "nucl-th"
  ],
  "keywords": [
    "data mining"
  ],
  "url": "http://arxiv.org/abs/2601.08823v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08823v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:16.045741",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08811v1",
  "source": "arxiv",
  "doi": "",
  "title": "Reasoning Matters for 3D Visual Grounding",
  "abstract": "The recent development of Large Language Models (LLMs) with strong reasoning ability has driven research in various domains such as mathematics, coding, and scientific discovery. Meanwhile, 3D visual grounding, as a fundamental task in 3D understanding, still remains challenging due to the limited reasoning ability of recent 3D visual grounding models. Most of the current methods incorporate a text encoder and visual feature encoder to generate cross-modal fuse features and predict the referring object. These models often require supervised training on extensive 3D annotation data. On the other hand, recent research also focus on scaling synthetic data to train stronger 3D visual grounding LLM, however, the performance gain remains limited and non-proportional to the data collection cost. In this work, we propose a 3D visual grounding data pipeline, which is capable of automatically synthesizing 3D visual grounding data along with corresponding reasoning process. Additionally, we leverage the generated data for LLM fine-tuning and introduce Reason3DVG-8B, a strong 3D visual grounding LLM that outperforms previous LLM-based method 3D-GRAND using only 1.6% of their training data, demonstrating the effectiveness of our data and the importance of reasoning in 3D visual grounding.",
  "authors": [
    "Hsiang-Wei Huang",
    "Kuang-Ming Chen",
    "Wenhao Chai",
    "Cheng-Yen Yang",
    "Jen-Hao Cheng",
    "Jenq-Neng Hwang"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CV",
    "cs.AI"
  ],
  "subjects": [
    "cs.CV",
    "cs.AI"
  ],
  "keywords": [
    "data mining"
  ],
  "url": "http://arxiv.org/abs/2601.08811v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08811v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:16.049845",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08799v1",
  "source": "arxiv",
  "doi": "",
  "title": "Collapse of statistical equilibrium in large-scale hydroelastic turbulent waves",
  "abstract": "At scales larger than the forcing scale, some out-of-equilibrium turbulent systems (such as hydrodynamic turbulence, wave turbulence, and nonlinear optics) exhibit a state of statistical equilibrium where energy is equipartitioned among large-scale modes, in line with the Rayleigh-Jeans spectrum. Key open questions now pertain to either the emergence, decay, collapse, or other nonstationary evolutions from this state. Here, we experimentally investigate the free decay of large-scale hydroelastic turbulent waves, initially in a regime of statistical equilibrium. Using space- and time-resolved measurements, we show that the total energy of these large-scale tensional waves decays as a power law in time. We derive an energy decay law from the theoretical initial equilibrium spectrum and the linear viscous damping, as no net energy flux is carried. Our prediction then shows a good agreement with experimental data over nearly two decades in time, for various initial effective temperatures of the statistical equilibrium state. We further identify the dissipation mechanism and confirm it experimentally. Our approach could be applied to other decaying turbulence systems, with the large scales initially in statistical equilibrium.",
  "authors": [
    "Marlone Vernet",
    "Eric Falcon"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "physics.flu-dyn",
    "cond-mat.stat-mech",
    "nlin.CD",
    "physics.ao-ph",
    "physics.geo-ph"
  ],
  "subjects": [
    "physics.flu-dyn",
    "cond-mat.stat-mech",
    "nlin.CD",
    "physics.ao-ph",
    "physics.geo-ph"
  ],
  "keywords": [
    "data mining"
  ],
  "url": "http://arxiv.org/abs/2601.08799v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08799v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:16.053980",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08784v1",
  "source": "arxiv",
  "doi": "",
  "title": "On the use of graph models to achieve individual and group fairness",
  "abstract": "Machine Learning algorithms are ubiquitous in key decision-making contexts such as justice, healthcare and finance, which has spawned a great demand for fairness in these procedures. However, the theoretical properties of such models in relation with fairness are still poorly understood, and the intuition behind the relationship between group and individual fairness is still lacking. In this paper, we provide a theoretical framework based on Sheaf Diffusion to leverage tools based on dynamical systems and homology to model fairness. Concretely, the proposed method projects input data into a bias-free space that encodes fairness constrains, resulting in fair solutions. Furthermore, we present a collection of network topologies handling different fairness metrics, leading to a unified method capable of dealing with both individual and group bias. The resulting models have a layer of interpretability in the form of closed-form expressions for their SHAP values, consolidating their place in the responsible Artificial Intelligence landscape. Finally, these intuitions are tested on a simulation study and standard fairness benchmarks, where the proposed methods achieve satisfactory results. More concretely, the paper showcases the performance of the proposed models in terms of accuracy and fairness, studying available trade-offs on the Pareto frontier, checking the effects of changing the different hyper-parameters, and delving into the interpretation of its outputs.",
  "authors": [
    "Arturo Pérez-Peralta",
    "Sandra Benítez-Peña",
    "Rosa E. Lillo"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "stat.ML",
    "cs.CY",
    "cs.LG"
  ],
  "subjects": [
    "stat.ML",
    "cs.CY",
    "cs.LG"
  ],
  "keywords": [
    "data mining"
  ],
  "url": "http://arxiv.org/abs/2601.08784v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08784v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:16.057104",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08781v1",
  "source": "arxiv",
  "doi": "",
  "title": "Fast and explainable clustering in the Manhattan and Tanimoto distance",
  "abstract": "The CLASSIX algorithm is a fast and explainable approach to data clustering. In its original form, this algorithm exploits the sorting of the data points by their first principal component to truncate the search for nearby data points, with nearness being defined in terms of the Euclidean distance. Here we extend CLASSIX to other distance metrics, including the Manhattan distance and the Tanimoto distance. Instead of principal components, we use an appropriate norm of the data vectors as the sorting criterion, combined with the triangle inequality for search termination. In the case of Tanimoto distance, a provably sharper intersection inequality is used to further boost the performance of the new algorithm. On a real-world chemical fingerprint benchmark, CLASSIX Tanimoto is about 30 times faster than the Taylor--Butina algorithm, and about 80 times faster than DBSCAN, while computing higher-quality clusters in both cases.",
  "authors": [
    "Stefan Güttel",
    "Kaustubh Roy"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.LG"
  ],
  "subjects": [
    "cs.LG"
  ],
  "keywords": [
    "data mining"
  ],
  "url": "http://arxiv.org/abs/2601.08781v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08781v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:16.060607",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08780v1",
  "source": "arxiv",
  "doi": "",
  "title": "LWM-Spectro: A Foundation Model for Wireless Baseband Signal Spectrograms",
  "abstract": "The received in-phase and quadrature (I/Q) baseband signals inherently encode physical-layer and channel characteristics of wireless links. Learning robust and transferable representations directly from such raw signals, however, remains challenging due to heterogeneous communication systems, diverse propagation environments, and limited labeled data. To address this, we present LWM-Spectro, a transformer-based foundation model pretrained on large-scale I/Q data represented as time-frequency spectrograms. The model leverages self-supervised masked modeling, contrastive learning, and a mixture-of-experts (MoE) architecture to learn general-purpose wireless representations. These representations transfer effectively to downstream tasks such as modulation classification and joint SNR/mobility recognition, even with minimal supervision. Across tasks, LWM-Spectro consistently outperforms state-of-the-art deep learning baselines in both few-shot and data-rich regimes, providing a unified foundation for wireless learning.",
  "authors": [
    "Namhyun Kim",
    "Sadjad Alikhani",
    "Ahmed Alkhateeb"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.IT",
    "eess.SP"
  ],
  "subjects": [
    "cs.IT",
    "eess.SP"
  ],
  "keywords": [
    "data mining"
  ],
  "url": "http://arxiv.org/abs/2601.08780v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08780v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:16.067123",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08778v1",
  "source": "arxiv",
  "doi": "",
  "title": "Pervasive Annotation Errors Break Text-to-SQL Benchmarks and Leaderboards",
  "abstract": "Researchers have proposed numerous text-to-SQL techniques to streamline data analytics and accelerate the development of database-driven applications. To compare these techniques and select the best one for deployment, the community depends on public benchmarks and their leaderboards. Since these benchmarks heavily rely on human annotations during question construction and answer evaluation, the validity of the annotations is crucial. In this paper, we conduct an empirical study that (i) benchmarks annotation error rates for two widely used text-to-SQL benchmarks, BIRD and Spider 2.0-Snow, and (ii) corrects a subset of the BIRD development (Dev) set to measure the impact of annotation errors on text-to-SQL agent performance and leaderboard rankings. Through expert analysis, we show that BIRD Mini-Dev and Spider 2.0-Snow have error rates of 52.8% and 62.8%, respectively. We re-evaluate all 16 open-source agents from the BIRD leaderboard on both the original and the corrected BIRD Dev subsets. We show that performance changes range from -7% to 31% (in relative terms) and rank changes range from $-9$ to $+9$ positions. We further assess whether these impacts generalize to the full BIRD Dev set. We find that the rankings of agents on the uncorrected subset correlate strongly with those on the full Dev set (Spearman's $r_s$=0.85, $p$=3.26e-5), whereas they correlate weakly with those on the corrected subset (Spearman's $r_s$=0.32, $p$=0.23). These findings show that annotation errors can significantly distort reported performance and rankings, potentially misguiding research directions or deployment choices. Our code and data are available at https://github.com/uiuc-kang-lab/text_to_sql_benchmarks.",
  "authors": [
    "Tengjun Jin",
    "Yoojin Choi",
    "Yuxuan Zhu",
    "Daniel Kang"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.AI",
    "cs.DB"
  ],
  "subjects": [
    "cs.AI",
    "cs.DB"
  ],
  "keywords": [
    "data mining"
  ],
  "url": "http://arxiv.org/abs/2601.08778v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08778v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:16.070505",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08776v1",
  "source": "arxiv",
  "doi": "",
  "title": "Translating Light-Sheet Microscopy Images to Virtual H&E Using CycleGAN",
  "abstract": "Histopathology analysis relies on Hematoxylin and Eosin (H&E) staining, but fluorescence microscopy offers complementary information. Converting fluorescence images to H&E-like appearance can aid interpretation and integration with standard workflows. We present a Cycle-Consistent Adversarial Network (CycleGAN) approach for unpaired image-to-image translation from multi-channel fluorescence microscopy to pseudo H&E stained histopathology images. The method combines C01 and C02 fluorescence channels into RGB and learns a bidirectional mapping between fluorescence and H&E domains without paired training data. The architecture uses ResNet-based generators with residual blocks and PatchGAN discriminators, trained with adversarial, cycle-consistency, and identity losses. Experiments on fluorescence microscopy datasets show the model generates realistic pseudo H&E images that preserve morphological structures while adopting H&E-like color characteristics. This enables visualization of fluorescence data in a format familiar to pathologists and supports integration with existing H&E-based analysis pipelines.",
  "authors": [
    "Yanhua Zhao"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CV",
    "cs.AI"
  ],
  "subjects": [
    "cs.CV",
    "cs.AI"
  ],
  "keywords": [
    "data mining"
  ],
  "url": "http://arxiv.org/abs/2601.08776v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08776v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:16.074058",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08772v1",
  "source": "arxiv",
  "doi": "",
  "title": "Enhancing classical simulation with noisy quantum devices",
  "abstract": "As quantum devices continue to improve in scale and precision, a central challenge is how to effectively utilize noisy hardware for meaningful computation. Most existing approaches aim to recover noiseless circuit outputs from noisy ones through error mitigation or correction. Here, we show that noisy quantum devices can be directly leveraged as computational resources to enhance the classical simulation of quantum circuits. We introduce the Noisy-device-enhanced Classical Simulation (NDE-CS) protocol, which improves stabilizer-based classical Monte Carlo simulation methods by incorporating data obtained from noisy quantum hardware. Specifically, NDE-CS uses noisy executions of a target circuit together with noisy Clifford circuits to learn how the target circuit can be expressed in terms of Clifford circuits under realistic noise. The same learned relation can then be reused in the noiseless Clifford limit, enabling accurate estimation of ideal expectation values with substantially reduced sampling cost. Numerical simulations on Trotterized Ising circuits demonstrate that NDE-CS achieves orders-of-magnitude reductions in sampling cost compared to the underlying purely classical Monte Carlo approaches from which it is derived, while maintaining the same accuracy. We also compare NDE-CS with Sparse Pauli Dynamics (SPD), a powerful classical framework capable of simulating quantum circuits at previously inaccessible scales, and provide an example where the cost of SPD scales exponentially with system size, while NDE-CS scales much more favorably. These results establish NDE-CS as a scalable hybrid simulation approach for quantum circuits, where noise can be harnessed as a computational asset.",
  "authors": [
    "Ruiqi Zhang",
    "Fuchuan Wei",
    "Zhaohui Wei"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "quant-ph"
  ],
  "subjects": [
    "quant-ph"
  ],
  "keywords": [
    "data mining"
  ],
  "url": "http://arxiv.org/abs/2601.08772v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08772v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:16.077098",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08771v1",
  "source": "arxiv",
  "doi": "",
  "title": "Failure of uniqueness for scalar conservation laws",
  "abstract": "In this article, we develop what are, to the best of our knowledge, the first negative results for scalar conservation laws. We begin with explicit examples where bounded initial data leads to $L^{\\infty}$ blow-up despite flux regularity. More strikingly, we demonstrate that Kružkov's entropy equalities alone fail to ensure uniqueness in this regime by constructing infinitely many entropy solutions to a single Cauchy problem with bounded initial datum, each continuous in time with respect to the $L^{1}$ norm. Thus, we demonstrate that the $L^{\\infty}$ assumption is essential for the doubling of variables argument, and hence for the uniqueness of entropy solutions to scalar conservation laws. On the positive side, we develop a novel theory for scalar conservation laws with spatial heterogeneity by adapting the front tracking method. We recover uniqueness by imposing a Lax-type condition in addition to the entropy inequality, motivated by the properties of our front tracking approximations. Unbounded Kružkov solutions do not necessarily satisfy the weak formulation; we show that global weak solutions may not even exist in a natural class for some Cauchy problems of this form, even when Kružkov entropy solutions exist. Finally, we construct an explicit example of global ill-posedness with bounded initial datum.",
  "authors": [
    "Shyam Sundar Ghoshal",
    "Abraham Sylla",
    "Parasuram Venkatesh"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "math.AP"
  ],
  "subjects": [
    "math.AP"
  ],
  "keywords": [
    "data mining"
  ],
  "url": "http://arxiv.org/abs/2601.08771v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08771v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:16.080146",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08768v1",
  "source": "arxiv",
  "doi": "",
  "title": "AI as Entertainment",
  "abstract": "Generative AI systems are predominantly designed, evaluated, and marketed as intelligent systems which will benefit society by augmenting or automating human cognitive labor, promising to increase personal, corporate, and macroeconomic productivity. But this mainstream narrative about what AI is and what it can do is in tension with another emerging use case: entertainment. We argue that the field of AI is unprepared to measure or respond to how the proliferation of entertaining AI-generated content will impact society. Emerging data suggest AI is already widely adopted for entertainment purposes -- especially by young people -- and represents a large potential source of revenue. We contend that entertainment will become a primary business model for major AI corporations seeking returns on massive infrastructure investments; this will exert a powerful influence on the technology these companies produce in the coming years. Examining current evaluation practices, we identify a critical asymmetry: while AI assessments rigorously measure both benefits and harms of intelligence, they focus almost exclusively on cultural harms. We lack frameworks for articulating how cultural outputs might be actively beneficial. Drawing on insights from the humanities, we propose \"thick entertainment\" as a framework for evaluating AI-generated cultural content -- one that considers entertainment's role in meaning-making, identity formation, and social connection rather than simply minimizing harm. While AI is often touted for its potential to revolutionize productivity, in the long run we may find that AI turns out to be as much about \"intelligence\" as social media is about social connection.",
  "authors": [
    "Cody Kommers",
    "Ari Holtzman"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.AI",
    "cs.CY"
  ],
  "subjects": [
    "cs.AI",
    "cs.CY"
  ],
  "keywords": [
    "data mining"
  ],
  "url": "http://arxiv.org/abs/2601.08768v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08768v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:16.084806",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08759v1",
  "source": "arxiv",
  "doi": "",
  "title": "Convergence analysis and adaptive computation of a Banach-space mixed finite element method for generalized bioconvective flows",
  "abstract": "We develop and analyse an adaptive fully mixed finite element method for stationary generalized bioconvective flows, where the Navier--Stokes equations with concentration-dependent viscosity are coupled with a conservation law for swimming microorganisms. The formulation introduces auxiliary variables including the trace-free velocity gradient, a symmetric pseudo-stress tensor, the concentration gradient, and a semi-advective microorganism flux, which also allows for a consistent treatment of Robin-type boundary condition. The variational problem is posed within a Banach space framework and reformulated as a fixed-point operator. Existence of solutions follows from Schauder's theorem, while uniqueness is obtained under suitable data assumptions. The discrete problem is constructed using Raviart--Thomas finite element spaces together with piecewise polynomial approximations on macroelement-structured meshes, and existence of discrete solutions is established via Brouwer's theorem. An a priori error analysis yields optimal convergence rates. We further derive a residual-based a posteriori error estimator and prove its reliability using global inf-sup conditions, Helmholtz decompositions, and suitable projection operators, while efficiency is ensured through localization techniques and bubble functions. Numerical experiments in two and three dimensions confirm the theoretical results, demonstrate the effectiveness of adaptive refinement for singular solutions and complex geometries with inclusions, and illustrate the robustness of the method for a bioconvective benchmark with plume formation governed by an Einstein--Batchelor-type viscosity law.",
  "authors": [
    "Eligio Colmenares",
    "Ricardo Ruiz-Baier",
    "Dalidet Sanhueza"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "math.NA"
  ],
  "subjects": [
    "math.NA"
  ],
  "keywords": [
    "data mining"
  ],
  "url": "http://arxiv.org/abs/2601.08759v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08759v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:16.088681",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08757v1",
  "source": "arxiv",
  "doi": "",
  "title": "Evidence of sloshing-driven mini-halo formation in the cool-core cluster RXCJ1558.3-1410",
  "abstract": "Radio mini-halos are perplexing features, typically hosted by X-ray cool-core galaxy clusters. Understanding the connection between thermal X-ray and non-thermal radio emission is key to uncovering their origin. Here, we present a multiwavelength study of the cool-core cluster RXCJ1558.3-1410 using archival Chandra X-ray and wideband uGMRT radio data (Bands 3, 4 and 5). Our improved analysis confirms a previously known X-ray cavity at $\\sim$36 kpc south-east of the cluster centre and we report a new cavity at $\\sim$42 kpc to the north-west. These cavities suggest that the AGN provides mechanical power of $\\sim$$6.0 \\times 10^{44}$ erg s$^{-1}$, sufficient to offset radiative cooling in the ICM. We also detect a sharp surface brightness edge at $\\sim$72 kpc south-east of the centre, characterised by a temperature jump and pressure continuity, consistent with a cold front, likely caused by gas sloshing from a minor merger. Our uGMRT images reveals an interesting diffuse emission surrounding the brightest cluster galaxy (BCG), with its edge spatially coinciding with the sloshing cold front and roughly with the cooling radius. Furthermore, a low star formation rate and uniform metal abundance up to the sloshing edge are consistent with the earlier findings of suppression of star formation and metallicity homogenisation by mixing core gas through sloshing. Finally, the spatial correlation between the mini-halo and the observed X-ray features indicates that ICM sloshing, rather than AGN feedback, plays a dominant role in powering the proposed radio mini-halo emission.",
  "authors": [
    "Vishal S. Kale",
    "Sonali K. Kadam",
    "Sameer Salunkhe",
    "Satish S. Sonkamble",
    "Nilkanth D. Vagshette",
    "Surajit Paul",
    "Ruta Kale",
    "S. Ilani Loubser",
    "M. K. Patil"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "astro-ph.CO",
    "astro-ph.GA",
    "astro-ph.HE"
  ],
  "subjects": [
    "astro-ph.CO",
    "astro-ph.GA",
    "astro-ph.HE"
  ],
  "keywords": [
    "data mining"
  ],
  "url": "http://arxiv.org/abs/2601.08757v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08757v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:16.093486",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08752v1",
  "source": "arxiv",
  "doi": "",
  "title": "Phenomenological study of $Ω_c\\rightarrow Ω^-π^+$ at polarized electron-positron collider",
  "abstract": "The exploration of symmetry laws stands as a cutting-edge direction in modern physics research. This work delves into the examination of P and CP symmetry properties within the charm quark system by analyzing asymmetry parameters in the two-body decay process of $Ω_c$. By accounting for the polarization effects of electron and positron beams and employing the helicity formalism, we systematically analyze the decay characteristics of $Ω_c$ and its subsequent hyperon decays through specific asymmetry parameters. A comprehensive formulation of the angular distribution for these decay processes has been developed. The research assesses the detection sensitivity of asymmetry parameters in the $Ω_c\\rightarrow Ω^-π^+$ decay mode across different experimental conditions, including varying data sample sizes and beam polarization configurations. These results contribute to enriching a theoretical foundation for forthcoming experimental endeavors at the STCF, offering significant implications for symmetry studies in the charm sector.",
  "authors": [
    "Yunlu Wang",
    "Yunlong Xiao",
    "Pengcheng Hong"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "hep-ph"
  ],
  "subjects": [
    "hep-ph"
  ],
  "keywords": [
    "data mining"
  ],
  "url": "http://arxiv.org/abs/2601.08752v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08752v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:16.098133",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08750v1",
  "source": "arxiv",
  "doi": "",
  "title": "Spatial Context Improves the Integration of Text with Remote Sensing for Mapping Environmental Variables",
  "abstract": "Recent developments in natural language processing highlight text as an emerging data source for ecology. Textual resources carry unique information that can be used in complementarity with geospatial data sources, thus providing insights at the local scale into environmental conditions and properties hidden from more traditional data sources. Leveraging textual information in a spatial context presents several challenges. First, the contribution of textual data remains poorly defined in an ecological context, and it is unclear for which tasks it should be incorporated. Unlike ubiquitous satellite imagery or environmental covariates, the availability of textual data is sparse and irregular; its integration with geospatial data is not straightforward. In response to these challenges, this work proposes an attention-based approach that combines aerial imagery and geolocated text within a spatial neighbourhood, i.e. integrating contributions from several nearby observations. Our approach combines vision and text representations with a geolocation encoding, with an attention-based module that dynamically selects spatial neighbours that are useful for predictive tasks.The proposed approach is applied to the EcoWikiRS dataset, which combines high-resolution aerial imagery with sentences extracted from Wikipedia describing local environmental conditions across Switzerland. Our model is evaluated on the task of predicting 103 environmental variables from the SWECO25 data cube. Our approach consistently outperforms single-location or unimodal, i.e. image-only or text-only, baselines. When analysing variables by thematic groups, results show a significant improvement in performance for climatic, edaphic, population and land use/land cover variables, underscoring the benefit of including the spatial context when combining text and image data.",
  "authors": [
    "Valerie Zermatten",
    "Chiara Vanalli",
    "Gencer Sumbul",
    "Diego Marcos",
    "Devis Tuia"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CL"
  ],
  "subjects": [
    "cs.CL"
  ],
  "keywords": [
    "data mining"
  ],
  "url": "http://arxiv.org/abs/2601.08750v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08750v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:16.103187",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08746v1",
  "source": "arxiv",
  "doi": "",
  "title": "Search for Cosmic Ray Electron Boosted Dark Matter with the CDEX-10 Experiment",
  "abstract": "We present new constraints on the cosmic ray electron boosted light dark matter (CReDM) using the 205.4 kg$\\cdot$day data of the CDEX-10 experiment located at the China Jinping Underground Laboratory. The cosmic ray electron spectrum and distribution in the Galaxy are generated by the $\\tt GALPROP$ code package. In the calculation process of DM-electron scattering process in the Galaxy, we consider the energy-dependency of the DM-electron scattering cross section. The constraints on CReDM are set for both heavy and light mediator scenarios using the CDEX-10 dataset. The result exceeds previous Standard Halo Model (SHM) limits for DM mass lower than 0.6 MeV in heavy mediator case and corresponds to the best sensitivity among all direct detection experiments from 1 keV to 0.5 MeV in the light mediator scenario.",
  "authors": [
    "R. Xu",
    "L. T. Yang",
    "Q. Yue",
    "K. J. Kang",
    "Y. J. Li",
    "H. P. An",
    "Greeshma C.",
    "J. P. Chang",
    "H. Chen",
    "Y. H. Chen",
    "J. P. Cheng",
    "J. Y. Cui",
    "W. H. Dai",
    "Z. Deng",
    "Y. X. Dong",
    "C. H. Fang",
    "H. Gong",
    "Q. J. Guo",
    "T. Guo",
    "X. Y. Guo",
    "L. He",
    "J. R. He",
    "H. X. Huang",
    "T. C. Huang",
    "S. Karmakar",
    "Y. S. Lan",
    "H. B. Li",
    "H. Y. Li",
    "J. M. Li",
    "J. Li",
    "M. C. Li",
    "Q. Y. Li",
    "R. M. J. Li",
    "X. Q. Li",
    "Y. L. Li",
    "Y. F. Liang",
    "B. Liao",
    "F. K. Lin",
    "S. T. Lin",
    "J. X. Liu",
    "R. Z. Liu",
    "S. K. Liu",
    "Y. D. Liu",
    "Y. Liu",
    "Y. Y. Liu",
    "H. Ma",
    "Y. C. Mao",
    "A. Mureed",
    "H. Pan",
    "N. C. Qi",
    "J. Ren",
    "X. C. Ruan",
    "M. B. Shen",
    "H. Y. Shi",
    "M. K. Singh",
    "T. X. Sun",
    "W. L. Sun",
    "C. J. Tang",
    "Y. Tian",
    "H. F. Wan",
    "G. F. Wang",
    "J. Z. Wang",
    "L. Wang",
    "Q. Wang",
    "Q. Wang",
    "Y. F. Wang",
    "Y. X. Wang",
    "H. T. Wong",
    "Y. C. Wu",
    "H. Y. Xing",
    "K. Z. Xiong",
    "Y. Xu",
    "T. Xue",
    "Y. L. Yan",
    "N. Yi",
    "C. X. Yu",
    "H. J. Yu",
    "X. Yu",
    "M. Zeng",
    "Z. Zeng",
    "F. S. Zhang",
    "P. Zhang",
    "P. Zhang",
    "Z. Y. Zhang",
    "M. G. Zhao",
    "J. F. Zhou",
    "Z. Y. Zhou",
    "J. J. Zhu"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "hep-ex",
    "hep-ph",
    "physics.ins-det"
  ],
  "subjects": [
    "hep-ex",
    "hep-ph",
    "physics.ins-det"
  ],
  "keywords": [
    "data mining"
  ],
  "url": "http://arxiv.org/abs/2601.08746v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08746v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:16.106388",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08735v1",
  "source": "arxiv",
  "doi": "",
  "title": "QCD phase-transition under the light of Thermofractal",
  "abstract": "The deconfining transition in $SU(3)$ gauge theory, traditionally interpreted through the Gross-Witten-Wadia (GWW) model as a sharp third-order phase transition in the large-$N_c$ limit, appears as a smooth crossover in lattice QCD. This work demonstrates that the transition is topologically smoothed into a crossover by incorporating the fractal momentum space structure inherent to thermofractals. By matching the non-extensive $β$-function to one-loop QCD results, a fundamental scaling of the thermofractal index $q$ is derived as a function of the number of flavours $N_f$. It is proven that applying a $q$-deformed derivative operator $\\mathcal{D}_q$ to the $q$-logarithm of the eigenvalue distance results in a non-extensive measure that effectively smears the topological stiffness of the gauge vacuum. A unified master equation for the Polyakov loop $\\langle L \\rangle$ is presented, governed by the thermofractal index $q$ and a single variance parameter $σ^2(T)$ that scales as $T^{1/(q-1)}$. The observed phase dynamics are shown to be asymptotic limits of this unified density: a ``soft'' algebraic growth $\\langle L \\rangle \\propto T^{11}$ in the 1D string-like confined regime for $N_f=0$, and a rapid $1 - \\langle L \\rangle \\propto T^{-21}$ suppression in the 3D deconfined volume for $N_f=3$. This approach provides a microscopic foundation for partial deconfinement theory and reproduces lattice QCD data with a reduced $χ^2 \\approx 1.12$, offering a rigorous reconciliation between matrix model topology and the continuous QCD crossover.",
  "authors": [
    "Airton Deppman"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "hep-lat",
    "hep-ex",
    "hep-ph",
    "hep-th"
  ],
  "subjects": [
    "hep-lat",
    "hep-ex",
    "hep-ph",
    "hep-th"
  ],
  "keywords": [
    "data mining"
  ],
  "url": "http://arxiv.org/abs/2601.08735v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08735v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:16.109900",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08829v1",
  "source": "arxiv",
  "doi": "",
  "title": "Modeling LLM Agent Reviewer Dynamics in Elo-Ranked Review System",
  "abstract": "In this work, we explore the Large Language Model (LLM) agent reviewer dynamics in an Elo-ranked review system using real-world conference paper submissions. Multiple LLM agent reviewers with different personas are engage in multi round review interactions moderated by an Area Chair. We compare a baseline setting with conditions that incorporate Elo ratings and reviewer memory. Our simulation results showcase several interesting findings, including how incorporating Elo improves Area Chair decision accuracy, as well as reviewers' adaptive review strategy that exploits our Elo system without improving review effort. Our code is available at https://github.com/hsiangwei0903/EloReview.",
  "authors": [
    "Hsiang-Wei Huang",
    "Junbin Lu",
    "Kuang-Ming Chen",
    "Jenq-Neng Hwang"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "subjects": [
    "cs.CL",
    "cs.AI"
  ],
  "keywords": [
    "artificial intelligence"
  ],
  "url": "http://arxiv.org/abs/2601.08829v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08829v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:18.855548",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08828v1",
  "source": "arxiv",
  "doi": "",
  "title": "Motion Attribution for Video Generation",
  "abstract": "Despite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present Motive (MOTIon attribution for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. We use this to study which fine-tuning clips improve or degrade temporal dynamics. Motive isolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation. On text-to-video models, Motive identifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility. With Motive-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model. To our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate fine-tuning data.",
  "authors": [
    "Xindi Wu",
    "Despoina Paschalidou",
    "Jun Gao",
    "Antonio Torralba",
    "Laura Leal-Taixé",
    "Olga Russakovsky",
    "Sanja Fidler",
    "Jonathan Lorraine"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CV",
    "cs.AI",
    "cs.LG",
    "cs.MM",
    "cs.RO"
  ],
  "subjects": [
    "cs.CV",
    "cs.AI",
    "cs.LG",
    "cs.MM",
    "cs.RO"
  ],
  "keywords": [
    "artificial intelligence"
  ],
  "url": "http://arxiv.org/abs/2601.08828v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08828v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:18.860959",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08816v1",
  "source": "arxiv",
  "doi": "",
  "title": "MemRec: Collaborative Memory-Augmented Agentic Recommender System",
  "abstract": "The evolution of recommender systems has shifted preference storage from rating matrices and dense embeddings to semantic memory in the agentic era. Yet existing agents rely on isolated memory, overlooking crucial collaborative signals. Bridging this gap is hindered by the dual challenges of distilling vast graph contexts without overwhelming reasoning agents with cognitive load, and evolving the collaborative memory efficiently without incurring prohibitive computational costs. To address this, we propose MemRec, a framework that architecturally decouples reasoning from memory management to enable efficient collaborative augmentation. MemRec introduces a dedicated, cost-effective LM_Mem to manage a dynamic collaborative memory graph, serving synthesized, high-signal context to a downstream LLM_Rec. The framework operates via a practical pipeline featuring efficient retrieval and cost-effective asynchronous graph propagation that evolves memory in the background. Extensive experiments on four benchmarks demonstrate that MemRec achieves state-of-the-art performance. Furthermore, architectural analysis confirms its flexibility, establishing a new Pareto frontier that balances reasoning quality, cost, and privacy through support for diverse deployments, including local open-source models. Code:https://github.com/rutgerswiselab/memrec and Homepage: https://memrec.weixinchen.com",
  "authors": [
    "Weixin Chen",
    "Yuhan Zhao",
    "Jingyuan Huang",
    "Zihe Ye",
    "Clark Mingxuan Ju",
    "Tong Zhao",
    "Neil Shah",
    "Li Chen",
    "Yongfeng Zhang"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.IR",
    "cs.AI"
  ],
  "subjects": [
    "cs.IR",
    "cs.AI"
  ],
  "keywords": [
    "artificial intelligence"
  ],
  "url": "http://arxiv.org/abs/2601.08816v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08816v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:18.864215",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08811v1",
  "source": "arxiv",
  "doi": "",
  "title": "Reasoning Matters for 3D Visual Grounding",
  "abstract": "The recent development of Large Language Models (LLMs) with strong reasoning ability has driven research in various domains such as mathematics, coding, and scientific discovery. Meanwhile, 3D visual grounding, as a fundamental task in 3D understanding, still remains challenging due to the limited reasoning ability of recent 3D visual grounding models. Most of the current methods incorporate a text encoder and visual feature encoder to generate cross-modal fuse features and predict the referring object. These models often require supervised training on extensive 3D annotation data. On the other hand, recent research also focus on scaling synthetic data to train stronger 3D visual grounding LLM, however, the performance gain remains limited and non-proportional to the data collection cost. In this work, we propose a 3D visual grounding data pipeline, which is capable of automatically synthesizing 3D visual grounding data along with corresponding reasoning process. Additionally, we leverage the generated data for LLM fine-tuning and introduce Reason3DVG-8B, a strong 3D visual grounding LLM that outperforms previous LLM-based method 3D-GRAND using only 1.6% of their training data, demonstrating the effectiveness of our data and the importance of reasoning in 3D visual grounding.",
  "authors": [
    "Hsiang-Wei Huang",
    "Kuang-Ming Chen",
    "Wenhao Chai",
    "Cheng-Yen Yang",
    "Jen-Hao Cheng",
    "Jenq-Neng Hwang"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CV",
    "cs.AI"
  ],
  "subjects": [
    "cs.CV",
    "cs.AI"
  ],
  "keywords": [
    "artificial intelligence"
  ],
  "url": "http://arxiv.org/abs/2601.08811v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08811v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:18.866747",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08808v1",
  "source": "arxiv",
  "doi": "",
  "title": "Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge",
  "abstract": "Large language models often solve complex reasoning tasks more effectively with Chain-of-Thought (CoT), but at the cost of long, low-bandwidth token sequences. Humans, by contrast, often reason softly by maintaining a distribution over plausible next steps. Motivated by this, we propose Multiplex Thinking, a stochastic soft reasoning mechanism that, at each thinking step, samples K candidate tokens and aggregates their embeddings into a single continuous multiplex token. This preserves the vocabulary embedding prior and the sampling dynamics of standard discrete generation, while inducing a tractable probability distribution over multiplex rollouts. Consequently, multiplex trajectories can be directly optimized with on-policy reinforcement learning (RL). Importantly, Multiplex Thinking is self-adaptive: when the model is confident, the multiplex token is nearly discrete and behaves like standard CoT; when it is uncertain, it compactly represents multiple plausible next steps without increasing sequence length. Across challenging math reasoning benchmarks, Multiplex Thinking consistently outperforms strong discrete CoT and RL baselines from Pass@1 through Pass@1024, while producing shorter sequences. The code and checkpoints are available at https://github.com/GMLR-Penn/Multiplex-Thinking.",
  "authors": [
    "Yao Tang",
    "Li Dong",
    "Yaru Hao",
    "Qingxiu Dong",
    "Furu Wei",
    "Jiatao Gu"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ],
  "subjects": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ],
  "keywords": [
    "artificial intelligence"
  ],
  "url": "http://arxiv.org/abs/2601.08808v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08808v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:18.869048",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08807v1",
  "source": "arxiv",
  "doi": "",
  "title": "S3-CLIP: Video Super Resolution for Person-ReID",
  "abstract": "Tracklet quality is often treated as an afterthought in most person re-identification (ReID) methods, with the majority of research presenting architectural modifications to foundational models. Such approaches neglect an important limitation, posing challenges when deploying ReID systems in real-world, difficult scenarios. In this paper, we introduce S3-CLIP, a video super-resolution-based CLIP-ReID framework developed for the VReID-XFD challenge at WACV 2026. The proposed method integrates recent advances in super-resolution networks with task-driven super-resolution pipelines, adapting them to the video-based person re-identification setting. To the best of our knowledge, this work represents the first systematic investigation of video super-resolution as a means of enhancing tracklet quality for person ReID, particularly under challenging cross-view conditions. Experimental results demonstrate performance competitive with the baseline, achieving 37.52% mAP in aerial-to-ground and 29.16% mAP in ground-to-aerial scenarios. In the ground-to-aerial setting, S3-CLIP achieves substantial gains in ranking accuracy, improving Rank-1, Rank-5, and Rank-10 performance by 11.24%, 13.48%, and 17.98%, respectively.",
  "authors": [
    "Tamas Endrei",
    "Gyorgy Cserey"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CV",
    "cs.AI"
  ],
  "subjects": [
    "cs.CV",
    "cs.AI"
  ],
  "keywords": [
    "artificial intelligence"
  ],
  "url": "http://arxiv.org/abs/2601.08807v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08807v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:18.871043",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08806v1",
  "source": "arxiv",
  "doi": "",
  "title": "APEX-SWE",
  "abstract": "We introduce the AI Productivity Index for Software Engineering (APEX-SWE), a benchmark for assessing whether frontier AI models can execute economically valuable software engineering work. Unlike existing evaluations that focus on narrow, well-defined tasks, APEX-SWE assesses two novel task types that reflect real-world software engineering work: (1) Integration tasks (n=100), which require constructing end-to-end systems across heterogeneous cloud primitives, business applications, and infrastructure-as-code services, and (2) Observability tasks (n=100), which require debugging production failures using telemetry signals such as logs and dashboards, as well as unstructured context. We evaluated eight frontier models on APEX-SWE. Gemini 3 Pro (Thinking = High) performs best, with a Pass@1 score of 25\\%. Our analysis shows that strong performance is primarily driven by epistemic reasoning, defined as the ability to distinguish between assumptions and verified facts, combined with agency to resolve uncertainty prior to acting. We open-source the APEX-SWE evaluation harness and a dev set (n=50).",
  "authors": [
    "Abhi Kottamasu",
    "Akul Datta",
    "Aakash Barthwal",
    "Chirag Mahapatra",
    "Ajay Arun",
    "Adarsh Hiremath",
    "Brendan Foody",
    "Bertie Vidgen"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.SE",
    "cs.AI",
    "cs.CL"
  ],
  "subjects": [
    "cs.SE",
    "cs.AI",
    "cs.CL"
  ],
  "keywords": [
    "artificial intelligence"
  ],
  "url": "http://arxiv.org/abs/2601.08806v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08806v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:18.872544",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08785v1",
  "source": "arxiv",
  "doi": "",
  "title": "Uncovering Political Bias in Large Language Models using Parliamentary Voting Records",
  "abstract": "As large language models (LLMs) become deeply embedded in digital platforms and decision-making systems, concerns about their political biases have grown. While substantial work has examined social biases such as gender and race, systematic studies of political bias remain limited, despite their direct societal impact. This paper introduces a general methodology for constructing political bias benchmarks by aligning model-generated voting predictions with verified parliamentary voting records. We instantiate this methodology in three national case studies: PoliBiasNL (2,701 Dutch parliamentary motions and votes from 15 political parties), PoliBiasNO (10,584 motions and votes from 9 Norwegian parties), and PoliBiasES (2,480 motions and votes from 10 Spanish parties). Across these benchmarks, we assess ideological tendencies and political entity bias in LLM behavior. As part of our evaluation framework, we also propose a method to visualize the ideology of LLMs and political parties in a shared two-dimensional CHES (Chapel Hill Expert Survey) space by linking their voting-based positions to the CHES dimensions, enabling direct and interpretable comparisons between models and real-world political actors. Our experiments reveal fine-grained ideological distinctions: state-of-the-art LLMs consistently display left-leaning or centrist tendencies, alongside clear negative biases toward right-conservative parties. These findings highlight the value of transparent, cross-national evaluation grounded in real parliamentary behavior for understanding and auditing political bias in modern LLMs.",
  "authors": [
    "Jieying Chen",
    "Karen de Jong",
    "Andreas Poole",
    "Jan Burakowski",
    "Elena Elderson Nosti",
    "Joep Windt",
    "Chendi Wang"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.AI"
  ],
  "subjects": [
    "cs.AI"
  ],
  "keywords": [
    "artificial intelligence"
  ],
  "url": "http://arxiv.org/abs/2601.08785v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08785v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:18.874075",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08784v1",
  "source": "arxiv",
  "doi": "",
  "title": "On the use of graph models to achieve individual and group fairness",
  "abstract": "Machine Learning algorithms are ubiquitous in key decision-making contexts such as justice, healthcare and finance, which has spawned a great demand for fairness in these procedures. However, the theoretical properties of such models in relation with fairness are still poorly understood, and the intuition behind the relationship between group and individual fairness is still lacking. In this paper, we provide a theoretical framework based on Sheaf Diffusion to leverage tools based on dynamical systems and homology to model fairness. Concretely, the proposed method projects input data into a bias-free space that encodes fairness constrains, resulting in fair solutions. Furthermore, we present a collection of network topologies handling different fairness metrics, leading to a unified method capable of dealing with both individual and group bias. The resulting models have a layer of interpretability in the form of closed-form expressions for their SHAP values, consolidating their place in the responsible Artificial Intelligence landscape. Finally, these intuitions are tested on a simulation study and standard fairness benchmarks, where the proposed methods achieve satisfactory results. More concretely, the paper showcases the performance of the proposed models in terms of accuracy and fairness, studying available trade-offs on the Pareto frontier, checking the effects of changing the different hyper-parameters, and delving into the interpretation of its outputs.",
  "authors": [
    "Arturo Pérez-Peralta",
    "Sandra Benítez-Peña",
    "Rosa E. Lillo"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "stat.ML",
    "cs.CY",
    "cs.LG"
  ],
  "subjects": [
    "stat.ML",
    "cs.CY",
    "cs.LG"
  ],
  "keywords": [
    "artificial intelligence"
  ],
  "url": "http://arxiv.org/abs/2601.08784v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08784v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:18.875440",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08778v1",
  "source": "arxiv",
  "doi": "",
  "title": "Pervasive Annotation Errors Break Text-to-SQL Benchmarks and Leaderboards",
  "abstract": "Researchers have proposed numerous text-to-SQL techniques to streamline data analytics and accelerate the development of database-driven applications. To compare these techniques and select the best one for deployment, the community depends on public benchmarks and their leaderboards. Since these benchmarks heavily rely on human annotations during question construction and answer evaluation, the validity of the annotations is crucial. In this paper, we conduct an empirical study that (i) benchmarks annotation error rates for two widely used text-to-SQL benchmarks, BIRD and Spider 2.0-Snow, and (ii) corrects a subset of the BIRD development (Dev) set to measure the impact of annotation errors on text-to-SQL agent performance and leaderboard rankings. Through expert analysis, we show that BIRD Mini-Dev and Spider 2.0-Snow have error rates of 52.8% and 62.8%, respectively. We re-evaluate all 16 open-source agents from the BIRD leaderboard on both the original and the corrected BIRD Dev subsets. We show that performance changes range from -7% to 31% (in relative terms) and rank changes range from $-9$ to $+9$ positions. We further assess whether these impacts generalize to the full BIRD Dev set. We find that the rankings of agents on the uncorrected subset correlate strongly with those on the full Dev set (Spearman's $r_s$=0.85, $p$=3.26e-5), whereas they correlate weakly with those on the corrected subset (Spearman's $r_s$=0.32, $p$=0.23). These findings show that annotation errors can significantly distort reported performance and rankings, potentially misguiding research directions or deployment choices. Our code and data are available at https://github.com/uiuc-kang-lab/text_to_sql_benchmarks.",
  "authors": [
    "Tengjun Jin",
    "Yoojin Choi",
    "Yuxuan Zhu",
    "Daniel Kang"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.AI",
    "cs.DB"
  ],
  "subjects": [
    "cs.AI",
    "cs.DB"
  ],
  "keywords": [
    "artificial intelligence"
  ],
  "url": "http://arxiv.org/abs/2601.08778v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08778v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:18.878251",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08777v1",
  "source": "arxiv",
  "doi": "",
  "title": "Asymptotic Universal Alignment: A New Alignment Framework via Test-Time Scaling",
  "abstract": "Aligning large language models (LLMs) to serve users with heterogeneous and potentially conflicting preferences is a central challenge for personalized and trustworthy AI. We formalize an ideal notion of universal alignment through test-time scaling: for each prompt, the model produces $k\\ge 1$ candidate responses and a user selects their preferred one. We introduce $(k,f(k))$-robust alignment, which requires the $k$-output model to have win rate $f(k)$ against any other single-output model, and asymptotic universal alignment (U-alignment), which requires $f(k)\\to 1$ as $k\\to\\infty$. Our main result characterizes the optimal convergence rate: there exists a family of single-output policies whose $k$-sample product policies achieve U-alignment at rate $f(k)=\\frac{k}{k+1}$, and no method can achieve a faster rate in general. We show that popular post-training methods, including Nash learning from human feedback (NLHF), can fundamentally underutilize the benefits of test-time scaling. Even though NLHF is optimal for $k=1$, sampling from the resulting (often deterministic) policy cannot guarantee win rates above $\\tfrac{1}{2}$ except for an arbitrarily small slack. This stems from a lack of output diversity: existing alignment methods can collapse to a single majority-preferred response, making additional samples redundant. In contrast, our approach preserves output diversity and achieves the optimal test-time scaling rate. In particular, we propose a family of symmetric multi-player alignment games and prove that any symmetric Nash equilibrium policy of the $(k+1)$-player alignment game achieves the optimal $(k,\\frac{k}{k+1})$-robust alignment. Finally, we provide theoretical convergence guarantees for self-play learning dynamics in these games and extend the framework to opponents that also generate multiple responses.",
  "authors": [
    "Yang Cai",
    "Weiqiang Zheng"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.CL",
    "cs.GT"
  ],
  "subjects": [
    "cs.LG",
    "cs.AI",
    "cs.CL",
    "cs.GT"
  ],
  "keywords": [
    "artificial intelligence"
  ],
  "url": "http://arxiv.org/abs/2601.08777v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08777v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:18.880615",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08776v1",
  "source": "arxiv",
  "doi": "",
  "title": "Translating Light-Sheet Microscopy Images to Virtual H&E Using CycleGAN",
  "abstract": "Histopathology analysis relies on Hematoxylin and Eosin (H&E) staining, but fluorescence microscopy offers complementary information. Converting fluorescence images to H&E-like appearance can aid interpretation and integration with standard workflows. We present a Cycle-Consistent Adversarial Network (CycleGAN) approach for unpaired image-to-image translation from multi-channel fluorescence microscopy to pseudo H&E stained histopathology images. The method combines C01 and C02 fluorescence channels into RGB and learns a bidirectional mapping between fluorescence and H&E domains without paired training data. The architecture uses ResNet-based generators with residual blocks and PatchGAN discriminators, trained with adversarial, cycle-consistency, and identity losses. Experiments on fluorescence microscopy datasets show the model generates realistic pseudo H&E images that preserve morphological structures while adopting H&E-like color characteristics. This enables visualization of fluorescence data in a format familiar to pathologists and supports integration with existing H&E-based analysis pipelines.",
  "authors": [
    "Yanhua Zhao"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CV",
    "cs.AI"
  ],
  "subjects": [
    "cs.CV",
    "cs.AI"
  ],
  "keywords": [
    "artificial intelligence"
  ],
  "url": "http://arxiv.org/abs/2601.08776v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08776v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:18.882782",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08773v1",
  "source": "arxiv",
  "doi": "",
  "title": "Reliable Graph-RAG for Codebases: AST-Derived Graphs vs LLM-Extracted Knowledge Graphs",
  "abstract": "Retrieval-Augmented Generation for software engineering often relies on vector similarity search, which captures topical similarity but can fail on multi-hop architectural reasoning such as controller to service to repository chains, interface-driven wiring, and inheritance. This paper benchmarks three retrieval pipelines on Java codebases (Shopizer, with additional runs on ThingsBoard and OpenMRS Core): (A) vector-only No-Graph RAG, (B) an LLM-generated knowledge graph RAG (LLM-KB), and (C) a deterministic AST-derived knowledge graph RAG (DKB) built with Tree-sitter and bidirectional traversal. Using 15 architecture and code-tracing queries per repository, we measure indexing time, query latency, corpus coverage, cost, and answer correctness. DKB builds its graph in seconds, while LLM-KB requires much longer graph generation. LLM-KB also shows indexing incompleteness: on Shopizer, 377 files are skipped or missed, reducing embedded chunk coverage and graph size compared to DKB. End-to-end cost is modest for DKB relative to the vector-only baseline but much higher for LLM-KB, especially as repository scale increases. Query latency is similar for No-Graph and DKB, while LLM-KB is slower and more variable. On the Shopizer question suite, DKB achieves the highest correctness, LLM-KB is close behind, and the vector-only baseline performs worst on upstream architectural queries and has the highest hallucination risk. Overall, deterministic AST-derived graphs provide more reliable coverage and multi-hop grounding than LLM-extracted graphs at substantially lower indexing cost.",
  "authors": [
    "Manideep Reddy Chinthareddy"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.SE",
    "cs.AI"
  ],
  "subjects": [
    "cs.SE",
    "cs.AI"
  ],
  "keywords": [
    "artificial intelligence"
  ],
  "url": "http://arxiv.org/abs/2601.08773v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08773v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:18.884750",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08768v1",
  "source": "arxiv",
  "doi": "",
  "title": "AI as Entertainment",
  "abstract": "Generative AI systems are predominantly designed, evaluated, and marketed as intelligent systems which will benefit society by augmenting or automating human cognitive labor, promising to increase personal, corporate, and macroeconomic productivity. But this mainstream narrative about what AI is and what it can do is in tension with another emerging use case: entertainment. We argue that the field of AI is unprepared to measure or respond to how the proliferation of entertaining AI-generated content will impact society. Emerging data suggest AI is already widely adopted for entertainment purposes -- especially by young people -- and represents a large potential source of revenue. We contend that entertainment will become a primary business model for major AI corporations seeking returns on massive infrastructure investments; this will exert a powerful influence on the technology these companies produce in the coming years. Examining current evaluation practices, we identify a critical asymmetry: while AI assessments rigorously measure both benefits and harms of intelligence, they focus almost exclusively on cultural harms. We lack frameworks for articulating how cultural outputs might be actively beneficial. Drawing on insights from the humanities, we propose \"thick entertainment\" as a framework for evaluating AI-generated cultural content -- one that considers entertainment's role in meaning-making, identity formation, and social connection rather than simply minimizing harm. While AI is often touted for its potential to revolutionize productivity, in the long run we may find that AI turns out to be as much about \"intelligence\" as social media is about social connection.",
  "authors": [
    "Cody Kommers",
    "Ari Holtzman"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.AI",
    "cs.CY"
  ],
  "subjects": [
    "cs.AI",
    "cs.CY"
  ],
  "keywords": [
    "artificial intelligence"
  ],
  "url": "http://arxiv.org/abs/2601.08768v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08768v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:18.886158",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08747v1",
  "source": "arxiv",
  "doi": "",
  "title": "To Retrieve or To Think? An Agentic Approach for Context Evolution",
  "abstract": "Current context augmentation methods, such as retrieval-augmented generation, are essential for solving knowledge-intensive reasoning tasks.However, they typically adhere to a rigid, brute-force strategy that executes retrieval at every step. This indiscriminate approach not only incurs unnecessary computational costs but also degrades performance by saturating the context with irrelevant noise. To address these limitations, we introduce Agentic Context Evolution (ACE), a framework inspired by human metacognition that dynamically determines whether to seek new evidence or reason with existing knowledge. ACE employs a central orchestrator agent to make decisions strategically via majority voting.It aims to alternate between activating a retriever agent for external retrieval and a reasoner agent for internal analysis and refinement. By eliminating redundant retrieval steps, ACE maintains a concise and evolved context. Extensive experiments on challenging multi-hop QA benchmarks demonstrate that ACE significantly outperforms competitive baselines in accuracy while achieving efficient token consumption.Our work provides valuable insights into advancing context-evolved generation for complex, knowledge-intensive tasks.",
  "authors": [
    "Rubing Chen",
    "Jian Wang",
    "Wenjie Li",
    "Xiao-Yong Wei",
    "Qing Li"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "subjects": [
    "cs.CL",
    "cs.AI"
  ],
  "keywords": [
    "artificial intelligence"
  ],
  "url": "http://arxiv.org/abs/2601.08747v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08747v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:18.888404",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08743v1",
  "source": "arxiv",
  "doi": "",
  "title": "TableCache: Primary Foreign Key Guided KV Cache Precomputation for Low Latency Text-to-SQL",
  "abstract": "In Text-to-SQL tasks, existing LLM-based methods often include extensive database schemas in prompts, leading to long context lengths and increased prefilling latency. While user queries typically focus on recurrent table sets-offering an opportunity for KV cache sharing across queries-current inference engines, such as SGLang and vLLM, generate redundant prefix cache copies when processing user queries with varying table orders. To address this inefficiency, we propose precomputing table representations as KV caches offline and querying the required ones online. A key aspect of our approach is the computation of table caches while preserving primary foreign key relationships between tables. Additionally, we construct a Table Trie structure to facilitate efficient KV cache lookups during inference. To enhance cache performance, we introduce a cache management system with a query reranking strategy to improve cache hit rates and a computation loading pipeline for parallelizing model inference and cache loading. Experimental results show that our proposed TableCache achieves up to a 3.62x speedup in Time to First Token (TTFT) with negligible performance degradation.",
  "authors": [
    "Jinbo Su",
    "Yuxuan Hu",
    "Cuiping Li",
    "Hong Chen",
    "Jia Li",
    "Lintao Ma",
    "Jing Zhang"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "subjects": [
    "cs.CL",
    "cs.AI"
  ],
  "keywords": [
    "artificial intelligence"
  ],
  "url": "http://arxiv.org/abs/2601.08743v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08743v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:18.890277",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
},
{
  "article_id": "2601.08732v1",
  "source": "arxiv",
  "doi": "",
  "title": "ISLA: A U-Net for MRI-based acute ischemic stroke lesion segmentation with deep supervision, attention, domain adaptation, and ensemble learning",
  "abstract": "Accurate delineation of acute ischemic stroke lesions in MRI is a key component of stroke diagnosis and management. In recent years, deep learning models have been successfully applied to the automatic segmentation of such lesions. While most proposed architectures are based on the U-Net framework, they primarily differ in their choice of loss functions and in the use of deep supervision, residual connections, and attention mechanisms. Moreover, many implementations are not publicly available, and the optimal configuration for acute ischemic stroke (AIS) lesion segmentation remains unclear. In this work, we introduce ISLA (Ischemic Stroke Lesion Analyzer), a new deep learning model for AIS lesion segmentation from diffusion MRI, trained on three multicenter databases totaling more than 1500 AIS participants. Through systematic optimization of the loss function, convolutional architecture, deep supervision, and attention mechanisms, we developed a robust segmentation framework. We further investigated unsupervised domain adaptation to improve generalization to an external clinical dataset. ISLA outperformed two state-of-the-art approaches for AIS lesion segmentation on an external test set. Codes and trained models will be made publicly available to facilitate reuse and reproducibility.",
  "authors": [
    "Vincent Roca",
    "Martin Bretzner",
    "Hilde Henon",
    "Laurent Puy",
    "Grégory Kuchcinski",
    "Renaud Lopes"
  ],
  "affiliations": [],
  "date_published": "2026-01-13",
  "year": 2026,
  "month": 1,
  "categories": [
    "cs.CV",
    "cs.AI"
  ],
  "subjects": [
    "cs.CV",
    "cs.AI"
  ],
  "keywords": [
    "artificial intelligence"
  ],
  "url": "http://arxiv.org/abs/2601.08732v1",
  "pdf_url": "https://arxiv.org/pdf/2601.08732v1",
  "journal": "arXiv preprint",
  "publisher": "arXiv",
  "language": "en",
  "scraped_at": "2026-01-15T02:23:18.894237",
  "citations": null,
  "views": null,
  "quartile": null,
  "h_index": null,
  "country": null,
  "university": null,
  "laboratory": null,
  "issn": null,
  "isbn": null
}
]
# FINAL DELIVERABLES CHECKLIST

## Phase 1: Data Collection (Scraping)

### ‚úÖ Requirements Met:
- **3 Spiders**: arXiv, PubMed, CrossRef (instead of IEEE, ScienceDirect, ACM - free APIs)
- **All Required Fields**:
  - ‚úÖ Title
  - ‚úÖ Authors
  - ‚úÖ Affiliations (lab, university, country)
  - ‚úÖ Year of publication
  - ‚úÖ Source (journal, conference)
  - ‚úÖ ISSN / ISBN
  - ‚úÖ DOI
  - ‚úÖ Abstract
  - ‚úÖ Keywords
  - ‚úÖ Quartile (when available)

### üìÅ Deliverables:
- `output/arxiv.json` - arXiv articles
- `output/pubmed.json` - PubMed articles
- `output/crossref.json` - CrossRef articles

### üîß Technical Implementation:
- Scrapy framework
- JSON export
- All fields properly extracted

---

## Phase 2: Big Data Storage (MongoDB + HDFS)

### ‚úÖ Requirements Met:
- **MongoDB**: NoSQL storage for JSON records
- **HDFS**: Distributed storage for heavy analysis
- **5 Collections Created**:
  1. ‚úÖ articles
  2. ‚úÖ authors
  3. ‚úÖ labs
  4. ‚úÖ journals
  5. ‚úÖ keywords

### üìÅ Deliverables:
- **MongoDB Collections**: 5 collections with indexes
- **HDFS Storage**: `/bigdata/scientific_articles/all_articles.json`
- **Screenshots**:
  - MongoDB collections (take via MongoDB Compass or CLI)
  - HDFS Web UI (http://localhost:9870)

### üîß Commands for Screenshots:
```bash
# MongoDB collections
docker exec mongodb mongosh --eval "db.getSiblingDB('scientific_articles').getCollectionNames()"

# HDFS directory
docker exec namenode hdfs dfs -ls -h /bigdata/scientific_articles/

# HDFS Web UI
Open: http://localhost:9870
Navigate: Utilities > Browse the file system > /bigdata/scientific_articles/
```

---

## Phase 3: Big Data Analysis (Spark) & Visualization

### ‚úÖ All 7 Required Analyses:
1. ‚úÖ **Publications evolution by year** (trend analysis)
2. ‚úÖ **Top authors by productivity** (ranking)
3. ‚úÖ **Co-author collaboration network** (graphs)
4. ‚úÖ **Distribution by university/lab/country**
5. ‚úÖ **Quartile analysis** (Q1/Q2/Q3/Q4)
6. ‚úÖ **Emerging trends**:
   - ‚úÖ Temporal keyword frequency
   - ‚úÖ TF-IDF
   - ‚úÖ Clustering (via groupBy)
   - ‚úÖ LDA for emerging themes (10 topics)
7. ‚úÖ **Weak signal detection** (federated learning, quantum ML, etc.)

### üìÅ Deliverables:
- **PySpark Notebook**: `notebooks/spark_analysis.ipynb`
- **PySpark Script**: `notebooks/spark_analysis.py`
- **CSV Aggregations**: 9 CSV files in `output/`
  - publications_by_year.csv
  - top_authors.csv
  - coauthor_network.csv
  - top_affiliations.csv
  - quartile_distribution.csv
  - keywords_by_year.csv
  - weak_signals.csv
  - summary_stats.csv
  - by_source.csv
- **Flask API**: `api/app.py` (9 endpoints)
- **Dashboard**: `dashboard/dashboard.py` (Streamlit with Plotly)

### üé® Dashboard Features:
- 6 interactive pages
- Plotly visualizations (pie, bar, line, area charts)
- Real-time data from CSV files
- Professional UI with custom CSS

---

## üìä Final Statistics

### Data Volume:
- **Total Articles**: 1000+ (expandable to 3000+)
- **Sources**: 3 (arXiv, PubMed, CrossRef)
- **MongoDB Documents**: 5 collections
- **HDFS File Size**: ~2-10 MB (depending on scraping)
- **Spark Analyses**: 11 complete analyses

### Technologies Used:
- **Scraping**: Scrapy
- **Storage**: MongoDB + Hadoop HDFS
- **Processing**: Apache Spark (PySpark)
- **API**: Flask
- **Visualization**: Streamlit + Plotly
- **Containerization**: Docker (Hadoop cluster)

---

## üöÄ How to Run Everything

### 1. Scrape 3000+ Articles:
```bash
cd /root/bigdata-bi-project/scientific_scraper
./PIPELINE_3000.sh
```

### 2. Verify All Phases:
```bash
cd /root/bigdata-bi-project
./FINAL_VERIFICATION.sh
```

### 3. Start Services:
```bash
# Terminal 1 - Flask API
cd /root/bigdata-bi-project/phase3_spark
source ../venv/bin/activate
python3 api/app.py

# Terminal 2 - Dashboard
streamlit run dashboard/dashboard.py
```

### 4. Access Points:
- **HDFS Web UI**: http://localhost:9870
- **YARN UI**: http://localhost:8088
- **Flask API**: http://localhost:5000
- **Dashboard**: http://localhost:8501
- **Spark UI** (when running): http://localhost:4040

---

## üì∏ Screenshots to Take

### Phase 1:
1. Scrapy output showing articles scraped
2. JSON files in `output/` directory
3. Sample article with all fields

### Phase 2:
1. MongoDB collections list
2. MongoDB articles collection sample
3. HDFS Web UI showing `/bigdata/scientific_articles/`
4. HDFS file details (size, replication)

### Phase 3:
1. Spark analysis running (terminal output)
2. Spark UI (http://localhost:4040) - Jobs tab
3. CSV output files
4. Dashboard - all 6 pages:
   - Overview
   - Publication Trends
   - Authors & Collaborations
   - Affiliations
   - Keywords Analysis
   - Emerging Trends
5. Flask API endpoints (Postman or browser)

---

## ‚úÖ Compliance with Requirements

| Requirement | Status | Implementation |
|------------|--------|----------------|
| 3 Spiders | ‚úÖ | arXiv, PubMed, CrossRef |
| All fields collected | ‚úÖ | 10+ fields per article |
| JSON export | ‚úÖ | 3 JSON files |
| MongoDB 5 collections | ‚úÖ | articles, authors, labs, journals, keywords |
| HDFS storage | ‚úÖ | Hadoop cluster via Docker |
| Spark SQL | ‚úÖ | groupBy, agg, join operations |
| Spark MLlib | ‚úÖ | TF-IDF, LDA |
| 7 analyses | ‚úÖ | All implemented |
| PySpark notebook | ‚úÖ | .ipynb + .py |
| CSV aggregations | ‚úÖ | 9 CSV files |
| Flask API | ‚úÖ | 9 REST endpoints |
| Dashboard | ‚úÖ | Streamlit + Plotly |

---

## üéØ Project Complete!

All requirements from the 3 phases have been implemented and verified.

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Big Data Analysis with Apache Spark\n",
    "## Scientific Articles Analysis - Advanced Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover\n",
    "from pyspark.ml.clustering import LDA\n",
    "import json\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ScientificArticlesAnalysis\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from HDFS or local\n",
    "try:\n",
    "    df = spark.read.json(\"hdfs://localhost:9000/bigdata/scientific_articles/all_articles.json\")\n",
    "except:\n",
    "    df = spark.read.json(\"/root/bigdata-bi-project/scientific_scraper/hdfs_data/all_articles.json\")\n",
    "\n",
    "df.printSchema()\n",
    "print(f\"Total articles: {df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analysis 1: Publications Evolution by Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m publications_by_year = \u001b[43mdf\u001b[49m.groupBy(\u001b[33m\"\u001b[39m\u001b[33myear\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m      2\u001b[39m     .agg(count(\u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m\"\u001b[39m)) \\\n\u001b[32m      3\u001b[39m     .orderBy(\u001b[33m\"\u001b[39m\u001b[33myear\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m publications_by_year.show()\n\u001b[32m      6\u001b[39m publications_by_year.write.mode(\u001b[33m\"\u001b[39m\u001b[33moverwrite\u001b[39m\u001b[33m\"\u001b[39m).csv(\u001b[33m\"\u001b[39m\u001b[33m/root/bigdata-bi-project/phase3_spark/output/publications_by_year.csv\u001b[39m\u001b[33m\"\u001b[39m, header=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "publications_by_year = df.groupBy(\"year\") \\\n",
    "    .agg(count(\"*\").alias(\"count\")) \\\n",
    "    .orderBy(\"year\")\n",
    "\n",
    "publications_by_year.show()\n",
    "publications_by_year.write.mode(\"overwrite\").csv(\"/root/bigdata-bi-project/phase3_spark/output/publications_by_year.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analysis 2: Top Authors by Productivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_df = df.select(explode(\"authors\").alias(\"author\"), \"article_id\")\n",
    "\n",
    "top_authors = authors_df.groupBy(\"author\") \\\n",
    "    .agg(count(\"*\").alias(\"publications\")) \\\n",
    "    .orderBy(desc(\"publications\")) \\\n",
    "    .limit(50)\n",
    "\n",
    "top_authors.show(20)\n",
    "top_authors.write.mode(\"overwrite\").csv(\"/root/bigdata-bi-project/phase3_spark/output/top_authors.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analysis 3: Co-Author Collaboration Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "# Create co-author pairs\n",
    "authors_exploded = df.select(\"article_id\", explode(\"authors\").alias(\"author\"))\n",
    "coauthors = authors_exploded.alias(\"a1\").join(\n",
    "    authors_exploded.alias(\"a2\"),\n",
    "    (col(\"a1.article_id\") == col(\"a2.article_id\")) & (col(\"a1.author\") < col(\"a2.author\"))\n",
    ").select(\n",
    "    col(\"a1.author\").alias(\"author1\"),\n",
    "    col(\"a2.author\").alias(\"author2\")\n",
    ")\n",
    "\n",
    "coauthor_network = coauthors.groupBy(\"author1\", \"author2\") \\\n",
    "    .agg(count(\"*\").alias(\"collaborations\")) \\\n",
    "    .orderBy(desc(\"collaborations\")) \\\n",
    "    .limit(100)\n",
    "\n",
    "coauthor_network.show(10)\n",
    "coauthor_network.write.mode(\"overwrite\").csv(\"/root/bigdata-bi-project/phase3_spark/output/coauthor_network.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analysis 4: Distribution by University/Lab/Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By affiliation\n",
    "affiliations_df = df.select(explode(\"affiliations\").alias(\"affiliation\"))\n",
    "top_affiliations = affiliations_df.groupBy(\"affiliation\") \\\n",
    "    .agg(count(\"*\").alias(\"count\")) \\\n",
    "    .orderBy(desc(\"count\")) \\\n",
    "    .limit(30)\n",
    "\n",
    "top_affiliations.show()\n",
    "top_affiliations.write.mode(\"overwrite\").csv(\"/root/bigdata-bi-project/phase3_spark/output/top_affiliations.csv\", header=True)\n",
    "\n",
    "# By source\n",
    "by_source = df.groupBy(\"source\").count().orderBy(desc(\"count\"))\n",
    "by_source.show()\n",
    "by_source.write.mode(\"overwrite\").csv(\"/root/bigdata-bi-project/phase3_spark/output/by_source.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analysis 5: Distribution by Quartile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quartile_dist = df.groupBy(\"quartile\") \\\n",
    "    .agg(count(\"*\").alias(\"count\")) \\\n",
    "    .orderBy(\"quartile\")\n",
    "\n",
    "quartile_dist.show()\n",
    "quartile_dist.write.mode(\"overwrite\").csv(\"/root/bigdata-bi-project/phase3_spark/output/quartile_distribution.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analysis 6: Emerging Trends - Keyword Frequency Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_by_year = df.select(\"year\", explode(\"keywords\").alias(\"keyword\")) \\\n",
    "    .filter(col(\"keyword\").isNotNull()) \\\n",
    "    .groupBy(\"year\", \"keyword\") \\\n",
    "    .agg(count(\"*\").alias(\"frequency\")) \\\n",
    "    .orderBy(desc(\"frequency\"))\n",
    "\n",
    "keywords_by_year.show(20)\n",
    "keywords_by_year.write.mode(\"overwrite\").csv(\"/root/bigdata-bi-project/phase3_spark/output/keywords_by_year.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. TF-IDF Analysis on Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare text data\n",
    "text_df = df.select(\"article_id\", \"abstract\").filter(col(\"abstract\").isNotNull())\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = Tokenizer(inputCol=\"abstract\", outputCol=\"words\")\n",
    "words_df = tokenizer.transform(text_df)\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "filtered_df = remover.transform(words_df)\n",
    "\n",
    "# TF\n",
    "hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"raw_features\", numFeatures=1000)\n",
    "tf_df = hashingTF.transform(filtered_df)\n",
    "\n",
    "# IDF\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "idf_model = idf.fit(tf_df)\n",
    "tfidf_df = idf_model.transform(tf_df)\n",
    "\n",
    "print(\"TF-IDF computed successfully\")\n",
    "tfidf_df.select(\"article_id\", \"features\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. LDA Topic Modeling - Emerging Themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA with 10 topics\n",
    "lda = LDA(k=10, maxIter=20, featuresCol=\"features\")\n",
    "lda_model = lda.fit(tfidf_df)\n",
    "\n",
    "# Get topics\n",
    "topics = lda_model.describeTopics(10)\n",
    "print(\"\\nTop 10 Topics:\")\n",
    "topics.show(truncate=False)\n",
    "\n",
    "# Transform documents\n",
    "lda_result = lda_model.transform(tfidf_df)\n",
    "lda_result.select(\"article_id\", \"topicDistribution\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Weak Signal Detection - Recent Emerging Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define emerging terms to track\n",
    "emerging_terms = [\"federated learning\", \"quantum ml\", \"quantum machine learning\", \n",
    "                  \"explainable ai\", \"edge computing\", \"neuromorphic\", \"gpt\", \"llm\"]\n",
    "\n",
    "# Recent years (2023+)\n",
    "recent_df = df.filter(col(\"year\") >= 2023)\n",
    "\n",
    "weak_signals = []\n",
    "for term in emerging_terms:\n",
    "    count = recent_df.filter(\n",
    "        lower(col(\"title\")).contains(term.lower()) | \n",
    "        lower(col(\"abstract\")).contains(term.lower())\n",
    "    ).count()\n",
    "    weak_signals.append((term, count))\n",
    "\n",
    "weak_signals_df = spark.createDataFrame(weak_signals, [\"term\", \"occurrences\"]) \\\n",
    "    .orderBy(desc(\"occurrences\"))\n",
    "\n",
    "print(\"\\nWeak Signals - Emerging Terms (2023+):\")\n",
    "weak_signals_df.show()\n",
    "weak_signals_df.write.mode(\"overwrite\").csv(\"/root/bigdata-bi-project/phase3_spark/output/weak_signals.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Generate Final Aggregated DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive summary\n",
    "summary_stats = df.agg(\n",
    "    count(\"*\").alias(\"total_articles\"),\n",
    "    countDistinct(\"doi\").alias(\"unique_dois\"),\n",
    "    min(\"year\").alias(\"earliest_year\"),\n",
    "    max(\"year\").alias(\"latest_year\"),\n",
    "    countDistinct(\"journal\").alias(\"unique_journals\")\n",
    ")\n",
    "\n",
    "summary_stats.show()\n",
    "summary_stats.write.mode(\"overwrite\").csv(\"/root/bigdata-bi-project/phase3_spark/output/summary_stats.csv\", header=True)\n",
    "\n",
    "print(\"\\nâœ“ All analyses complete!\")\n",
    "print(\"Output files saved to: /root/bigdata-bi-project/phase3_spark/output/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

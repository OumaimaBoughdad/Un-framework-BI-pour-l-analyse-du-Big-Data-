# Big Data BI Project - Analyse de Publications Scientifiques

## ğŸ¯ Projet Complet: Pipeline Big Data & Business Intelligence

### Pipeline en 4 Phases:
1. âœ… **Phase 1**: Collecte de donnÃ©es (Scraping Web)
2. âœ… **Phase 2**: Stockage Big Data (MongoDB + Hadoop HDFS)
3. âœ… **Phase 3**: Analyse Big Data (Apache Spark)
4. âœ… **Phase 4**: Visualisation & BI (Dashboard)

---

## ğŸš€ Une Seule Commande Pour Tout ExÃ©cuter

```bash
./RUN_ALL.sh
```

**Cette commande exÃ©cute automatiquement:**
1. Scraping de 1000 articles par source (arXiv, PubMed, CrossRef)
2. Combinaison des donnÃ©es dans `all_articles.json`
3. Upload vers HDFS (Hadoop)
4. Analyse Spark (11 analyses complÃ¨tes)
5. Lancement du dashboard Ã  http://localhost:8501

**DurÃ©e totale**: ~30 minutes

---

## ğŸ“Š DonnÃ©es CollectÃ©es

### Pour chaque article (10+ champs):
- âœ… Titre, Auteurs, Affiliations
- âœ… AnnÃ©e, Source (journal), ISSN/ISBN, DOI
- âœ… RÃ©sumÃ©/Abstract, Mots-clÃ©s, Quartile

### Volume actuel:
- **1025+ articles** collectÃ©s
- **3 sources**: arXiv (866), PubMed (9), CrossRef (150)
- **5 collections MongoDB**: articles, authors, labs, journals, keywords
- **HDFS**: 2.3 MB de donnÃ©es distribuÃ©es

---

## ğŸ”§ Modifier le Nombre d'Articles

Ã‰ditez `RUN_ALL.sh` ligne 8:
```bash
ARTICLES_PER_SOURCE=1000  # Changez Ã  5000 pour plus de donnÃ©es
```

---

## ğŸ“ Structure du Projet

```
bigdata-bi-project/
â”œâ”€â”€ RUN_ALL.sh                          # â­ Script master (lancez ceci!)
â”œâ”€â”€ RAPPORT_FINAL.md                    # ğŸ“„ Rapport complet du projet
â”œâ”€â”€ DELIVERABLES_CHECKLIST.md           # âœ… Checklist des livrables
â”‚
â”œâ”€â”€ scientific_scraper/                 # PHASE 1: Scraping
â”‚   â”œâ”€â”€ scientific_scraper/spiders/
â”‚   â”‚   â”œâ”€â”€ arxiv_spider.py            # Spider arXiv
â”‚   â”‚   â”œâ”€â”€ pubmed_spider.py           # Spider PubMed
â”‚   â”‚   â””â”€â”€ crossref_spider.py         # Spider CrossRef
â”‚   â”œâ”€â”€ output/                         # Fichiers JSON scrapÃ©s
â”‚   â””â”€â”€ hdfs_data/
â”‚       â””â”€â”€ all_articles.json          # DonnÃ©es combinÃ©es
â”‚
â””â”€â”€ phase3_spark/                       # PHASES 3 & 4: Spark + BI
    â”œâ”€â”€ notebooks/
    â”‚   â”œâ”€â”€ spark_analysis.ipynb       # Notebook Jupyter
    â”‚   â””â”€â”€ spark_analysis.py          # Script Spark
    â”œâ”€â”€ api/
    â”‚   â””â”€â”€ app.py                     # API Flask (9 endpoints)
    â”œâ”€â”€ dashboard/
    â”‚   â””â”€â”€ dashboard.py               # Dashboard Streamlit
    â””â”€â”€ output/                         # RÃ©sultats CSV (9 fichiers)
```

---

## ğŸ“ˆ Analyses Spark RÃ©alisÃ©es

### 7 Analyses Principales:
1. **Ã‰volution des publications par annÃ©e** (trend analysis)
2. **Classement des auteurs** par productivitÃ©
3. **Analyse des collaborations** (graphes co-auteurs)
4. **Distribution gÃ©ographique** (universitÃ©/laboratoire/pays)
5. **Analyse par quartile** (Q1/Q2/Q3/Q4)
6. **Tendances Ã©mergentes**:
   - FrÃ©quence temporelle des mots-clÃ©s
   - TF-IDF (1000 features)
   - Clustering
   - LDA (10 topics)
7. **DÃ©tection de signaux faibles** (federated learning, quantum ML, etc.)

### Technologies Spark:
- âœ… Spark SQL (requÃªtes distribuÃ©es)
- âœ… Spark MLlib (TF-IDF, LDA)
- âœ… PySpark (API Python)

---

## ğŸ¨ Dashboard Interactif

### 6 Pages de Visualisation:
1. **Overview** - MÃ©triques clÃ©s et distribution
2. **Publication Trends** - Ã‰volution temporelle
3. **Authors & Collaborations** - Top auteurs et rÃ©seaux
4. **Affiliations** - Institutions et quartiles
5. **Keywords Analysis** - Analyse thÃ©matique
6. **Emerging Trends** - Signaux faibles

### Visualisations Plotly:
- Pie charts, Bar charts, Line charts
- Area fills, Heatmaps
- Graphiques interactifs (zoom, hover, export)

---

## ğŸŒ Points d'AccÃ¨s

- **Dashboard**: http://localhost:8501
- **HDFS Web UI**: http://localhost:9870
- **YARN UI**: http://localhost:8088
- **Flask API**: http://localhost:5000
- **Spark UI**: http://localhost:4040 (pendant exÃ©cution)

---

## ğŸ› ï¸ PrÃ©requis

- Docker (cluster Hadoop)
- Python 3.12 avec venv
- MongoDB
- 8 GB RAM minimum
- 10 GB espace disque

---

## ğŸ“‹ Commandes Rapides

```bash
# Lancer le projet complet
./RUN_ALL.sh

# Nettoyer les anciens fichiers
./CLEANUP.sh

# VÃ©rifier HDFS
docker exec namenode hdfs dfs -ls /bigdata/scientific_articles/

# VÃ©rifier MongoDB
docker exec mongodb mongosh --eval "db.getSiblingDB('scientific_articles').getCollectionNames()"

# Lancer uniquement le dashboard
cd phase3_spark
source ../venv/bin/activate
streamlit run dashboard/dashboard.py
```

---

## ğŸ“¦ Livrables du Projet

### Phase 1 - Collecte:
- âœ… 3 spiders Scrapy
- âœ… Fichiers JSON (arxiv.json, pubmed.json, crossref.json)
- âœ… 1025+ articles collectÃ©s

### Phase 2 - Stockage:
- âœ… MongoDB: 5 collections
- âœ… HDFS: DonnÃ©es distribuÃ©es
- âœ… Screenshots (MongoDB + HDFS Web UI)

### Phase 3 - Analyse:
- âœ… Notebook PySpark (.ipynb)
- âœ… Script Python (.py)
- âœ… 9 fichiers CSV d'agrÃ©gation
- âœ… 11 analyses complÃ¨tes

### Phase 4 - Visualisation:
- âœ… API Flask (9 endpoints)
- âœ… Dashboard Streamlit (6 pages)
- âœ… 15+ visualisations interactives
- âœ… Export PDF disponible

### Documentation:
- âœ… README.md (ce fichier)
- âœ… RAPPORT_FINAL.md (rapport complet 5+ pages)
- âœ… DELIVERABLES_CHECKLIST.md
- âœ… Code commentÃ©

---

## ğŸ¯ RÃ©sultats ClÃ©s

### Tendances IdentifiÃ©es:
- **IA/ML**: Dominance dans les publications rÃ©centes
- **Quantum Computing**: Ã‰mergence significative
- **Federated Learning**: Signal faible devenu tendance

### Signaux Faibles DÃ©tectÃ©s:
- Federated Learning (+250% croissance)
- Quantum ML (Ã©mergence 2023)
- Explainable AI (rÃ©glementation)
- LLM (explosion post-ChatGPT)

### Recommandations StratÃ©giques:
1. Investir dans l'IA explicable
2. Explorer Quantum ML
3. Renforcer collaborations internationales
4. Viser publications Q1/Q2

---

## ğŸ“š Documentation ComplÃ¨te

Consultez `RAPPORT_FINAL.md` pour:
- Analyse dÃ©taillÃ©e des tendances
- MÃ©thodologie complÃ¨te
- RÃ©sultats et insights
- Recommandations stratÃ©giques
- Architecture technique

---

## âœ… Statut du Projet

**COMPLET ET OPÃ‰RATIONNEL**

Toutes les phases sont implÃ©mentÃ©es et fonctionnelles.
Une seule commande lance l'intÃ©gralitÃ© du pipeline.

---

**Technologies**: Scrapy, MongoDB, Hadoop, Spark, Flask, Streamlit, Plotly

**Date**: Janvier 2026

**Cours**: Big Data & Business Intelligence
